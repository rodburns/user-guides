{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ALCF User Guides","text":"<p>We are moving our ALCF documentation into GitHub to make it easier to contribute and collaborate to our user and machine guides.</p> <p>Our user guides contain information for: </p> <ul> <li>Account and Project Management: Information and instructions on how to manage your ALCF account and awarded project.  </li> <li>Data Management: Information on our file systems that are mounted globally across all of our production systems.</li> <li>Polaris: Information on how to get started our newest supercomputer.</li> <li>Theta: Information on how to use our Cray XC40/KNL supercomputer.</li> <li>ThetaGPU: Information on how to use our NVIDIA DGX A100 supercomputer.</li> <li>Cooley: Information on how to use our visualization cluster.</li> <li>AI Testbed: Information on how to use our AI Accelerators.</li> <li>Aurora/Sunspot: Information on getting your code ready for our upcoming exacale supercomputer.</li> <li>Services: Information on how to use various services provided across clusters.</li> <li>Facility Policies: Information on our policies and procedures.</li> </ul>"},{"location":"#how-to-get-access","title":"How to Get Access","text":"<p>Researchers interested in using the ALCF systems (including Polaris and the AI Testbed\u2019s Cerebras CS-2 and SambaNova DataScale platforms) can now submit project proposals via the ALCF\u2019s Director\u2019s Discretionary program. Calls for porposals for additional allocation programs will be open at a later date.</p> <p>Submit your proposal requests at: Allocation Request Page</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you'd like to get started using our ALCF resources, our Getting Started webpage provides information on what you need to do in order to get time on our systems, get an account, and how to start running jobs.</p> <p>If you have an account and an award for Polaris, we suggest visiting on Getting on Polaris webpage.</p> <p>If you'd like to user ThetaGPU, visit our Getting Started on ThetaGPU webpage.</p> <p>If you'd like to user our AI accelerators, visit our Getting Started on AI Testbed webpage.</p> <p>Please send feedback to support@alcf.anl.gov</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/","title":"Accounts and Access FAQ","text":""},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-request-a-new-projectallocation","title":"How do I request a new project/allocation?","text":"<p>There are 3 allocation opportunities at ALCF. Please see How to Get an Allocation on how to get time on our systems.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#who-do-i-contact-if-my-discretionary-project-allocation-expires-or-if-i-need-to-request-additional-hours","title":"Who do I contact if my Discretionary Project Allocation expires or if I need to request additional hours?","text":"<p>To request an extension of your existing discretionary allocation or to request additional hours, please email support@alcf.anl.gov with answers to the following or fill out the form at request an extension/additional hours: - What you have accomplished with your original allocation?   - Please include a brief description of any publications or major presentations that were (or will be) generated in full or in part because of this allocation. - What you will do with the extra time? - What you are requesting as your new expiration date? - How many additional hours you are requesting?</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-join-a-project","title":"How do I join a project?","text":"<p>To join a project, please go to https://accounts.alcf.anl.gov, then click \"join a project\". Once there, scroll down to the project you want to join and click on it. At the bottom of the next page, please click on the \"Request Membership\" button. Once we receive approval from the PI regarding your membership request, we will provide you with access to the necessary resources.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-request-a-reservation","title":"How do I request a reservation?","text":"<p>Reservation requests must include information detailed here: </p> <ul> <li>Machine Reservations: Please email the completed reservation request to support@alcf.anl.gov. We will contact you after your request is reviewed by our reservations committee.</li> </ul>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-apply-for-a-new-account","title":"How do I apply for a new account?","text":"<p>Note: All ALCF accounts must be associated with an allocated project.</p> <ul> <li>Request a new account: https://www.alcf.anl.gov/support-center/get-started/request-account</li> <li>ALCF Accounts: https://accounts.alcf.anl.gov/</li> </ul>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#what-do-i-do-when-my-alcf-account-expires","title":"What do I do when my ALCF account expires?","text":"<p>Please forward your account expiry email to your Sponsor. As soon as we receive an approval email from your Sponsor, we'll proceed with your account renewal process as needed.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#what-do-i-do-when-i-receive-a-warning-that-my-593-has-expired-is-about-to-expire","title":"What do I do when I receive a warning that my 593 has expired / is about to expire?","text":"<p>If you are planning to extend this assignment/computer user account, please let us know, so a new 593 (Foreign Visit &amp; Assignment Request form) will be filed for you using the information from before. In case any other documents are needed from your end, you'll be contacted as necessary. In order to allow sufficient time for an indices check, it is recommended that your response be submitted as soon as possible.</p> <p>If you are not planning to extend your account, also let us know so that we may close out your records.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/","title":"ALCF Passcode Tokens","text":"<p>Please note: An account can be associated with a single token only (Mobile or Physical token). Please contact accounts@alcf.anl.gov to change your token preference.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#mobile-token","title":"Mobile Token","text":"<p>The SafeNet MobilePass+ Mobile Token allows access to ALCF systems. This security mobile token uses one-time passwords combined with your PIN for controlled access to the login systems. The mobile token utilizes an app that is keyed to your user account and for which you are responsible on your Android, iPhone or Windows mobile device. Please safeguard your phone as you would your credit cards or house keys: Do not store username, PIN, or other account-related records with the token. Sharing of mobile tokens is strictly forbidden. A mobile token can be associated with a single device only.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#step-1-download-the-safenet-mobilepass-app-for-your-device","title":"Step 1. Download the SafeNet MobilePass+ app for your device:","text":"<p>The SafeNet MobilePASS+ app turns your mobile phone into a two-factor authentication device, removing the need to carry an additional hardware token. As a SafeNet MobilePASS+ user, you can generate passcodes on your mobile device and use those passcodes to authenticate on ALCF computing resources. See supported OS and platforms for more information.</p> <p>SafeNet MobilePass+ for Android can be found here: https://play.google.com/store/apps/details?id=com.gemalto.mpassplus</p> <p>SafeNet MobilePass+ for iPhone can be found here: https://itunes.apple.com/us/app/safenet-mobilepass/id1056481326?mt=8</p> <p>SafeNet MobilePass+ for Windows can be found here: https://www.microsoft.com/en-us/p/safenet mobilepass/9nblggh10pdq?activetab=pivot%3Aoverviewtab</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#step-2-enroll-your-mobilepass-mobile-token","title":"Step 2. Enroll your MobilePass+ mobile token:","text":"<p>After you\u2019ve been provisioned a mobile token, you will receive a notification email with the subject line \"ALCF Mobile Token Self-Enrollment\" which you must access from your mobile phone.</p> <p>Auto-Enrollment (to enroll SafeNet MobilePass+ token automatically):</p> <ol> <li>Click on the http:// link in the email. The SafeNet Authentication Service Self-Enrollment will open.</li> <li>Click enroll your SafeNet MobilePass+ token.</li> <li>When prompted to open in MobilePass+ tap Open.</li> <li>You will now be prompted to enter a 6 digit all numeric PIN.</li> <li>Enter your PIN in the Token PIN field and repeat in the Confirm PIN field.</li> <li>You will be taken to the Enrollment Complete screen to name the token.</li> <li>Insert the desired name in the Token Name field or leave it as is. This name is not utilized by the server; it is for you only.</li> <li>The newly enrolled SafeNet MobilePass+ token is now displayed in the SafeNet MobilePass+ app.</li> </ol> <p>Manual Enrollment:</p> <ol> <li>Copy the activation string from the SafeNet provision email.</li> <li>Open the SafeNet MobilePass+ app and tap the manual option.</li> <li>Paste the enrollment string into the field provided and tap the Enroll button.</li> <li>You will now be prompted to enter a 6 all numeric PIN.</li> <li>Enter your PIN in the Token PIN field and repeat in the Confirm PIN field.</li> <li>You will be taken to the Enrollment Complete screen to name the token.</li> <li>Insert the desired name in the Token Name field or leave it as is. This name is not utilized by the server; it is for you only.</li> </ol>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#logging-in-to-an-alcf-system-using-a-mobile-token","title":"Logging in to an ALCF System using a Mobile Token","text":"<ol> <li>Open the MobilePASS+ (MobilePASS for Windows) app on your device. Then initiate an SSH session and type the following:</li> </ol> <pre><code>ssh &lt;ALCF username&gt;@&lt;system_name&gt;.alcf.anl.gov\n</code></pre> <ol> <li> <p>When prompted for a password, click the SafeNet MobilePASS+ app on your phone. Click on the token name listed within the app, and enter your PIN.</p> </li> <li> <p>The app will display your passcode immediately.  Enter the passcode as the login password for the system within the SSH session. Please Note: You do NOT have to enter the PIN on the SSH screen when logging into a resource. This only needs to be done to access the passcode within the SafeNet MobilePASS+ (MobilePASS for Windows) app.</p> </li> <li> <p>Each generated passcode is valid on the SafeNet MobilePass+ app window until your mobile device screen times out.</p> </li> </ol>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#troubleshooting-your-mobile-token","title":"Troubleshooting your Mobile Token","text":"<p>Case 1: Forgotten PIN: If you enter a PIN for your mobile Token and you get an invalid PIN, you will be asked to re-enter your PIN. After 6 failed attempts your token will be deleted and you will need to call the ALCF help desk or send an email to ALCF support to have a new mobile token provisioned.</p> <p>Case 2: Account Lockout: If you fail to enter the correct password 6 times, you will get a permission denied error on the SSH screen. Upon 4 more failed attempts, your IP will be blocked. You will need to call the ALCF help desk and submit a ticket to have the IP unblocked.</p> <p>Case 3: PIN Change: While logged in to the mobile token, click on token settings then tap change PIN. Enter the current PIN followed by the new PIN and confirm.</p> <p>Case 4: Re-Sync: If you are unable to log in to a resource after entering the correct PIN and passcode your token may be out of sync with the server. Please email ALCF Service Desk at support at alcf.anl.gov for assistance.</p> <p>Case 5: New Mobile Device: If you have a new mobile device, please email the ALCF Service Desk at support at alcf.anl.gov to have a new mobile token provisioned. </p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#physical-token","title":"Physical Token","text":"<p>The physical token allows access to the ALCF systems. This security token uses one-time passwords combined with your PIN for controlled access to the login systems. The physical token is a tracked asset for which you are responsible and is keyed to your use. Please safeguard your token as you would your credit cards or house keys: Do not store username, PIN, or other account-related records with the token. Sharing of tokens is strictly forbidden. Please do not mark on the token or alter it in any way.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#enabling-your-alcf-physical-token","title":"Enabling Your ALCF Physical Token","text":"<p>Upon receipt of CRYPTOCard token, contact support@alcf.anl.gov must verify identity and activate token. If this step is not performed the CRYPTOCard token will not be able to log on to the ALCF resource.</p> <p>ALCF Support Desk Info Hours: Monday-Friday 9 a.m. - 5 p.m. (Central time); Email: support@alcf.anl.gov</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#logging-in-to-an-alcf-system-using-a-physical-token","title":"Logging in to an ALCF System using a Physical Token","text":"<p>When the physical token is activated, an initial PIN will be provided. This will be a four-digit number that will prepend to the one-time password string generated by the token.</p> <p>Upon INITIAL login (to one of the ALCF machines), a prompt to change the PIN will appear. PINs must be at least four characters long and must only contain numbers.</p> <ol> <li> <p>Initiate an SSH session using: <pre><code>ssh &lt;ALCF username&gt;@&lt;system_name&gt;.alcf.anl.gov\n</code></pre></p> </li> <li> <p>A password prompt will be received. At this point, push the button on the physical token once.</p> </li> <li> <p>An eight-character, one-time password made up of letters and numbers will appear on the token\u2019s display. This one-time password is case-sensitive.</p> </li> <li> <p>Type your PIN followed immediately by the one-time password at the SSH password prompt.</p> </li> </ol> <p>For example, if your PIN is 1234 and you received the one-time password string ABCD9876, you would type 1234ABCD9876 at the password prompt.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#troubleshooting-your-physical-token","title":"Troubleshooting Your Physical Token","text":"<p>Case 1: It says \"locked\": The physical token may be locked due to too many failed attempts. Please contact the ALCF Help Desk to return the defective token and so a replacement can be sent.</p> <p>Case 2: You have a PIN for your physical token: Once a PIN has been set for your physical token, you will need to prepend your PIN to the token password. Otherwise you will not be able to log in. If you do not remember your PIN, please email us so we can verify your identity and reset your Initial PIN.</p> <p>Case 3: It does not say \"locked\" but still does not work: It is likely that your token has fallen out of sync with the server. If you have pushed the button on your physical token more than 10 times without successfully logging in, it will fail to authenticate because it has lost synchronization with the server. Please try connecting to Theta first. If it still fails, please follow the re-sync instructions below.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#re-sync-instructions","title":"Re-Sync Instructions","text":"<p>If you have pushed the button on your physical token more than 10 times, it will fail to authenticate because it has lost synchronization with the server. You can re-synchronize your token using the following procedure:</p> <pre><code>1. Have your physical token ready.\n\n2. Obtain a challenge sequence:\n    - Initiate an SSH session to a host that allows token\n      authentication (such as theta.alcf.anl.gov). At the password\n      prompt, just hit 'Enter'. This will cause the Cryptocard service\n      to produce a challenge string consisting of 8 numbers.\n\n3. Hold down the button on your token for a few seconds until the\n    display says \"Init\", then let go.\n\n4. The token will scroll through a series of menu options. When it\n    displays \"ReSync\", hit the button again.\n\n5. The display will say\n\n     Resync?0\n\n6. The number at the end will start cycling from 0 to 9, over and over.\n\n7. Look at the numbers in your challenge string. When the number\n    displayed on your token changes to the first number of the challenge\n    string, press the button. The display will now show this number, and\n    the second digit will start cycling.\n\n8. Enter each of the numbers from your challenge string in the same\n    manner, until the display on your token matches the entire challenge string.\n    Choose the \"&lt;\" to backspace and re-enter the previous number if\n    necessary.\n\n9. Once you've entered all 8 digits, re-check to make sure they're\n    accurate. Then, while all 8 digits are displayed on the token, press\n    the button to generate a new password.\n\n10. Enter your PIN followed by the new password, and hit 'Enter'. \n     If successful, you will be logged in to the resource. You're now back \n     in sync with the authentication server.\n\nIf you are unsuccessful, you will be presented with another challenge string. \nAt this point, you may need to perform the re-sync instructions again.\n</code></pre> <p>If there are still problems after completing the re-synchronization procedures, please email us at support@alcf.anl.gov so we can run a test on the physical token to determine if it is defective. </p> <p>If it is found to be defective we will promptly replace it. Physical tokens are the property of Argonne National Laboratory.</p> <p>Please return them to us at:</p> <pre><code>ALCF Help Desk\nArgonne National Laboratory\n9700 S. Cass Ave.\nBldg. 240, Rm. 2129\nLemont, IL 60439\n</code></pre>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#resetting-the-physical-token-pin","title":"Resetting the Physical Token PIN","text":"<p>Please email us at support at alcf.anl.gov for PIN resets. Once your identity has been verified, we will provide you with a new PIN for your CRYPTOcard token.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#returning-a-physical-token","title":"Returning a Physical Token","text":"<p>If you no longer need your physical token, please return it to this address:</p> <pre><code>ALCF Help Desk\nArgonne National Laboratory\n9700 S. Cass Ave.\nBldg. 240, Rm. 2129\nLemont, IL 60439\n</code></pre>"},{"location":"account-project-management/accounts-and-access/user-account-overview/","title":"ALCF User Account Overview","text":"<p>All computing carried out on the ALCF systems is associated with a user \"account.\" This account is used to log onto the login servers and run jobs on the resources. If someone has a user account, then he or she has a login name that is recorded in the user database. This web page describes the process that users will need to understand to manage account details, including policies and procedures.</p> <p>If you need an account, visit the Accounts and Project Management website: Request an account</p> <p>If you want to learn how to get started, visit the Get Started Guide: Get Started Guide</p>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#who-can-get-an-account","title":"Who Can Get an Account","text":"<p>Those who are interested in having an account on a ALCF resource must first request an allocation and provide a detailed description of the work, including computational requirements and coding capabilities for the Blue Gene platform. Another means of acquiring an allocation on the ALCF system is to be part of a project team that already has an active allocation. Once an allocation has been granted, new users should complete an account request. A project\u2019s Principal Investigator (PI) must sponsor these accounts\u2014if the PI is the user, an ALCF staff member must serve as sponsor. Sponsors are asked annually to evaluate the accounts they have sponsored to determine whether or not these accounts should be kept active.</p>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#account-abilities","title":"Account Abilities","text":"<p>A user with an active account can login to the ALCF login servers (e.g theta.alcf.anl.gov or cooley.alcf.anl.gov.) This account will have some home directory space, where file transfer can occur from that space via the login nodes, and where development activities, such as editing and compiling, can also occur.</p>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#account-states","title":"Account States","text":"<p>Accounts are classified in one of the following categories:</p> <ul> <li>Pending: An account that has been requested but has not yet been created.</li> <li>Active: An account that can be used to interact with the ALCF Login Servers. This is the normal state for all accounts.</li> <li>Inactive: An account that still exists on the system (that is, the account continues to be registered in the database and the user's files exist on disk) but the user cannot interact with the ALCF Login Servers. An account might be disabled due to misuse, security concerns, or because it is no longer allocated.</li> <li>Deleted: An account that existed on the system and is thus in the records and backups, but whose user no longer has access to the systems or files on disk.</li> </ul>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#more-information","title":"More Information","text":"<ul> <li>Account Policy</li> <li>User Authentication Policy</li> <li>Account Sponsorship and Retention Policy</li> </ul>"},{"location":"account-project-management/allocation-management/allocation-management/","title":"Managing Your Allocations","text":"<p>Allocations require management \u2013 balance checks, resource allocation, requesting more time, etc.</p>"},{"location":"account-project-management/allocation-management/allocation-management/#checking-for-an-active-allocation","title":"Checking for an Active Allocation","text":"<p>To determine if there is an active allocation, check Job Submission.</p> <p>For information on how to run the query, look at our documentation on our sbank Allocations Accounting System or email support@alcf.anl.gov and ask for all active allocations.</p>"},{"location":"account-project-management/allocation-management/allocation-management/#using-sbank-to-determine-the-balance-of-an-allocation","title":"Using sbank to Determine the Balance of an Allocation","text":"<p>To determine which platforms have an active balance, check our allocation accounting system sbank.</p> <ul> <li>To obtain the allocation balance, check the sbank command sbank-list-allocations.</li> <li>DD projects with a negative balance will not be able to run jobs until they have requested additional time, see Getting more time below.</li> <li>INCITE and ALCC PIs automatically email a summary of project usage.  If this is a DD project, please email support@alcf.anl.gov.</li> </ul>"},{"location":"account-project-management/allocation-management/allocation-management/#allocation-expiration","title":"Allocation Expiration","text":"<p>Projects and allocations at the ALCF are different.  A particular project might have multiple allocations of time. For example, a discretionary project that has been approved for more than 3 times will have 3 allocations (2 are probably expired) but just one project. Projects will not expire, allocations will. If allocations are expired, or have no hours left, jobs will not be able to run. Use the two bullets above (Checking for an active allocation and Determining the balance of an allocation) to determine active allocations.</p>"},{"location":"account-project-management/allocation-management/allocation-management/#getting-more-time","title":"Getting More Time","text":"<p>To request an extension of your existing discretionary allocation or to request additional hours, please email support@alcf.anl.gov with answers to the following:</p> <ul> <li>What you have accomplished with your original allocation?</li> <li>Please include a brief description of any publications or major presentations that were (or will be) generated in full or in part because of this allocation.</li> <li>What you will do with the extra time?</li> <li>What you are requesting as your new expiration date?</li> <li>How many additional hours you are requesting?</li> </ul>"},{"location":"account-project-management/allocation-management/allocation-management/#sub-allocations","title":"Sub-allocations","text":"<p>Suballocations let PIs control who in their team can runs jobs, how much they are allowed to consume (allocation amount), and when they are allowed to run jobs (start and end dates)</p> <p>Step 1: Create Suballocations (Project PI):</p> <p>PI creates suballocations </p> <p><code>sbank new sub &lt;allocationid&gt; **-name &lt;nameofsuballoc&gt;</code></p> <p>Tip: see <code>sbank new suballocation -h</code> for all the options. </p> <p>Step 2: Manage Suballocations (Project PI)</p> <p>PI adds users to suballocations</p> <p><code>sbank e sub &lt;projectname&gt;::&lt;nameofsuballoc&gt; --add-user=\"&lt;username1&gt; &lt;username2&gt; ...\"</code></p> <p>PI can change the name of a suballocation </p> <p><code>sbank e sub &lt;suballocationID&gt; --name=&lt;new_name_of_suballocation&gt;</code></p> <p>By default, the primary suballocation (which is the default suballocation created when the allocation is created by ALCF) is unrestricted .i.e. enabled for all project members.  That means all project members can submit jobs against the primary suballocation by default. All other suballocations are restricted by default and users have to be added for each of them.</p> <p>To change the default for the primary suballocation to restrict usage, PI must first edit the suballocation:</p> <p><code>sbank-edit-suballocation --restrict &lt;primary suballocation id&gt;</code></p> <p>Then add users with this command:</p> <p><code>sbank e sub &lt;primary suballocation id&gt;  --add-user=\"&lt;username1&gt; &lt;username2&gt; ...\"</code></p> <p>PI changes start and end dates for a suballocation:</p> <p><code>sbank e sub &lt;suallocationID&gt; -S &lt;start_date&gt; -E &lt;end_date&gt;</code></p> <p>PI adds hours to a suballocation:</p> <p><code>sbank e sub &lt;projectname&gt;::&lt;nameofsuballoc&gt; --hours-to-move &lt;hours&gt; --to-suballocation &lt;projectname&gt;::&lt;nameofsuballoc2&gt;</code></p> <p>Note: <code>hourstomove</code> must be greater than or equal to the available balance for the suballocation <code>nameofsuballoc</code></p> <p>Tip: see <code>sbank e suballocation -h</code> for all the options</p> <p>Step 3: Submit Jobs  (Project team)</p> <p>Submit jobs to a suballocation. Note that the user should be on the suballocation\u2019s user list </p> <p><code>Eg: qsub -l select=10,walltime=30:00,filesystems=grand:home -A &lt;suballoctionID&gt; -q demand test.sh</code></p> <p>Note: Once submanagement is enabled for a project allocation, all job submissions must specify the <code>suballocationID</code></p> <p>Useful commands: List all suballocations for a project that shows number of jobs run, charges, allocation balance, suballocation name, and list of users</p> <p><code>sbank-list-allocations -r polaris -p &lt;projectname&gt; -f\u201d+subname users_list\u201d</code></p> <p>Tip: see <code>sbank l a -h</code> for all the options and <code>sbank \u2013f\\?</code> for list of fields that can be displayed </p>"},{"location":"account-project-management/allocation-management/overview/","title":"Allocations on ALCF Computing Resources","text":""},{"location":"account-project-management/allocation-management/overview/#getting-an-allocation-award","title":"Getting an Allocation Award","text":""},{"location":"account-project-management/allocation-management/overview/#incite-alcc-and-adsp","title":"INCITE, ALCC, and ADSP","text":"<p>Researchers gain access to ALCF systems for computational science and engineering projects\u2014typically with awards of millions of core-hours\u2014through competitive, peer-reviewed allocation programs supported by the DOE and Argonne. Our peer-reviewed award programs consist of the INCITE, ALCC, and ADSP programs. More information about the programs, including dates for our CFPs, can be found on their web pages.</p>"},{"location":"account-project-management/allocation-management/overview/#directors-discretionary","title":"Director's Discretionary","text":"<p>Alternatively, ALCF offers a Director's Discretionary allocation award program to leadership computing preparation, INCITE and ALCC scaling, and application performance to maximize scientific application efficiency and productivity on leadership computing platforms. See the Director's Discretionary (DD) Program page for more information.</p>"},{"location":"account-project-management/allocation-management/overview/#initializing-your-awarded-allocation","title":"Initializing Your Awarded Allocation","text":"<p>Projects with INCITE, ALCC, and ADSP awards will be contacted directly by the ALCF staff with information on creating accounts.</p> <p>Director's Discretionary awards will receive information in the award confirmation email. </p>"},{"location":"account-project-management/allocation-management/overview/#allocation-resources","title":"Allocation Resources","text":"<p>While requesting an allocation, users can choose from:</p> <p>Computes:  - Polaris - Theta (KNL Node) - ThetaGPU (GPU Node) - Cooley</p> <p>File System:  - Grand - Eagle (Community Sharing)</p>"},{"location":"account-project-management/allocation-management/overview/#policy-information-related-to-allocations","title":"Policy Information Related to Allocations","text":"<p>Pullback Policy</p>"},{"location":"account-project-management/allocation-management/overview/#requesting-additional-allocation-hours","title":"Requesting Additional Allocation Hours","text":"<p>If you are a PI of a Director's Discretionary project that has an active allocation, you can request additional time or an extension using the allocation request form.</p> <p> </p> To request more hours, renew your project using the allocation request form."},{"location":"account-project-management/allocation-management/sbank-allocation-accounting-system/","title":"sbank Allocation Accounting System","text":"<p>sbank is the accounting system used within the ALCF. It tracks project allocations, usage charges, and refunds. sbank allows queries about the balance and expiration of project allocations, and has replaced the outdated cbank accounting system.</p> <p>The sbank accounting system helps users manage their allocations and usage per job. It gives the PIs the ability to monitor their allocation usage by user, job, and machine. It also allows the user to monitor their usage per allocation and provides insight on how many hours are left on the project.</p>"},{"location":"account-project-management/allocation-management/sbank-allocation-accounting-system/#getting-started-with-sbank","title":"Getting Started with sbank","text":"<p>sbank Example Commands provides a set of example commands on how to use the most common commands. </p>"},{"location":"account-project-management/allocation-management/sbank-allocation-accounting-system/#sbank-man-pages","title":"sbank Man Pages","text":"<p>Use these sbank man pages to get information on how to use the commands.</p> <ul> <li>sbank</li> <li>sbank-detail</li> <li>sbank-detail-allocations</li> <li>sbank-detail-jobs</li> <li>sbank-detail-projects</li> <li>sbank-detail-transactions</li> <li>sbank-detail-users</li> <li>sbank-list</li> <li>sbank-list-allocations</li> <li>sbank-list-jobs</li> <li>sbank-list-projects</li> <li>sbank-list-transactions</li> <li>sbank-list-users</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/","title":"Manpage for sbank-detail-allocations","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#sbank-detail-allocations-options","title":"sbank-detail-allocations [options] [ ... ] <p>Detail allocation information. </p> <p>NOTE:    1. The list of  arguments are optional.    2. you can also enter  list by using the -a option multiple times.    3. regardless, both are optional, and you can get detail allocation info using the option filters below.","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>filter on event id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>filter on jobid</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>filter on transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width=","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>**Date Parsing Precedence: **</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions), ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>also get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-o-get-only-inactive","title":"-O, --get-only-inactive","text":"<p>only inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-award-type-nameaward_type_name","title":"--award-type-name=AWARD_TYPE_NAME","text":"<p>filter on award type name</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-award-categoryaward_category","title":"--award-category=AWARD_CATEGORY","text":"<p>filter on award category</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>filter on Clusterbank reference id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-createdcreated_timestamp","title":"--created=CREATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-get-deleted","title":"--get-deleted","text":"<p>also get deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-get-only-deleted","title":"--get-only-deleted","text":"<p>only deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-history-date-rangeend","title":"--history-date-range=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-last-updatedlast_updated_timestamp","title":"--last-updated=LAST_UPDATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults: </p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-history","title":"--no-history","text":"<p>do not show history information</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-jobs/","title":"sbank-detail-jobs","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-jobs/#sbank-detail-jobs-options","title":"sbank-detail-jobs [options] [  |  ...  | ] <p>Detail job information.  NOTE: </p> <ol> <li>The arguments  or  are NOT REQUIRED;  <li>event_id is the JOB DATABASE ID; </li> <li> is the SCHEDULER CREATED ID, such as Cobalt;  <li> can also be entered using option -j  ;  <li> can also be entered using option -e  ;  <li> can also be entered using option -r  ;  <li>regardless, you can use options or arguments to get detail job information</li>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-jobs/#options","title":"OPTIONS","text":"<p>--version</p> <p>show program's version number and exit</p> <p>-h, --help</p> <p>show this help message and exit</p> <p>-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID</p> <p>filter on allocation id</p> <p>-e EVENT_ID, --event-id=EVENT_ID</p> <p>filter on event id</p> <p>-f FIELD_INFO, --field-to-display=FIELD_INFO</p> <p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\" <p>-j JOBID, --jobid=JOBID</p> <p>filter on jobid</p> <p>-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY</p> <p>set number of fields to display</p> <p>-p PROJECT, --project=PROJECT</p> <p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p> <p>-r RESOURCE, --resource=RESOURCE</p> <p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p> <p>-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID</p> <p>filter on transaction id</p> <p>-u USER, --user=USER</p> <p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p> <p>-w \"FIELD_INFO\", --field-width</p> <p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\" <p>-E END, --end=END</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>-H, --human-readable</p> <p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p> <p>-S START, --start=START</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;,&lt;=, &lt;, == . Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE</p> <p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p> <p>--created=CREATED_TIMESTAMP</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>--debug=DEBUG_LEVEL</p> <p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p> <p>--eligible=ELIGIBLE_TIMESTAMP</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>--get-not-charged</p> <p>only un-charged jobs</p> <p>--history-date-range=END</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>--last-updated=LAST_UPDATED_TIMESTAMP</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'gt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>--no-commas</p> <p>remove commas from comma separated thousands</p> <p>--no-header</p> <p>do not display the header</p> <p>--no-history</p> <p>do not show history information</p> <p>--no-rows</p> <p>do not display the row data</p> <p>--no-sys-msg</p> <p>do not display system message</p> <p>--no-totals</p> <p>do not display the totals</p> <p>--queued=QUEUED_TIMESTAMP</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/","title":"Manpage for sbank-detail-projects","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#sbank-detail-projects-options","title":"sbank-detail-projects [options] [ ... ] <p>Detail project information. </p> <p>NOTE:    1. The list of  arguments are optional   2. you can also enter  list by using the -p option multiple times   3. regardless, both are optional, and you can get detail project info using the option filters below","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:    - YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/","title":"Manpage for sbank-detail-transactions","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#sbank-detail-transactions-options","title":"sbank-detail-transactions [options] [ ... ] <p>Detail transaction information. </p> <p>NOTE:    1. The list of  arguments are optional   2. you can also enter  list by using the -t option multiple times   3. regardless, both are optional, and you can get detail transaction info using the option filters below","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-c-comment","title":"-c, --comment","text":"<p>display comment</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>filter on event id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:] for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>filter on jobid</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>filter on transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width=","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-e-job_end-endjob_end","title":"-E JOB_END, --end=JOB_END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-s-job_start-startjob_start","title":"-S JOB_START, --start=JOB_START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-attransaction_at_timestamp","title":"--at=TRANSACTION_AT_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>filter on Clusterbank reference id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-createdjob_created_timestamp","title":"--created=JOB_CREATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-queuedjob_queued_timestamp","title":"--queued=JOB_QUEUED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/","title":"Manpage for sbank-detail-users","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#sbank-detail-users-options","title":"sbank-detail-users [options] [ ... ] <p>Detail user information. </p> <p>**NOTE: **   1. Use -I to include inactive allocations   2. the list of  arguments are optional   3. you can also enter  list by using the -u option multiple times   4. regardless, both are optional, and you can get detail user info using the option filters below","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/","title":"Manpage for sbank-detail","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#sbank-detail-options","title":"sbank-detail  [options] <p>Detail Meta Command</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#commands","title":"COMMANDS","text":"<ul> <li>allocations [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) </li> <li>categories [-f|-n|-w|...] </li> <li>messages [-f|-n|-w|...] </li> <li>names [-f|-n|-w|...] jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] </li> <li>projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] </li> <li>transactions [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] </li> <li>users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-a-allocation","title":"-a --allocation","text":"<p>enter allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-c-comment","title":"-c --comment","text":"<p>enter comment for new or edit commands, display comment for list commands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-e-event-id","title":"-e --event-id","text":"<p>enter event db id; event db id is an internal id created by the charging system</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-f-field","title":"-f --field","text":"<p>enter [:], width is optional; enter -f? or -f \"?\" for available fields, + to add fields"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-h-help","title":"-h --help","text":"<p>command line help</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-j-jobid","title":"-j --jobid","text":"<p>enter jobid; jobid is created by the scheduler and is not unique</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-n-num-field","title":"-n --num-field","text":"<p>enter number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-p-project","title":"-p --project","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-r-resource","title":"-r --resource","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-s-suballocation","title":"-s --suballocation","text":"<p>enter suballocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-t-transaction","title":"-t --transaction","text":"<p>enter transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-u-user","title":"-u --user","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-w-field-width","title":"-w --field-width","text":"<p>enter the field width as follows: :, enter -w? or -w \"?\" for available fields"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-e-end","title":"-E --end","text":"<p>enter end datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-h-human-readable","title":"-H --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-i-get-inactive","title":"-I --get-inactive","text":"<p>include inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-o-get-only-inactive","title":"-O --get-only-inactive","text":"<p>get only inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-s-start","title":"-S --start","text":"<p>enter start datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-t-type","title":"-T --Type","text":"<p>enter type of transaction</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-all-charges","title":"--all-charges","text":"<p>for list allocations | projects | users, only show info with charges</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-at","title":"--at","text":"<p>enter transaction created datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-award-category","title":"--award-category","text":"<p>enter allocation award category</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-award-type-name","title":"--award-type-name","text":"<p>enter allocation award-type name</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-created","title":"--created","text":"<p>enter created datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-debug","title":"--debug","text":"<p>enter debug level</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-get-deleted","title":"--get-deleted","text":"<p>get deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-get-not-charged","title":"--get-not-charged","text":"<p>get jobs that have not been charged</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-get-only-deleted","title":"--get-only-deleted","text":"<p>get only deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-history-date-range","title":"--history-date-range","text":"<p>enter history datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-last-updated","title":"--last-updated","text":"<p>enter last updated datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-header","title":"--no-header","text":"<p>do not display header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-history","title":"--no-history","text":"<p>do not display history information</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-rows","title":"--no-rows","text":"<p>do not display rows</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-totals","title":"--no-totals","text":"<p>do not display totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-queued","title":"--queued","text":"<p>enter queued datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/","title":"sbank Example Commands","text":"<p>Below is a set of helpful commands to help you better manage the projects you have running at the ALCF.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#view-your-projects-allocations","title":"View your project's allocations","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#command-sbank-list-allocations","title":"Command: sbank-list-allocations","text":"<p>Use this command to list all of your active allocations for a specific project [Project-X]. This is useful when you need to provide this information in a report. <pre><code>&gt; sbank-list-allocations -p ProjectX -r all\n Id         Start       End         Resource   Project          Jobs        Charged          Available Balance \n ---------  ----------  ----------  ---------  ---------------  ----------  ---------------  ----------------- \n 2106       2016-01-04  2017-01-01  cooley     ProjectX              1,139          6,032.8           43,967.2 \n 2146       2016-01-14  2017-01-10  theta      ProjectX                983      1,084,770.3       25,483,927.5\n 6438       2020-09-22  2022-01-01  thetagpu   ProjectX                  3              0.0            2,000.0 \n\nTotals:\n  Rows: 3\n  Cooley:\n    Available Balance: 43,967.2 node hours\n    Charged          : 6,032.8 node hours\n    Jobs             : 1,139 \n Theta:\n    Available Balance: 25,483,927.5 node hours \n    Charged          : 1,084,770.3 node hours \n    Jobs             : 983 \n Thetagpu:\n    Available Balance: 2,000.0 node hours\n    Charged          : 0.0 node hours\n    Jobs             : 3 \n</code></pre></p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-your-projects-quota-on-grand-andor-eagle-file-system","title":"List your project's quota on Grand and/or Eagle File system","text":"<pre><code>&gt; sbank-list-allocations -p ProjectX -r grand\n Allocation  Suballocation  Start       End         Resource  Project      Quota\n ----------  -------------  ----------  ----------  --------  -----------  -----\n 6687        6555           2020-12-16  2022-01-01  grand     ProjectX    1.0\n\nTotals:\n  Rows: 1\n  Grand:\n    Quota: 1.0 TB\n\n&gt; sbank-list-allocations -p ProjectX -r eagle\n Allocation  Suballocation  Start       End         Resource  Project      Quota\n ----------  -------------  ----------  ----------  --------  -----------  -----\n 6688        6556           2020-12-16  2022-01-01  eagle     ProjectX    1.0\n\nTotals:\n  Rows: 1\n  Eagle:\n    Quota: 1.0 TB\n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-only-the-created-timestamp-field-for-all-allocations-that-were-created-before-01-01-2015-for-projectx-accross-all-resources","title":"List only the created timestamp field for all allocations that were created before 01-01-2015 for ProjectX accross all resources","text":"<pre><code>&gt; sbank-list-allocations  --created \"&lt;20150101\" -r all -p ProjectX \"-f created\"\n Created    \n ---------- \n 2016-01-04 \n 2016-01-14 \n 2016-01-15 \n\nTotals:\n  Rows: 3\nDate  filters (UTC): created &lt; \"2015-01-01 00:00:00\",  \n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-all-active-allocations-for-all-resources-for-project-projectx-and-add-the-field-created-to-the-display-list","title":"List all active allocations for all resources for project ProjectX and add the field Created \ufeffto the display list","text":"<pre><code>shrubbery~ &gt; sbank-list-allocations -r all  -p ProjectX -f \"+created\"\n Id         Start       End         Resource   Project          Jobs        Charged          Available Balance  Created    \n ---------  ----------  ----------  ---------  ---------------  ----------  ---------------  -----------------  ---------- \n 279        2011-08-30  2020-01-01  theta      ProjectX              6,361     12,332,699.9      -12,332,699.9  2013-02-22 \n 2106       2016-01-04  2017-01-01  cooley     ProjectX              1,150          6,080.9           43,919.1  2016-01-04  \n\nTotals:\n  Rows: 2\n  Theta:\n    Available Balance: -12,332,699.9 node hours\n    Charged          : 12,332,699.9 node hours\n    Jobs             : 6,361 \n  Cooley:\n    Available Balance: 43,919.1 node hours\n    Charged          : 6,080.9 node hours\n    Jobs             : 1,150 \n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-all-available-fields-for-the-sbank-list-allocations-command","title":"List all available fields for the sbank-list-allocations command","text":"<pre><code>&gt; sbank-list-allocations  -f \"?\"\navailable fields:\n id\n start_timestamp\n end_timestamp\n resource\n project_name\n jobs_count\n charged_sum\n available_balance_sum\n created_timestamp\n award_category\n award_type_name\n admin_name\n cbank_ref\n comment\n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#view-your-projects-users","title":"View your project's users","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#command-sbank-list-users","title":"Command: sbank-list-users","text":"<p>List all charges for userx on theta on project ProjectX <pre><code>&gt; sbank-list-users -p ProjectX -r theta -u userx\n User             Jobs        Charged         \n ---------------  ----------  --------------- \n userx                 1,814          9,884.5\n\nTotals:\n  Rows: 1\n  Resources: theta\n  Charged: 9,884.5 node hours\n  Jobs   : 1,814 \n  ```\n\n### List charges for all users in ProjectX on Cooley.\nThis works for project leads (i.e. PIs, Co-PIs, Proxies), since they can see everything in their own projects.\n</code></pre></p> <p>sbank-list-users -p ProjectX -r theta  User             Jobs        Charged         </p> <p>user1                   120          4,243.7   user2                     0              0.0   user3                     0              0.0   user4                   181          1,195.5   user5                     0              0.0   user6                 2,560         10,868.7   user7                     0              0.0   user8                     0              0.0   user9                     0              0.0   user10                    7              3.5   user11                    0              0.0 </p> <p>Totals:   Rows: 11   Resources: theta   Charged: 16,311.4 node hours   Jobs   : 2,868 </p> <p><pre><code>## View your project's jobs\nList jobs for user \"userx\" for jobs that started in the range 2016-02-15&lt;= started &lt; 2016-02-29 and add the transactions related to the job\n\n### **Command:** sbank-list-jobs\n\n**Note:** The job with the refund ```transaction_ids_list field can be shorten all the way to \"t\" in the -f \"+ t\"```\n</code></pre> shrubbery~ &gt; sbank-list-jobs -u userx -f \"+ t\" -S \"2016-02-15...2016-02-29\"  Id         Jobid      Resource   Project          Allocation  User       Duration   Charged          Transaction Ids </p> <p>1013857    730417     theta       ProjectX         1740        userx      1:53:07           61,776.8  CHARGE-1011230  1013860    730558     theta       ProjectX         1740        userx      1:53:07           61,776.8  CHARGE-1011233  1014168    730668     theta       ProjectX         1740        userx      1:53:25           61,940.6  CHARGE-1011541  </p> <p>Totals:   Rows: 3   Theta:     Charged      : 185,494.2 node hours     Duration     : 6:44:00  Date  filters (UTC): \"2016-02-15 00:00:00\" &lt;= start &lt; \"2016-02-29 00:00:00\", <pre><code>### List the nodes used, runtime and start timestamp for Cooley job 744160\n**Note**: To display the date and time we increased the the number of characters of start_timestamp to 19\n</code></pre> catapult~ &gt; sbank l j -r theta -j 50576 -f \"jobid nodes_used runtime start_timestamp:19\" Jobid Nodes Used Runtime Start --------- ---------- --------- ------------------- 50576 512 1:00:49 2013-01-16 21:49:30 Totals: Rows: 1 <pre><code>## View your project's transactions\n### **Command:** sbank-list-transactions\n\nList of transactions that where at or after 2016-02-29 for ProjectX add fields: job_duration, nodes_used and hosts\n\n**Note**: \n- job_duration, nodes_used and hosts are shorten, but they are still uniquely identified\n- host has the left justified width of 20, specified as \"h:-20\"\n</code></pre> catapult~ &gt; sbank-list-transactions -p ProjectX --at \"ge 2016-02-29\" -f \"+ job_d nodes_u h:-20\" -r theta  Id         Resource   Project          Allocation  At          User             Transaction Type  Amount           Jobid      Job Duration  Nodes Used  Hosts                </p> <p>1025426    theta       ProjectX         2147        2016-02-29  userx            CHARGE                   48,005.1  740587     1:27:54       2048        MIR-00800-33BF1-2048   1028046    theta       ProjectX         2147        2016-03-01  userx            CHARGE                  147,647.1  742090     4:30:21       2048        MIR-40000-733F1-2048   1028755    theta       ProjectX         2147        2016-03-02  userx            CHARGE                1,576,068.0  742126     6:00:44       16384       MIR-04000-77FF1-1638 </p> <p>Totals:   Rows: 3   Theta:     Charges Amount: 1,771,720.2 node hours     Job Duration  : 11:58:98  Date  filters (UTC) : at &gt;= \"2016-02-29 00:00:00\", ```</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/","title":"Manpage for sbank-list-allocations","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#sbank-list-allocations-options","title":"sbank-list-allocations [options]","text":"<p>Generate allocation list report. </p> <p>Notes:  1. Use -I to include inactive allocations 2. enter \"-r all\" to get information for all resources</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-c-comment","title":"-c, --comment","text":"<p>display comment</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>filter on event id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>filter on jobid</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>filter on transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-o-get-only-inactive","title":"-O, --get-only-inactive","text":"<p>only inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-award-type-nameaward_type_name","title":"--award-type-name=AWARD_TYPE_NAME","text":"<p>filter on award-type name</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-award-categoryaward_category","title":"--award-category=AWARD_CATEGORY","text":"<p>filter on award category</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>filter on Clusterbank reference id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-createdcreated_timestamp","title":"--created=CREATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-get-deleted","title":"--get-deleted","text":"<p>get deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-get-only-deleted","title":"--get-only-deleted","text":"<p>get only deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-last-updatedlast_updated_timestamp","title":"--last-updated=LAST_UPDATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/","title":"Manpage for sbank-list-jobs","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#sbank-list-jobs-options","title":"sbank-list-jobs [options]","text":"<p>Generate job list report Note: To get information for all resources, enter \"-r all\".</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>filter on event id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>filter on jobid</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>filter on transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-createdcreated_timestamp","title":"--created=CREATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-eligibleeligible_timestamp","title":"--eligible=ELIGIBLE_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-get-not-charged","title":"--get-not-charged","text":"<p>get only jobs that have not been charged</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-last-updatedlast_updated_timestamp","title":"--last-updated=LAST_UPDATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-queuedqueued_timestamp","title":"--queued=QUEUED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/","title":"Manpage for sbank-list-projects","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#sbank-list-projects-options","title":"sbank-list-projects [options]","text":"<p>Generate project list report. </p> <p>Notes: </p> <ol> <li>Use -I to include inactive allocations</li> <li> <ol> <li>to get information for all resources, enter \"-r all\"</li> </ol> </li> </ol>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\" <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/","title":"Manpage for sbank-list-transactions","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#sbank-list-transactions-options","title":"sbank-list-transactions [options]","text":"<p>Generate transaction list report. </p> <p>Note: To get information for all resources, enter \"-r all\".</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-c-comment","title":"-c, --comment","text":"<p>display comment</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>filter on event id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>filter on jobid</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>filter on transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-e-job_end-endjob_end","title":"-E JOB_END, --end=JOB_END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-s-job_start-startjob_start","title":"-S JOB_START, --start=JOB_START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-attransaction_at_timestamp","title":"--at=TRANSACTION_AT_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>filter on Clusterbank reference id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-createdjob_created_timestamp","title":"--created=JOB_CREATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-queuedjob_queued_timestamp","title":"--queued=JOB_QUEUED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/","title":"Manpage for sbank-list-users","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#sbank-list-users-options","title":"sbank-list-users [options]","text":"<p>Generate user list report. </p> <p>Notes: </p> <ol> <li>Use -I to include inactive allocations</li> <li> <ol> <li>for information for all resources, use \"-r all\"</li> </ol> </li> </ol>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>also get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/","title":"Manpage for sbank-list","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#sbank-list-options","title":"sbank-list  [options] <p>List Meta Command</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#commands","title":"COMMANDS","text":"<ul> <li>allocations [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) </li> <li>categories [-f|-n|-w|...] messages [-f|-n|-w|...] names [-f|-n|-w|...] </li> <li>jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] </li> <li>projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] </li> <li>transactions [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] </li> <li>users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-a-allocation","title":"-a --allocation","text":"<p>enter allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-c-comment","title":"-c --comment","text":"<p>enter comment for new or edit commands, display comment for list commands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-e-event-id","title":"-e --event-id","text":"<p>enter event db id; event db id is an internal id created by the charging system</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-f-field","title":"-f --field","text":"<p>enter [:], width is optional; enter -f? or -f \"?\" for available fields, + to add fields"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-h-help","title":"-h --help","text":"<p>command line help</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-j-jobid","title":"-j --jobid","text":"<p>enter jobid; jobid is created by the scheduler and is not unique</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-n-num-field","title":"-n --num-field","text":"<p>enter number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-p-project","title":"-p --project","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-r-resource","title":"-r --resource","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-s-suballocation","title":"-s --suballocation","text":"<p>enter suballocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-t-transaction","title":"-t --transaction","text":"<p>enter transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-u-user","title":"-u --user","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-w-field-width","title":"-w --field-width","text":"<p>enter the field width as follows: :, enter -w? or -w \"?\" for available fields"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-e-end","title":"-E --end","text":"<p>enter end datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-h-human-readable","title":"-H --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-i-get-inactive","title":"-I --get-inactive","text":"<p>include inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-o-get-only-inactive","title":"-O --get-only-inactive","text":"<p>include inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-s-start","title":"-S --start","text":"<p>enter start datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-t-type","title":"-T --Type","text":"<p>enter type of transaction</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-all-charges","title":"--all-charges","text":"<p>for list allocations | projects | users, only show info with charges</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-at","title":"--at","text":"<p>enter transaction-created datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-award-category","title":"--award-category","text":"<p>enter allocation award category</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-award-type-name","title":"--award-type-name","text":"<p>enter allocation award-type name</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-created","title":"--created","text":"<p>enter created datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-debug","title":"--debug","text":"<p>enter debug level</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-get-deleted","title":"--get-deleted","text":"<p>get deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-get-not-charged","title":"--get-not-charged","text":"<p>get jobs that have not been charged</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-get-only-deleted","title":"--get-only-deleted","text":"<p>get only deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-history-date-range","title":"--history-date-range","text":"<p>enter history datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-last-updated","title":"--last-updated","text":"<p>enter last updated datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-header","title":"--no-header","text":"<p>do not display header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-history","title":"--no-history","text":"<p>do not display history information</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-rows","title":"--no-rows","text":"<p>do not display rows</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-totals","title":"--no-totals","text":"<p>do not display totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-queued","title":"--queued","text":"<p>enter queued datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/","title":"Manpage for sbank Commands","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#sbank-options","title":"sbank   [options]","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#description","title":"DESCRIPTION","text":"<p>HPC Accounting System Command Line Interface</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#detail-meta-command","title":"detail meta command","text":"<p>\"detail\" meta command displays information in a long format with history updates, where appropriate.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#list-meta-command","title":"list meta command","text":"<p>\"list\" meta command displays information in a table format, but no history updates are displayed.</p> <p>IMPORTANT NOTES   1. All dates entered shall be interpreted as UTC   2. non-admin users will only be able to see their content (jobs, charges, etc.)   3. project admin users will be able to see all of the content for their projects   4. staff admin users will be able to see all the content   5. --help and -h are the help options.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#meta-commands","title":"META COMMANDS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-detail-options","title":"- detail  [options]","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-list-options-default","title":"- list  [options] (DEFAULT) <p>DETAIL COMMANDS   * allocations [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] [ ... ] (DEFAULT)    * jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] [ ... ]    * projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] [ ... ]    * transactions [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] [ ... ]    * users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...] [ ... ] <p>LIST COMMANDS   * allocations [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT)    * jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...]    * transactions [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...]    * users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-a-allocation","title":"-a --allocation <p>enter allocation id</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-c-comment","title":"-c --comment <p>enter comment for new or edit commands, display comment for list commands</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-e-event-id","title":"-e --event-id <p>enter event db id; event db id is an internal id created by the charging system</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-f-field","title":"-f --field <p>enter [:], width is optional; enter -f? or -f \"?\" for available fields, + to add fields","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-h-help","title":"-h --help <p>command line help</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-j-jobid","title":"-j --jobid <p>enter jobid; jobid is created by the scheduler and is not unique</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-n-num-field","title":"-n --num-field <p>enter number of fields to display</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-p-project","title":"-p --project <p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-r-resource","title":"-r --resource <p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-s-suballocation","title":"-s --suballocation <p>enter suballocation id</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-t-transaction","title":"-t --transaction <p>enter transaction id</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-u-user","title":"-u --user <p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-w-field-width","title":"-w --field-width <p>enter the field width as follows: :, enter -w? or -w \"?\" for available fields","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-e-end","title":"-E --end <p>enter end datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-h-human-readable","title":"-H --human-readable <p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-i-get-inactive","title":"-I --get-inactive <p>include inactive allocations</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-o-get-only-inactive","title":"-O --get-only-inactive <p>get only inactive allocations</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-s-start","title":"-S --start <p>enter start datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-t-type","title":"-T --Type <p>enter type of transaction</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-all-charges","title":"--all-charges <p>for list allocations | projects | users, only show info with charges</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-at","title":"--at <p>enter transaction-created datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-award-category","title":"--award-category <p>enter allocation award category</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-award-type-name","title":"--award-type-name <p>enter allocation award-type name</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-created","title":"--created <p>enter created datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-debug","title":"--debug <p>enter debug level</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-get-deleted","title":"--get-deleted <p>get deleted objects</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-get-not-charged","title":"--get-not-charged <p>get jobs that have not been charged</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-get-only-deleted","title":"--get-only-deleted <p>get only deleted objects</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-history-date-range","title":"--history-date-range <p>enter history datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-home-dir","title":"--home-dir <p>enter the directory to store the pbs meta file</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-ignore-pbs-files","title":"--ignore-pbs-files <p>all new pbs files will be ignored and marked as processed</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-last-updated","title":"--last-updated <p>enter last updated datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-commas","title":"--no-commas <p>remove commas from comma-separated thousands</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-header","title":"--no-header <p>do not display header</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-history","title":"--no-history <p>do not display history information</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-rows","title":"--no-rows <p>do not display rows</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-sys-msg","title":"--no-sys-msg <p>do not display system message</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-totals","title":"--no-totals <p>do not display totals</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-queued","title":"--queued <p>enter queued datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#more-option-explanations","title":"MORE OPTION EXPLANATIONS","text":"<p>For -a, -e, -f, -w, -j, -p, -r, -t, -u, -T, --award-categories, --award_type_names, --cbank_refs options:</p> <p>These options can be entered multiple times for different values or entered once for multiple values. </p> <p>Examples: </p> <ol> <li> <p>sbank-list-allocation -u \"pershey rojas allcock\" or &gt; sbank-list-allocation -u pershey -u rojas -u allcock </p> </li> <li> <p>sbank-list-allocation -f \"id p avail\" or &gt; sbank-list-allocation -f id -f p -f avail For -u, -p and -r the use of wild card \"*\" is allowed, but only on names, not ids: </p> </li> </ol> <p>Examples: </p> <ol> <li>The following command will find allocations for users whose names start with \"pers\" and also users rojas and allcock. &gt; sbank-list-allocation -u \"pers* rojas allcock\" </li> <li>The following command will find allocations for projects that contain \"ratio\" in the name. &gt; sbank-list-allocation -p ratio </li> <li>The following command will find allocations for projects that end with \"tion\" in the name. &gt; sbank-list-allocation -p *tion </li> <li>The following command will find allocations for projects that start with \"ab\" and end with \"ng\" in the name. &gt; sbank-list-allocation -p ab*ng</li> </ol> <p>For -f option: This option is the display field option. </p> <p>To get the available fields enter -f? or -f \"?\". Default fields columns will be displayed if no field option is specified. </p> <p>To replace the current fields to display, enter:  <pre><code>&gt; sbank-list-allocations ... -f \"FIELD[:WIDTH]...FIELD[:WIDTH]\" or &gt; sbank-list-allocations ... -f FIELD[:WIDTH] ... -f FIELD[:WIDTH] \n</code></pre></p> <p>If you wish to add fields to the default fields, enter one + symbol anywhere in the quoted string:  <pre><code>&gt; sbank-list-allocations ... -f \"+ FIELD[:WIDTH]...FIELD[:WIDTH]\", only one + symbol is needed.\n</code></pre></p> <p>The fields will be displayed in table format and in the order entered in the command line. You can specify the field width, where WIDTH can be positive or negative value. Left alignment use -, right alignment use + or nothing.</p> <p>For -w option:</p> <p>FIELD:WIDTH, if the field is displayed it will change the width for the specified field. </p> <p>NOTE: This will not add the field as in -f option, only change the width. To get available fields you can also use -w? or -w \"?\" as in -f option.</p> <p>For -S, -E, --created, --queued, --last-updated, --history-date-range options:</p> <p>These are the date filter options. All dates are treated as UTC. </p> <p>You can use any reasonable date string that resembles a date Ambiguous dates will be parsed with the following parsing precedence: **YEAR then MONTH then DAY **</p> <p>For example, 10-11-12 or 101112 will be the following date: Oct. 11, 2012 Not: Nov. 12, 2010 or Nov. 10, 2012 </p> <p>Or you can specify a single date as follows:  <pre><code>\"[OPER]UTC_DATE\" You can specify a date range as follows: \n\"[OPER1]UTC_DATE1...[OPER2]UTC_DATE2\" Where OPER can be one of the following operators: \"==\", \"&gt;=\", \"&lt;=\", \"&gt;\", \"&lt;\" or \"eq\", \"ge\", \"le\", \"gt\", \"lt\" \n</code></pre></p> <p>Note: The following defaults for OPER, OPER1, OPER2 for the following options:  <pre><code>Options OPER OPER1 OPER2 ------------------------- ---- ----- ----- -E, &lt; &gt;= &lt; -S, &gt;= &gt;= &lt; --at &gt;= &gt;= &lt; --created &gt;= &gt;= &lt; --eligible &gt;= &gt;= &lt; --last-updated &gt;= &gt;= &lt; --queued &gt;= &gt;= &lt; \n</code></pre></p> <p>You can also use the following key letters \"n\", \"t\", \"d\", \"w\", \"y\" as follows:  <pre><code>KEY SYNTAX DEFINITIONS ---------- ----------- n[ow] now, where \"now\" is current-date current-time UTC t[oday] today, where \"today\" is current-date 00:00:00 UTC [+/-]d specified \"number\" of +/- days from \"today\" in UTC [+/-]w specified \"number\" of +/- weeks from \"today\" in UTC [+/-]y specified \"number\" of +/- years from \"today\" in UTC\n</code></pre></p> <p>For -T option:</p> <p>Transaction type option. The following are the valid transaction types and their explanation: CHARGE filter on job charges PULLBACK filter on allocation pullbacks DEPOSIT filter on allocation deposits REFUND filter on job refunds VOID filter on void transactions</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#invocation","title":"INVOCATION","text":"<p>sbank sbank sbank sbank-detail sbank detail sbank d sbank-detail-allocations sbank detail allocations sbank d a sbank-detail-jobs sbank detail jobs sbank d j sbank-detail-projects sbank detail project sbank d p sbank-detail-transactions sbank detail transactions sbank d t sbank-detail-users sbank detail users sbank d u sbank-list sbank list sbank l sbank-list-allocations sbank list allocations sbank l a sbank-list-jobs sbank list jobs sbank l j sbank-list-projects sbank list projects sbank l p sbank-list-transactions sbank list transactions sbank l t sbank-list-users sbank list users sbank l u</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#environment-variables","title":"ENVIRONMENT VARIABLES","text":"<p>Command line default options: Define the following environment variables as you would in the command line. Once the environment variable is defined, it will be used as the default options and arguments for the specific command. Command line options will take precedence.</p> <p>sbank_DETAIL_ALLOCATIONS_ARGS</p> <p>Default arguments and options for sbank-detail-allocations.</p> <p>sbank_DETAIL_CATEGORIES_ARGS</p> <p>Default arguments and options for sbank-detail-categories.</p> <p>sbank_DETAIL_NAMES_ARGS</p> <p>Default arguments and options for sbank-detail-names.</p> <p>sbank_DETAIL_MESSAGES_ARGS</p> <p>Default arguments and options for sbank-detail-messages.</p> <p>sbank_DETAIL_JOBS_ARGS</p> <p>Default arguments and options for sbank-detail-jobs.</p> <p>sbank_DETAIL_PROJECTS_ARGS</p> <p>Default arguments and options for sbank-detail-projects.</p> <p>sbank_DETAIL_TRANSACTIONS_ARGS</p> <p>Default arguments and options for sbank-detail-transactions.</p> <p>sbank_DETAIL_USERS_ARGS</p> <p>Default arguments and options for sbank-detail-users.</p> <p>sbank_LIST_ALLOCATIONS_ARGS</p> <p>Default arguments and options for sbank-list-allocations.</p> <p>sbank_LIST_JOBS_ARGS</p> <p>Default arguments and options for sbank-list-jobs.</p> <p>sbank_LIST_PROJECTS_ARGS</p> <p>Default arguments and options for sbank-list-projects.</p> <p>sbank_LIST_TRANSACTIONS_ARGS</p> <p>Default arguments and options for sbank-list-transactions.</p> <p>sbank_LIST_USERS_ARGS</p> <p>Default arguments and options for sbank-list-users.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#examples","title":"EXAMPLES <p>Example 1: -f, --field <pre><code>&gt; sbank-list-transactions ... -f field1:-20 -f field2:20 -f field3 or &gt; sbank-list-transactions ... -f \"field1:-20 field2:20 field3\" \n</code></pre> Explanation: Fields will be displayed in order of appearance, where field1:-20 means 20 characters long, left align; where field2:20 means 20 characters long, right align; where field3 uses default sizes. Number fields default to right aligned. Text fields default to left aligned.</p> <p>Example 2: -S, -E, --created, --queued, --last-updated, --history-start, --history-end</p> <p>Single date-string examples: </p> <ul> <li>  <p>sbank-list-allocations -S \"&gt;=Oct 11, 2014\" start dates that are &gt;= \"2014-10-11 00:00:00\" </p>  </li> <li>  <p>sbank-list-allocations -S \"&lt;=2014-11-10\" start dates that are &lt;= \"2014-11-10 00:00:00\" </p>  </li> <li>  <p>sbank-list-allocations -E \"&lt;20141110\" end dates that are &lt; \"2014-11-10 00:00:00\" </p>  </li> <li>  <p>sbank-list-allocations -E \"22:30:10\" end dates that are &lt; \" 22:30:10\"    <li>  <p>sbank-list-allocations -S \"&gt;today\" start dates that are &gt; \" 00:00:00\"    <li>  <p>sbank-list-allocations -E t end dates that are &lt; \" 00:00:00\"    <li>  <p>sbank-list-allocations -S gtnow start dates that are &gt; \" \"    <li>  <p>sbank-list-allocations -E len end dates that are &lt;= \" \"    <li>  <p>sbank-list-allocations -S \"1d\" start dates that are &gt;= \"today +1 day\" </p>  </li> <li>  <p>sbank-list-allocations -E \"-2w\" end dates that are &lt; \"today -2 weeks\" </p>  </li> <li>  <p>sbank-list-allocations -S \"&gt;=1y\" start dates that are &gt;= \"today +1 year\" </p>  </li> <li>  <p>sbank-list-allocations -S \"&gt;2012\" start dates that are &gt; \"2012-- 00:00:00\"     <p>Range date-string examples: </p> <ul> <li>  <p>sbank-list-allocations -S \"2013-01-01...2014-01-01\" \"2013-01-01\" &lt;= DATES &lt; \"2014-01-01\" </p>  </li> <li>  <p>sbank-list-allocations -S \"-1y...t\" \"today -1 year\" &lt;= DATES &lt; \"today\" </p>  </li> <li>  <p>sbank-list-allocations -E \"2013...t\"\" \"2013--\" &lt;= DATES &lt; \"today\"    <li>  <p>sbank-list-allocations -E \"&gt;2013...&lt;=t\"\" \"2013--\" &lt; DATES &lt;= \"today\"    <p>Example 3: Command invocation examples</p> <ul> <li>  <p>sbank-list-projects list projects full command invocation </p>  </li> <li>  <p>sbank list projects list projects meta command invocation</p>  </li> <li>  <p>sbank s p list projects partial meta command invocation </p>  </li> <li>  <p>sbank p list projects where \"list\" is the default</p>  </li> <li>  <p>sbank list allocations is the default </p>  </li> <li>  <p>sbank a list allocations \"list\" is the default </p>  </li> <li>  <p>sbank s a list allocations partial meta command invocation</p>  </li> </ul> <p>Example 4: -h, --help</p> <ul> <li>  <p>sbank -h will give you help summary on all of sbank </p>  </li> <li>  <p>sbank list --help will give you help on all the \"list\" commands </p>  </li> <li>  <p>sbank list allocations -h will give you help on the \"list allocations\" command</p>  </li> <li>  <p>sbank-list-allocations -h will give you help on the \"list allocations\" command </p>  </li> <li>  <p>sbank l a --help will give you help on the \"list allocations\" command</p>  </li> </ul>","text":""},{"location":"account-project-management/project-management/project-reports/","title":"Quarterly and Year-End Reporting","text":"<p>The Argonne Leadership Computing Facility (ALCF) is required to report the progress and scientific accomplishments of all peer-reviwed projects. </p> <p>PIs of INCITE, ALCC, and ADSP projects are required to complete quarterly reports and a final end-of-project (EOY/EOP) report.</p>"},{"location":"account-project-management/project-management/project-reports/#due-dates","title":"Due dates","text":""},{"location":"account-project-management/project-management/project-reports/#due-dates-for-the-2023-incite-quarterly-eoy-and-the-eop-reports","title":"Due dates for the 2023 INCITE quarterly, EOY, and the EOP reports:","text":"<ul> <li>April 1, 2023 (CY2023 - Q1)</li> <li>July 1, 2023 (CY2023 - Q2)</li> <li>October 1, 2023 (CY2023 - Q3)</li> <li>January 1, 2024 (CY2024 - EOY) or February 15, 2024 (entire allocation period - EOP)</li> </ul>"},{"location":"account-project-management/project-management/project-reports/#due-dates-for-the-2023-2024-alcc-quarterly-and-the-eop-reports","title":"Due dates for the 2023-2024 ALCC quarterly and the EOP reports:","text":"<ul> <li>October 1, 2023 (CY2023 - Q3)</li> <li>January 1, 2024 (CY2024 - Q4)</li> <li>April 1, 2024 (CY2024 - Q1)</li> <li>August 15, 2024 (CY2024 - EOP)</li> </ul>"},{"location":"account-project-management/project-management/project-reports/#penalties","title":"Penalties","text":"<p>If a quarterly report is more than 30 days late: - The ability to submit jobs for the PI and users of the late project will be disabled.</p> <p>If a quarterly report is more than 90 days late: - The PI and users of the late project will have their accounts disabled.</p> <p>These penalties will be removed within three business days after the late quarterly or EOY report is submitted.</p>"},{"location":"account-project-management/project-management/project-reports/#alcc-specific-penalties","title":"ALCC Specific Penalties:","text":"<p>A similar penalty will also be applied to new ALCC projects with the same PI or co-PIs that have failed to submit the EOP report for a previous ALCC project. If the EOP report is more than 15 days late:</p> <ul> <li>The new ALCC project will be blocked. For a currently active ALCC project, the ability to submit jobs will be disabled for the project and all sub-projects. For a project that has not been created yet, the process for new project creation will be halted.</li> </ul>"},{"location":"account-project-management/project-management/project-reports/#appeals","title":"Appeals","text":"<p>A PI or user may appeal a project or account suspension to the ALCF Director by a request to support at alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/project-reports/#report-templates","title":"Report Templates","text":"<p>Templates for the quarterly and the EOY reports can be found at the links on the bottom of this page.</p> <p>Please modify the filename to replace PINAME with the last name of the PI of the INCITE/ALCC project, ALLOCATION to INCITE/ALCC, and YEAR to the corresponding calendar year.  For quarterly reports, please replace the X in the filename with the quarter number.</p> <p>For example, for a project with PI 'Joe Smith' that is submitting the quarterly report for the first quarter in 2021-2022 cycle for ALCC, the filename will be Smith_ALCC_Q1.docx.</p> <p>For an EOY report, replace YEARS with the years associated with your allocation. For example, an ALCC 2021-2022 project with PI 'Joe Smith' would have a filename of Smith_ALCC_2021-2022_EOY.docx.</p>"},{"location":"account-project-management/project-management/project-reports/#templates-for-incite-and-alcc","title":"Templates for INCITE and ALCC:","text":"<ul> <li>Quarterly Report Template</li> <li>End of Project Report Template</li> <li>End of Year Report Template</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/","title":"Starting Your ALCF Award","text":"<p>The following guide is for PIs and Proxies to get insight into managing projects and teams for ALCF awards. Please submit for questions or trouble tickets to support@alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#get-started-with-alcfs-systems","title":"Get Started with ALCF\u2019s Systems","text":"<p>To get started using our resources, please visit:  Connect &amp; Login</p> <p>We also encourage you to take full advantage of ALCF's training programs and user services. Some useful introductory materials and videos are listed below:</p> <ul> <li>Theta and Cooley Overview</li> <li>Running on Theta </li> <li>ThetaGPU Overview</li> <li>Lustre File Striping Basics</li> <li>Community Data Sharing with ACDC (using Eagle)</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/#project-terminology","title":"Project Terminology","text":"<p>Before your project begins, you will receive an email with the following project information:</p> <ul> <li>Project Short Name: The assigned, shortened name for your project. This will be the name that you\u2019ll use to access your project on the systems.</li> <li>Project Proxies: Project members designated by PIs that are authorized to add or renew project members on your behalf.</li> <li>Allocation System(s) and Allocation Amount: The approved system(s) and amount of your award in node hours.</li> <li>Approved Quota: The approved amount of disk space for your project directory.</li> <li>File System: The file system where your project directory will reside. For information on the Grand and Eagle file systems, see Storage and Networking.</li> <li>Assigned Catalyst: INCITE projects will have ALCF staff members that are assigned to the projects who are available to assist the team throughout the duration of the INCITE allocation.</li> <li>Allocation Start Date: The start date of your award.</li> <li>Allocation End Date: The end date of your award.</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/#account-setup","title":"Account Setup","text":"<p>If you do not have an ALCF account: You will need to request one at https://accounts.alcf.anl.gov/accountRequest. When prompted for project name, please select the project short name you were given in your award email from support@alcf.anl.gov.</p> <p>If you have an active ALCF account: Submit a request to join the newly awarded project at https://accounts.alcf.anl.gov/#!/joinProject.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#information-for-foreign-national-access","title":"Information for Foreign National Access","text":"<p>The U.S. Department of Energy has guidelines and requirements for foreign nationals who access its facilities and sites. This guidance is issued in DOE Order 142.3, which is part of Argonne's contract; therefore, all foreign nationals (non-U.S. Citizens) must obtain authorization prior to using ALCF resources.</p> <p>If you are a foreign national and do not have current authorization credentials, you are required to submit a ANL-593 (Foreign National Access Request) form. It is critical that identity documentation requests sent by ALCF staff are completed as early as possible to facilitate timely processing for your account approval.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#user-agreement-for-incite-alcc-and-adsp","title":"User Agreement for INCITE, ALCC, and ADSP","text":"<p>Note: This does not apply to Director's Discretionary awards.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#insitution-master-agreement-for-incite-alcc-and-adsp","title":"Insitution Master Agreement for INCITE, ALCC, and ADSP","text":"<p>If you are not an employee of Argonne National Laboratory, a user agreement must be signed by your home institution to perform research at Argonne\u2019s user facilities. This policy applies to every member of the project team who will be conducting research on ALCF resources.</p> <p>A list of home institutions that have master agreements in place is located on this webpage: https://www.aps.anl.gov/Users-Information/Legal-Financial/Argonne-User-Facility-Agreements</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#alcf-user-agreement-for-incite-alcc-and-adsp","title":"ALCF User Agreement for INCITE, ALCC, and ADSP","text":"<p>Note: This does not apply to Director's Discretionary awards.</p> <p>Every project team member who requests an ALCF account must sign and return an acknowledgment form, stating that they agree to the terms in the user agreement.</p> <p>The form is located at: https://www.alcf.anl.gov/files/Acknowledgement_Form.pdf. Please print, sign, scan and email it to accounts@alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#managing-project-team-membership","title":"Managing Project Team Membership","text":"<p>As a PI, you can add members to your project. You can assign proxies who are project members authorized to add or renew project members on your behalf.</p> <p>A project PI or proxy has the authority to:</p> <ul> <li>Approve and renew accounts</li> <li>Add and delete users to/from the project</li> <li>Approve Foreign Assignment/Visit Request form renewals for project members who are foreign nationals</li> </ul> <p>During your project setup, the ALCF Support Team will request the following information to establish your project members:</p> <ul> <li>The names, email addresses, and/or ALCF usernames (if already existing) of up to two proxies and all project members.</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/#about-project-and-unix-group-membership","title":"About Project and UNIX Group Membership","text":"<p>All project members have the ability to run jobs against your allocation. There is no limit to the number of project members you may authorize. Project members are automatically added to the project UNIX group giving them the ability to write to the project directory and to access project data. When a project member is added or removed from a project, this automatically be reflected in the project UNIX group membership. </p>"},{"location":"account-project-management/project-management/starting-alcf-award/#adding-project-members","title":"Adding Project Members","text":"<p>The PI or a proxy must approve each team member to access ALCF resources and run jobs on their project. PI/proxies can respond to emails from ALCF for account access approval with a \"yes\" or \"no\".</p> <p>PI/proxies with active ALCF accounts can also approve new account requests, project membership requests, account reactivation requests, add existing active ALCF users to the project by logging into the ALCF Account and Project Management application.  </p> <p>Note: If PI/proxies need to request an ALCF account, see the section below for instructions on \"how to apply\" for an account.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#accounts-and-access-for-your-project-members","title":"Accounts and Access for your Project Members","text":"<p>All project members will need an ALCF user account to access project data and to run jobs on ALCF systems.</p> <p>Members that do not have an ALCF account should request one at: https://accounts.alcf.anl.gov/accountRequest. When prompted for project name, they should select your project short name.</p> <p>If your project members have ALCF accounts that are no longer active, please ask them to submit a reactivation request here: https://accounts.alcf.anl.gov/accountReactivate. When prompted for project name, they should select your project short name.</p> <p>If you project members have active ALCF accounts but have not been added to your project, they should submit a request to join your project by going to this page: https://accounts.alcf.anl.gov/#!/joinProject.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#moving-your-data","title":"Moving Your Data","text":"<p>We encourage you to use Globus to move your project data to your ALCF project directory before your allocation begins. For details, see Using Globus on Theta.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#project-status-reports-for-incite-alcc-and-adsp","title":"Project Status Reports for INCITE, ALCC, and ADSP","text":"<p>Note: PIs that are awarded a Director's Discretionary will not receive weekly status project reports.</p> <p>Shortly after your allocation begins, we will begin sending you a weekly project status report via support@alcf.anl.gov to keep you informed or your award progress.</p> <p>Look for an email from us with the subject line: ALCF [ALLOCATION PROGRAM] Project Status Report for [PROJECT SHORT NAME]</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#reporting-requirements-for-incite-alcc-and-adsp","title":"Reporting Requirements for INCITE, ALCC, and ADSP","text":"<p>Note: PIs that are awarded a Director's Discretionary allocations are not required to submit project reports.</p> <p>If you receieved INCITE, ALCC, or ADSP allocation award, quarterly reporting is required to keep DOE informed of progress related to your allocation. </p> <p>The ALCF will send you a report template at the end of each quarter. Please complete the report promptly and submit it via email to support@alcf.anl.gov. For more information see the Quarterly Report webpage.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#policies","title":"Policies","text":""},{"location":"account-project-management/project-management/starting-alcf-award/#pullback-policy","title":"Pullback Policy","text":"<p>Please be aware that we will periodically monitor, and could potentially adjust, your project allocation if a large portion of it goes unused. You may view: Pullback Policy</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#allocation-overburn-policy","title":"Allocation Overburn Policy","text":"<p>Please see this page for overburn/overuse eligibility for INCITE projects that have exhausted their allocation in the first 11 months of its allocation year: Allocation Overburn</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#acknowledgment-in-publications","title":"Acknowledgment In Publications","text":"<p>Please follow the guidelines provided on the ALCF Acknowledgement Policy page to properly acknowledge the use of ALCF resources in all of your publications, both online and print.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#facility-policies","title":"Facility Policies","text":"<p>Facility policies have been established to provide consistent and reliable services. Please read about our [ALCF Facility Policies] (../policies/facility-policies.md).</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#useful-allocation-and-quota-commands","title":"Useful Allocation and Quota Commands","text":"<p>We have an allocation management tool called sbank, and below are a few helpful sbank commands.  </p> <ul> <li>myprojectquotas: log into Theta and type this command to view the project directory quotas for all your projects</li> <li>myquota: log into Theta and type this command to view your home directory quota</li> </ul> <p>You can use the following command to check your project balance on Theta: - sbank-list-allocations -p  -r  <p>For more command examples and details, see sbank.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#how-can-we-help","title":"How Can We Help?","text":"<p>We can also help resolve any issues or needs that may be delaying the start of your scientific campaign. - Are you in need of high-throughput software? - Are you having difficulty compiling your application? - Does your code have limited restart capabilities?</p> <p>If your project allocation usage is being held back for reasons due to one of our systems, please contact us for assistance by emailing support@alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/team-management/","title":"Managing Your Team Members","text":"<p>New project members will need a user account to access project data and to run jobs on ALCF systems.</p> <p>Please instruct any members who do not have an ALCF account to request one as soon as possible by visiting: https://accounts.alcf.anl.gov/#!/accountRequest. When prompted for project name, they should select the \"short name\" for your project. </p> <p>The PI or Proxy must approve each member of the team to gain access and to run project jobs on the ALCF's resources. If you have an active ALCF account, you can manage your project team by logging into the ALCF account and project management website and navigating to https://accounts.alcf.anl.gov/#!/manageProjects</p>"},{"location":"account-project-management/project-management/team-management/#accessing-your-projects","title":"Accessing your project(s)","text":"<ol> <li>Log in at https://accounts.alcf.anl.gov/#!/manageProjects using your credentials: ALCF username and Physical/Mobile token passcode for a password.</li> <li>Click on Project Management, located in the right sidebar.</li> <li>You will see a list of projects of which you are the Primary Investigator (PI).</li> <li>Click on the desired project to view information and management options for the selected project.</li> </ol>"},{"location":"account-project-management/project-management/team-management/#modifying-project-information","title":"Modifying project information","text":"<p>Some project information cannot be modified, but as the PI, you can modify the following: project title, institutions, and associated funding.</p> <p>Your project can be associated with multiple institutions, but you must specify a primary institution.</p>"},{"location":"account-project-management/project-management/team-management/#managing-project-members-with-an-existing-alcf-account","title":"Managing project members with an Existing ALCF Account","text":"<ol> <li>You can manage the membership for your project by clicking on the desired project from the Project Management screen.</li> <li>Add and/or remove proxies and team members by clicking on the red \"Remove\" button to the right of each member or clicking on \"Add new user.\"</li> <li>You can view account information for each user as it relates to the project:</li> <li>Account Status</li> <li>Project Role</li> <li>Proxy Permissions</li> <li> <p>Membership Status</p> </li> <li> <p>Proxies are individuals authorized to add or renew user accounts for the project PI. You have the ability to upgrade a user from a member to a Proxy, by clicking on the \"Proxy\" radio button that corresponds with the desired member.</p> </li> </ol>"},{"location":"ai-testbed/getting-started/","title":"ALCF AI Testbed","text":"<p>The ALCF AI Testbed houses some of the most advanced AI accelerators for scientific research. </p> <p>The goal of the testbed is to enable explorations into next-generation machine learning applications and workloads, enabling the ALCF and its user community to help define the role of AI accelerators in scientific computing and how to best integrate such technologies with supercomputing resources.</p> <p>The AI accelerators complement the ALCF's current and next-generation supercomputers to provide a state-of-the-art computing environment that supports pioneering research at the intersection of AI, big data, and high performance computing (HPC). </p> <p>The platforms are equipped with architectural features that support AI and data-centric workloads, making them well suited for research tasks involving the growing deluge of scientific data produced by powerful tools, such as supercomputers, light sources, telescopes, particle accelerators, and sensors. In addition, the testbed will allow researchers to explore novel workflows that combine AI methods with simulation and experimental science to accelerate the pace of discovery.</p>"},{"location":"ai-testbed/getting-started/#how-to-get-access","title":"How to Get Access","text":"<p>Researchers interested in using the AI Testbed\u2019s <code>Cerebras CS-2</code> and <code>SambaNova DataScale</code> platforms can now submit project proposals via the ALCF\u2019s Director\u2019s Discretionary program. Access to additional testbed resources, including <code>Graphcore</code>, <code>Groq</code>, and <code>Habana</code> accelerators, will be announced at a later date. </p> <p>Submit your proposal requests at: Allocation Request Page</p>"},{"location":"ai-testbed/getting-started/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Request a Director's Discretionary project on SambaNova/Cerebras.</p> </li> <li> <p>Apply for an ALCF account after the project request is approved. Choose the SambaNova/Cerebras project that your PI has created at ALCF. If you have an active ALCF account, request to join the project after your project is approved.</p> </li> <li> <p>Transfer data to ALCF using Globus after your account has been created.</p> <p>a. The endpoint for your data in ALCF is <code>alcf#ai_testbed_projects</code> with the path to your project being  <code>/&lt;project name&gt;</code>. </p> <p>b. The endpoint for your home directory on the AI Testbeds in ALCF is <code>alcf#ai_testbed_home</code>.</p> </li> <li> <p>Add/invite team members to your ALCF project on SambaNova/Cerebras. </p> </li> </ol>"},{"location":"ai-testbed/getting-started/#how-to-contribute-to-documentation","title":"How to Contribute to Documentation","text":"<p>The documentation is based on MkDocs and source files are on GitHub. You can contribute to the documentation by creating a pull request. </p> <p>Learn more on how to contribute to documentation.</p>"},{"location":"ai-testbed/cerebras/customizing-environment/","title":"Customizing Environments","text":""},{"location":"ai-testbed/cerebras/customizing-environment/#using-virtual-python-environments","title":"Using virtual Python environments","text":""},{"location":"ai-testbed/cerebras/customizing-environment/#to-make-a-pytorch-virtual-environment-for-cerebras","title":"To make a PyTorch virtual environment for Cerebras","text":"<pre><code>#Make your home directory navigable\nchmod a+xr ~/\nmkdir ~/R_1.9.1\nchmod a+x ~/R_1.9.1/\ncd ~/R_1.9.1\n# Note: \"deactivate\" does not actually work in scripts.\ndeactivate\nrm -r venv_pt\n/software/cerebras/python3.8/bin/python3.8 -m venv venv_pt\nsource venv_pt/bin/activate\npip3 install /opt/cerebras/wheels/cerebras_pytorch-1.9.1+1cf4d0632b-cp38-cp38-linux_x86_64.whl --find-links=/opt/cerebras/wheels\npip install numpy==1.23.4\npip install datasets transformers\n</code></pre>"},{"location":"ai-testbed/cerebras/customizing-environment/#to-make-a-tensorflow-virtual-environment-for-cerebras","title":"To make a TensorFlow virtual environment for Cerebras","text":"<pre><code>chmod a+xr ~/\nmkdir ~/R_1.9.1\nchmod a+x ~/R_1.9.1/\ncd ~/R_1.9.1\n# Note: \"deactivate\" does not actually work in scripts.\ndeactivate\nrm -r venv_tf\n/software/cerebras/python3.8/bin/python3.8 -m venv venv_tf\nsource venv_tf/bin/activate\n#pip install tensorflow_datasets\n#pip install spacy\npip3 install /opt/cerebras/wheels/cerebras_tensorflow-1.9.1+1cf4d0632b-cp38-cp38-linux_x86_64.whl --find-links=/opt/cerebras/wheels/\npip install numpy==1.23.4\n</code></pre>"},{"location":"ai-testbed/cerebras/customizing-environment/#activation-and-deactivation","title":"Activation and deactivation","text":"<p>To activate one of these virtual environments,</p> <pre><code>source ~/R_1.9.1/venv_pt/bin/activate\n</code></pre> <p>or</p> <pre><code>source ~/R_1.9.1/venv_tf/bin/activate\n</code></pre> <p>To deactivate a virtual environment,</p> <pre><code>deactivate\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/","title":"Example Programs","text":""},{"location":"ai-testbed/cerebras/example-programs/#use-a-local-copy-of-the-model-zoo","title":"Use a local copy of the model zoo","text":"<p>Make a working directory and a local copy of the Cerebras modelzoo and anl_shared repository, if not previously done, as follows.</p> <pre><code>mkdir ~/R_1.9.1\ncd ~/R_1.9.1\ngit clone https://github.com/Cerebras/modelzoo.git\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/#unet","title":"UNet","text":"<p>An implementation of this: U-Net: Convolutional Networks for Biomedical Image Segmentation, Ronneberger et.  al 2015 To run Unet with the Severstal: Steel Defect Detection kaggle dataset, using a pre-downloaded copy of the dataset: First, source a Cerebras PyTorch virtual environment.</p> <pre><code>source ~/R_1.9.1/venv_pt/bin/activate\n</code></pre> <p>Then</p> <pre><code>cd ~/R_1.9.1/modelzoo/modelzoo/vision/pytorch/unet\ncp /software/cerebras/dataset/severstal-steel-defect-detection/params_severstal_binary_rawds.yaml configs/params_severstal_binary_rawds.yaml\nexport MODEL_DIR=model_dir_unet\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=unet_pt --params configs/params_severstal_binary_rawds.yaml --model_dir $MODEL_DIR --mode train --mount_dirs /home/ /software --python_paths /home/$(whoami)/R_1.9.1/modelzoo/ --compile_dir $(whoami) |&amp; tee mytest.log \n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/#bert-pytorch","title":"BERT - PyTorch","text":"<p>The modelzoo/modelzoo/transformers/pytorch/bert directory is a PyTorch implementation of BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding This BERT-large msl128 example uses a single sample dataset for both training and evaluation. See the README.md in the source directory for details on how to build a dataset from text input. First, source a Cerebras PyTorch virtual environment.</p> <pre><code>source ~/R_1.9.1/venv_pt/bin/activate\n</code></pre> <p>Then</p> <pre><code>cd ~/R_1.9.1/modelzoo/modelzoo/transformers/pytorch/bert\ncp /software/cerebras/dataset/bert_large/bert_large_MSL128_sampleds.yaml configs/bert_large_MSL128_sampleds.yaml\nexport MODEL_DIR=model_dir_bert_large_pytorch\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=bert_pt --params configs/bert_large_MSL128_sampleds.yaml --num_workers_per_csx=1 --mode train --model_dir $MODEL_DIR --mount_dirs /home/ /software/ --python_paths /home/$(whoami)/R_1.9.1/modelzoo/ --compile_dir $(whoami) |&amp; tee mytest.log\n</code></pre> <p>The last parts of the output should resemble the following, with messages about cuda that should be ignored and are not shown.</p> <pre><code>2023-05-17 18:10:08,776 INFO:   Finished sending initial weights\n2023-05-17 18:15:11,548 INFO:   | Train Device=xla:0, Step=100, Loss=9.46875, Rate=4597.49 samples/sec, GlobalRate=4597.49 samples/sec\n2023-05-17 18:15:23,067 INFO:   | Train Device=xla:0, Step=200, Loss=8.94531, Rate=7173.00 samples/sec, GlobalRate=6060.68 samples/sec\n2023-05-17 18:15:41,547 INFO:   | Train Device=xla:0, Step=300, Loss=8.79688, Rate=6193.85 samples/sec, GlobalRate=5876.98 samples/sec\n2023-05-17 18:15:54,118 INFO:   | Train Device=xla:0, Step=400, Loss=8.28906, Rate=7365.06 samples/sec, GlobalRate=6316.84 samples/sec\n2023-05-17 18:16:12,430 INFO:   | Train Device=xla:0, Step=500, Loss=8.14844, Rate=6301.21 samples/sec, GlobalRate=6157.22 samples/sec\n2023-05-17 18:16:25,177 INFO:   | Train Device=xla:0, Step=600, Loss=8.06250, Rate=7340.44 samples/sec, GlobalRate=6406.58 samples/sec\n2023-05-17 18:16:43,315 INFO:   | Train Device=xla:0, Step=700, Loss=8.00000, Rate=6323.57 samples/sec, GlobalRate=6285.55 samples/sec\n2023-05-17 18:16:56,110 INFO:   | Train Device=xla:0, Step=800, Loss=7.96484, Rate=7331.29 samples/sec, GlobalRate=6458.82 samples/sec\n2023-05-17 18:17:14,564 INFO:   | Train Device=xla:0, Step=900, Loss=7.89844, Rate=6261.77 samples/sec, GlobalRate=6343.22 samples/sec\n2023-05-17 18:17:26,977 INFO:   | Train Device=xla:0, Step=1000, Loss=7.90234, Rate=7454.38 samples/sec, GlobalRate=6493.27 samples/sec\n2023-05-17 18:17:26,978 INFO:   Saving checkpoint at global step 1000\n2023-05-17 18:18:38,485 INFO:   Saving step 1000 in dataloader checkpoint\n2023-05-17 18:18:38,931 INFO:   Saved checkpoint at global step: 1000\n2023-05-17 18:18:38,932 INFO:   Training Complete. Completed 1024000 sample(s) in 229.65675950050354 seconds.\n2023-05-17 18:18:49,293 INFO:   Monitoring returned\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/#gpt-j-pytorch","title":"GPT-J PyTorch","text":"<p>GPT-J [github] is an auto-regressive language model created by EleutherAI. This PyTorch GPT-J 6B parameter pretraining sample uses 2 CS2s.</p> <p>First, source a Cerebras PyTorch virtual environment.</p> <pre><code>source ~/R_1.9.1/venv_pt/bin/activate\n</code></pre> <p>Then</p> <pre><code>cd ~/R_1.9.1/modelzoo/modelzoo/transformers/pytorch/gptj\ncp /software/cerebras/dataset/gptj/params_gptj_6B_sampleds.yaml configs/params_gptj_6B_sampleds.yaml\nexport MODEL_DIR=model_dir_gptj\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=gptj_pt --params configs/params_gptj_6B_sampleds.yaml --num_csx=2 --mode train --model_dir $MODEL_DIR --mount_dirs /home/ /software --python_paths /home/$(whoami)/R_1.9.1/modelzoo/ --compile_dir $(whoami) |&amp; tee mytest.log\n</code></pre> <p>The last parts of the output should resemble the following:</p> <pre><code>2023-05-17 18:44:38,290 INFO:   Finished sending initial weights\n2023-05-17 18:51:03,551 INFO:   | Train Device=xla:0, Step=100, Loss=8.46875, Rate=33.83 samples/sec, GlobalRate=33.83 samples/sec\n2023-05-17 18:57:26,199 INFO:   | Train Device=xla:0, Step=200, Loss=8.06250, Rate=33.92 samples/sec, GlobalRate=33.90 samples/sec\n2023-05-17 19:03:48,354 INFO:   | Train Device=xla:0, Step=300, Loss=7.71875, Rate=33.98 samples/sec, GlobalRate=33.94 samples/sec\n2023-05-17 19:10:10,299 INFO:   | Train Device=xla:0, Step=400, Loss=7.46875, Rate=34.01 samples/sec, GlobalRate=33.96 samples/sec\n2023-05-17 19:16:32,156 INFO:   | Train Device=xla:0, Step=500, Loss=7.21875, Rate=34.03 samples/sec, GlobalRate=33.98 samples/sec\n2023-05-17 19:16:32,157 INFO:   Saving checkpoint at global step 500\n2023-05-17 19:27:12,834 INFO:   Saving step 500 in dataloader checkpoint\n2023-05-17 19:27:13,435 INFO:   Saved checkpoint at global step: 500\n2023-05-17 19:27:13,436 INFO:   Training Complete. Completed 65000 sample(s) in 2554.1804394721985 seconds.\n</code></pre>"},{"location":"ai-testbed/cerebras/getting-started/","title":"Getting Started","text":""},{"location":"ai-testbed/cerebras/getting-started/#getting-started","title":"Getting Started","text":""},{"location":"ai-testbed/cerebras/getting-started/#connection-to-a-cs-2-node","title":"Connection to a CS-2 node","text":"<p> Connection to one of the CS-2 cluster login nodes requires an MFA passcode for authentication - either an 8-digit passcode generated by an app on your mobile device (e.g. MobilePASS+) or a CRYPTOCard-generated passcode prefixed by a 4-digit pin. This is the same passcode used to authenticate into other ALCF systems, such as Theta and Cooley. In the examples below, replace ALCFUserID with your ALCF user id. To connect to a CS-2 login:</p> <ol> <li>ssh to a desired login node:     <pre><code>ssh ALCFUserID@cer-login-01.ai.alcf.anl.gov\n</code></pre>     or     <pre><code>ssh ALCFUserID@cer-login-02.ai.alcf.anl.gov\n</code></pre>     or     <pre><code>ssh ALCFUserID@cer-login-03.ai.alcf.anl.gov\n</code></pre></li> <li>Alternatively, ssh randomly to one of the above three login nodes:     <pre><code>ssh ALCFUserID@cerebras.ai.alcf.anl.gov\n</code></pre></li> </ol>"},{"location":"ai-testbed/cerebras/job-queuing-and-submission/","title":"Job Queuing and Submission","text":"<p>The CS-2 cluster has its own Kubernetes-based system for job submission and queuing.</p> <p>Jobs are started automatically through the Python frameworks in modelzoo.common.pytorch.run_utils and modelzoo.common.tf.run_utils Continuous job status for a job is output to stdout/stderr; redirect the output, or consider using a persistent session started with screen, or tmux, or both.</p> <p>Jobs that have not yet completed can be listed as shown. Note: this command can take over a minute to complete.</p> <pre><code>(venv_pt) $ csctl get jobs\nNAME                          AGE  DURATION  PHASE    SYSTEMS     USER     LABELS        DASHBOARD\nwsjob-thjj8zticwsylhppkbmjqe  13s  1s        RUNNING  cer-cs2-01  username name=unet_pt  https://grafana.cerebras1.lab.alcf.anl.gov/d/WebHNShVz/wsjob-dashboard?orgId=1&amp;var-wsjob=wsjob-thjj8zticwsylhppkbmjqe&amp;from=1691705374000&amp;to=now\n(venv_pt) $\n</code></pre> <p>Jobs can be canceled as shown:</p> <pre><code>(venv_tf) $ csctl cancel job wsjob-eyjapwgnycahq9tus4w7id\nJob canceled successfully\n(venv_tf) $\n</code></pre> <p>Jobs can be labeled in the command line that launches them, if they are written with Cerebras's Python framework for running appliance jobs, by adding a command line option of this form: <pre><code> --job_labels labelname=labelvalue\n</code></pre></p> <p>Jobs can also be labeled after they have been started as shown: <pre><code>(venv_pt) $ csctl label job wsjob-ez6dyfronnsg2rz7f7fqw4 testlabel=test\njob/wsjob-ez6dyfronnsg2rz7f7fqw4 was patched\n(venv_pt) $\n</code></pre></p> <p>Jobs with a particular label/label value can be listed as shown: <pre><code>(venv_pt) $ csctl get jobs | grep \"testlabel=test\"\nwsjob-ez6dyfronnsg2rz7f7fqw4  19m SUCCEEDED  cer-cs2-02 username testlabel=test,user=username\n(venv_pt) $\n</code></pre></p> <p>See <code>csctl -h</code> for more options. Add <code>-h</code> to a command for help for that command, e.g. <code>csctl get -h</code> or <code>csctl cancel -h</code>. </p> <pre><code>$ csctl -h\nCerebras cluster command line tool.\nUsage:\n  csctl [command]\nAvailable Commands:\n  cancel             Cancel job\n  clear-worker-cache Clear the worker cache\n  config             View csctl config files\n  get                Get resources\n  label              Label resources\n  log-export         Gather and download logs.\n  types              Display resource types\nFlags:\n  -d, --debug int          higher debug values will display more fields in output objects\n  -h, --help               help for csctl\n      --namespace string   configure csctl to talk to different user namespaces\n  -v, --version            version for csctl\nUse \"csctl [command] --help\" for more information about a command.\n</code></pre>"},{"location":"ai-testbed/cerebras/miscellaneous/","title":"Miscellaneous","text":""},{"location":"ai-testbed/cerebras/miscellaneous/#porting-applications-to-the-cs-2","title":"Porting applications to the CS-2","text":"<p>Cerebras documentation for porting code to run on a Cerebras CS-2 system: Ways to port your model</p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/","title":"Running a Model/Program","text":""},{"location":"ai-testbed/cerebras/running-a-model-or-program/#getting-started","title":"Getting Started","text":""},{"location":"ai-testbed/cerebras/running-a-model-or-program/#job-submission-and-queuing","title":"Job submission and queuing","text":"<p>Cerebras jobs are initiated and tracked automatically within the Python frameworks in modelzoo.common.pytorch.run_utils and modelzoo.common.tf.run_utils. These frameworks interact with the Cerebras cluster management node.</p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#login-nodes","title":"Login nodes","text":"<p>Jobs are launched from login nodes. If you expect a loss of an internet connection for any reason, for long-running jobs we suggest logging into a specific login node and using either screen or tmux to create persistent command line sessions.  For details use:</p> <pre><code>man screen\n# or\nman tmux\n</code></pre>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#running-jobs-on-the-wafer","title":"Running jobs on the wafer","text":"<p>Follow these instructions to compile and train the <code>fc_mnist</code> TensorFlow and PyTorch samples. These models are a couple of fully connected layers plus dropout and RELU. </p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#cerebras-virtual-environments","title":"Cerebras virtual environments","text":"<p>First, make virtual environments for Cerebras for PyTorch and/or TensorFlow. See Customizing Environments for the procedures for making PyTorch and/or TensorFlow virtual environments for Cerebras. If the environments are made in <code>~/R_1.9.1/</code>, then they would be activated as follows: <pre><code>source ~/R_1.9.1/venv_pt/bin/activate\n</code></pre> or <pre><code>source ~/R_1.9.1/vent_tf/bin/activate\n</code></pre></p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#clone-the-cerebras-modelzoo","title":"Clone the Cerebras modelzoo","text":"<pre><code>mkdir ~/R_1.9.1\ncd ~/R_1.9.1\ngit clone https://github.com/Cerebras/modelzoo.git\ncd modelzoo\ngit tag\ngit checkout Release_1.9.1\n</code></pre>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#running-a-pytorch-sample","title":"Running a Pytorch sample","text":""},{"location":"ai-testbed/cerebras/running-a-model-or-program/#activate-your-pytorch-virtual-environment-and-change-to-the-working-directory","title":"Activate your PyTorch virtual environment, and change to the working directory","text":"<pre><code>source ~/R_1.9.1/venv_pt/bin/activate\ncd ~/R_1.9.1/modelzoo/modelzoo/fc_mnist/pytorch\n</code></pre> <p>Next, edit configs/params.yaml, making the following changes:</p> <pre><code> train_input:\n-    data_dir: \"./data/mnist/train\"\n+    data_dir: \"/software/cerebras/dataset/fc_mnist/data/mnist/train\"\n</code></pre> <p>and</p> <pre><code> eval_input:\n-    data_dir: \"./data/mnist/val\"\n+    data_dir: \"/software/cerebras/dataset/fc_mnist/data/mnist/train\"\n</code></pre> <p>If you want to have the sample download the dataset, you will need to specify absolute paths for the \"data_dir\"s.</p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#running-a-sample-pytorch-training-job","title":"Running a sample PyTorch training job","text":"<p>To run the sample:</p> <pre><code>export MODEL_DIR=model_dir\n# deletion of the model_dir is only needed if sample has been previously run\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=pt_smoketest --params configs/params.yaml --num_csx=1 --mode train --model_dir $MODEL_DIR --mount_dirs /home/ /software --python_paths /home/$(whoami)/R_1.9.1/modelzoo --compile_dir /$(whoami) |&amp; tee mytest.log\n</code></pre> <p>A successful fc_mnist PyTorch training run should finish with output resembling the following:</p> <pre><code>2023-05-15 16:05:54,510 INFO:   | Train Device=xla:0, Step=9950, Loss=2.30234, Rate=157300.30 samples/sec, GlobalRate=26805.42 samples/sec\n2023-05-15 16:05:54,571 INFO:   | Train Device=xla:0, Step=10000, Loss=2.29427, Rate=125599.14 samples/sec, GlobalRate=26905.42 samples/sec\n2023-05-15 16:05:54,572 INFO:   Saving checkpoint at global step 10000\n2023-05-15 16:05:59,734 INFO:   Saving step 10000 in dataloader checkpoint\n2023-05-15 16:06:00,117 INFO:   Saved checkpoint at global step: 10000\n2023-05-15 16:06:00,117 INFO:   Training Complete. Completed 1280000 sample(s) in 53.11996841430664 seconds.\n2023-05-15 16:06:04,356 INFO:   Monitoring returned\n</code></pre>"},{"location":"ai-testbed/cerebras/system-overview/","title":"System Overview","text":"<p>The Cerebras CS-2 is a wafer-scale deep learning accelerator comprising 850,000 processing cores, each providing 48KB of dedicated SRAM memory for an on-chip total of 40GB and interconnected to optimize bandwidth and latency. Its software platform integrates popular machine learning frameworks such as TensorFlow and PyTorch.</p> <p>The ALCF CS-2 systems are configured as a Cerebras Wafer-Scale Cluster, designed to support large-scale models (up to and well beyond 1 billion parameters) and large-scale inputs. The cluster contains two CS-2 systems and can distribute jobs across one or both CS-2 systems in a data-parallel framework. The supporting CPU cluster consists of MemoryX, SwarmX, management, and input worker nodes. The Cerebras Wafer-Scale cluster is run as an appliance: a user submits a job to the appliance, and the appliance manages preprocessing and streaming of the data, IO, and device orchestration within the appliance. It provides programming via PyTorch and TensorFlow(estimator) with data-parallel distribution when using more than one CS-2. This installation supports both Pipelined execution for models up to 1 billion parameters and Weight Streaming execution for models up to and above 1 billion parameters.</p> <p>The public Cerebras documentation is available here.</p> <p>A typical Cerebras Wafer-Scale Cluster is shown in the figure. Users connect (ssh) to one of the three login nodes. Either ssh to <code>cerebras.ai.alcf.anl.gov</code>, which randomly resolves to one of cer-login-0[1-3].ai.alcf.anl.gov, or ssh to a specific node, <code>cer-login-01.ai.alcf.anl.gov</code>, <code>cer-login-02.ai.alcf.anl.gov</code>, <code>cer-login-03.ai.alcf.anl.gov</code>. The rest of the nodes in the cluster infrastructure are not directly accessible, except by admins. The trees <code>/home</code>, <code>/projects</code>, and <code>/software</code> are shared across all three login nodes, the relevant cluster infrastructure nodes, and all ALCF AI testbed platforms.</p> <p> </p> CS-2 cluster figure <p>(Figure from https://docs.cerebras.net/en/latest/wsc/cerebras-basics/how-cerebras-works.html)</p> <p>As indicated in the figures, the CS-2 nodes on the right are responsible only for running and accelerating the computations for training and predictions with the model. The other work, including compilation, is performed by input nodes, and by MemoryX nodes, which are used for weight storage and broadcast, and SwarmX nodes, which are used for gradient accumulation. Some model verification work can be done on login nodes.</p>"},{"location":"ai-testbed/cerebras/tunneling-and-forwarding-ports/","title":"Tunneling and Forwarding Ports","text":"<p>See ALCF's Jupyter Instructions, and Tunneling and forwarding ports. The Cerebras login nodes are direct login; tunneling and port forwarding do not involve jump hosts.</p>"},{"location":"ai-testbed/data-management/data-management-overview/","title":"Data Management for the AI Testbed","text":""},{"location":"ai-testbed/data-management/data-management-overview/#home-file-system-space","title":"Home File System Space","text":"<p>Users have a shared home filesystem <code>/home</code> shared across the ALCF AI testbed systems, including the login and compute nodes. Default user quota is <code>1 TB</code> storage and <code>1,000,000 files</code>. This space is backed up. </p>"},{"location":"ai-testbed/data-management/data-management-overview/#project-file-system-space","title":"Project File System Space","text":"<p>The team project/campaign file system <code>/projects</code> is intended to facilitate project collaboration and is accessible to the team members of your project that have an ALCF account.  Default group storage quota is <code>2 TB</code> and <code>2,000,000 files</code>. Please note that this space isn't backed up.  Our policy is that data will be purged from disk 6 months after project completion.</p>"},{"location":"ai-testbed/data-management/data-management-overview/#data-transfer","title":"Data Transfer","text":"<p>Users can transfer data to and from the AI testbed using <code>Globus</code> or tools such as <code>scp</code> or <code>rsync</code>.</p>"},{"location":"ai-testbed/data-management/data-management-overview/#using-globus","title":"Using Globus","text":"<p>We have a Globus endpoint each to move data to and from the <code>/projects</code> and <code>/home</code> filesystem respectively.</p> <ul> <li>Use <code>alcf#ai_testbed_projects</code> for the <code>/projects</code> file system</li> <li>Use <code>alcf#ai_testbed_home</code> for the <code>/home</code> files system </li> </ul> <p>Relevant information regarding using globus can be found here</p>"},{"location":"ai-testbed/data-management/data-management-overview/#alcf-storage-policies","title":"ALCF Storage Policies","text":"<p>ALCF data policies is available here </p> <p>Please Note: The basic level of protection provided is UNIX file level permissions; it is the user's responsibility to ensure that file permissions and umasks are set to match their needs.</p>"},{"location":"ai-testbed/files/notes/","title":"Notes","text":"<pre><code>git submodule init; git submodule update\n</code></pre>"},{"location":"ai-testbed/files/todo/","title":"TODO","text":""},{"location":"ai-testbed/files/todo/#cosmictagger-v1x","title":"CosmicTagger v1.x","text":"<p>Note: Conversion of CT to the various machines is meant to be a tutorial as to how to convert a model.</p>"},{"location":"ai-testbed/files/todo/#cerebras-ct","title":"Cerebras CT","text":"<p>Cerebras cannot support CT and UNets in general as of 4/25/23.</p>"},{"location":"ai-testbed/files/todo/#graphcore-ct","title":"Graphcore CT","text":"<p>Alex has been very busy with conferences, etc.</p> <p>He ran CT but, it ran on the CPU.  He has stated that it may need to be completely written using, I can't remember which, Poplar or PopArt.  If that is necessary, Venkat should make the call.</p>"},{"location":"ai-testbed/files/todo/#groq-ct","title":"Groq CT","text":""},{"location":"ai-testbed/files/todo/#habana-ct","title":"Habana CT","text":"<p>Repo:    https://github.com/argonne-lcf/user-guides.git Branch:  feature/Habana002-DNP File:    docs/ai-testbed/habana/CosmicTagger-Conversion.md</p>"},{"location":"ai-testbed/files/todo/#sambanova-ct","title":"SambaNova CT","text":"<p>SN has a highly-engineered version of CT.</p> <p>They are working to support CT OOB, Out-Of-Box.</p>"},{"location":"ai-testbed/files/todo/#cerebras","title":"Cerebras","text":"<p>Repo:    https://github.com/argonne-lcf/user-guides.git Branch:  Talk to Bill.</p>"},{"location":"ai-testbed/files/todo/#graphcore","title":"Graphcore","text":"<p>Repo:  https://github.com/argonne-lcf/user-guides.git</p> <p>When you change back to 3.2, use virtual-environments.md from the commit a4ce3b5598f4d6feee7ca58accde1a6a0ea84244 \"virtual-environments.md with 3.2 edits.\"</p>"},{"location":"ai-testbed/files/todo/#groq","title":"Groq","text":"<p>Repo:   https://github.com/argonne-lcf/user-guides.git Branch: feature/Groq001-DNP</p>"},{"location":"ai-testbed/files/todo/#habana","title":"Habana","text":"<p>Repo:  https://github.com/argonne-lcf/user-guides.git Branch:  feature/Habana002-DNP</p>"},{"location":"ai-testbed/files/todo/#sambanova","title":"SambaNova","text":"<p>Repo:  https://github.com/argonne-lcf/user-guides.git</p>"},{"location":"ai-testbed/graphcore/documentation/","title":"Documentation links","text":"<p>Poplar SDK PyTorch for the IPU: User Guide Targetting the IPU from Tensorflow 2 IPU programming guide Examples Examples Github Repo POD systems POD64 specs </p>"},{"location":"ai-testbed/graphcore/example-programs/","title":"Example Programs","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git. Clone the examples repository to your personal directory structure: <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#mnist-poptorch","title":"MNIST - PopTorch","text":""},{"location":"ai-testbed/graphcore/example-programs/#activate-poptorch-environment","title":"Activate PopTorch Environment","text":"<pre><code>source ~/venvs/graphcore/poptorch31_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements","title":"Install Requirements","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/tutorials/simple_applications/pytorch/mnist\npip install torchvision==0.14.0\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-mnist","title":"Run MNIST","text":"<p>Execute the command: <pre><code>/opt/slurm/bin/srun --ipus=1 python mnist_poptorch.py\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#output","title":"Output","text":"<p>The expected output will start with downloads followed by:</p> <pre><code>TrainingModelWithLoss(\n  (model): Network(\n    (layer1): Block(\n      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer2): Block(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer3): Linear(in_features=1600, out_features=128, bias=True)\n    (layer3_act): ReLU()\n    (layer3_dropout): Dropout(p=0.5, inplace=False)\n    (layer4): Linear(in_features=128, out_features=10, bias=True)\n    (softmax): Softmax(dim=1)\n  )\n  (loss): CrossEntropyLoss()\n)\nEpochs:   0%|\n...\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:16&lt;00:00]\nAccuracy on test set: 98.04%\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#mnist-tensorflow2","title":"MNIST - Tensorflow2","text":""},{"location":"ai-testbed/graphcore/example-programs/#activate-tensorflow2-environment","title":"Activate Tensorflow2 Environment","text":"<p>Create a TensorFlow2 environment as explained in the tensorflow-2-environment-setup and activate the same. <pre><code>source ~/venvs/graphcore/tensorflow2_31_env/bin/activate\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements_1","title":"Install Requirements","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/tutorials/simple_applications/tensorflow2/mnist/\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-mnist-tensorflow","title":"Run MNIST - TensorFlow","text":"<p>Execute the command:</p> <pre><code>/opt/slurm/bin/srun --ipus=1 python mnist.py\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#output_1","title":"Output","text":"<p>The expected output will start with downloads followed by:</p> <pre><code>2023-04-26 14:42:32.179566: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 3.1.0 (e12d5f9f01) Poplar package: 9c103dc348\n2023-04-26 14:42:34.517107: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1619] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n11501568/11490434 [==============================] - 0s 0us/step\n2023-04-26 14:42:35.673768: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2023-04-26 14:42:35.947832: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2023-04-26 14:42:46.953720: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nEpoch 1/4\n2000/2000 [==============================] - 13s 7ms/step - loss: 0.6238\nEpoch 2/4\n2000/2000 [==============================] - 0s 222us/step - loss: 0.3361\nEpoch 3/4\n2000/2000 [==============================] - 0s 225us/step - loss: 0.2894\nEpoch 4/4\n2000/2000 [==============================] - 0s 226us/step - loss: 0.2601\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#resnet50","title":"ResNet50","text":""},{"location":"ai-testbed/graphcore/example-programs/#activate-poptorch-environment_1","title":"Activate PopTorch Environment","text":"<p>Create and activate a fresh PopTorch environment <code>poptorch31_resnet50_env</code> as outlined in the virtual environment section, then activate it. <pre><code>source ~/venvs/graphcore/poptorch31_resnet50_env/bin/activate\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements_2","title":"Install Requirements","text":"<p>Change directory <pre><code>cd ~/graphcore/examples/vision/cnns/pytorch\nmake install \nmake install-turbojpeg\npip install torch==1.13.0\n</code></pre></p> <p>Note: For 3.1.0 sdk, use the torch=1.13.0 version for the compatible version.</p>"},{"location":"ai-testbed/graphcore/example-programs/#update-configsyml","title":"Update configs.yml","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/vision/cnns/pytorch/train\n</code></pre> Open configs.yml with your favorite editor. Find in the resnet50 section <pre><code>use_bbox_info: true\n</code></pre> and change it to: <pre><code>use_bbox_info: false\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-resnet50","title":"Run ResNet50","text":"<p>The scripts to train a ResNet50 PyTorch model on Pod4 is located at https://github.com/graphcore/examples/tree/master/vision/cnns/pytorch/train</p> <p>Set the following environmental variables. <pre><code>mkdir -p ~/graphcore/tmp/pt_cache/\nexport PYTORCH_CACHE_DIR=~/graphcore/tmp/pt_cache/\n</code></pre> The command to run 4 replicas (a total for 4 IPUs) of the ResNet50 model is as follows. <pre><code>/opt/slurm/bin/srun --ipus=4 poprun -vv --num-instances=1 --num-replicas=4 --executable-cache-path=$PYTORCH_CACHE_DIR python3 /home/$USER/graphcore/examples/vision/cnns/pytorch/train/train.py --config resnet50-pod4 --imagenet-data-path /mnt/localdata/datasets/imagenet-raw-dataset --epoch 2 --validation-mode none --dataloader-worker 14 --dataloader-rebatch-size 256\n</code></pre> This model is run with the imagenet dataset.</p>"},{"location":"ai-testbed/graphcore/example-programs/#output_2","title":"Output","text":"<pre><code>04:22:59.948 3905692 POPRUN [I] V-IPU server address picked up from 'vipu': 10.1.3.101:8090\n04:22:59.950 3905692 POPRUN [I] Using V-IPU partition slurm_2657 as it is the only one available\n04:22:59.950 3905692 POPRUN [D] Connecting to 10.1.3.101:8090\n04:22:59.951 3905692 POPRUN [D] Status for partition slurm_2657: OK (error 0)\n04:22:59.951 3905692 POPRUN [I] Partition slurm_2657 already exists and is in state: PS_ACTIVE\n04:22:59.952 3905692 POPRUN [D] The reconfigurable partition slurm_2657 is OK\n ===========================\n|      poprun topology      |\n|===========================|\n| hosts     | gc-poplar-02  |\n|-----------|---------------|\n| ILDs      |       0       |\n|-----------|---------------|\n| instances |       0       |\n|-----------|---------------|\n| replicas  | 0 | 1 | 2 | 3 |\n ---------------------------\n04:22:59.952 3905692 POPRUN [D] Target options from environment: {}\n04:22:59.952 3905692 POPRUN [D] Target options from V-IPU partition: {\"ipuLinkDomainSize\":\"4\",\"ipuLinkConfiguration\":\"default\",\"ipuLinkTopology\":\"mesh\",\"gatewayMode\":\"true\",\"instanceSize\":\"4\"}\n04:22:59.998 3905692 POPRUN [D] Found 1 devices with 4 IPUs\n04:23:00.689 3905692 POPRUN [D] Attached to device 6\n04:23:00.689 3905692 POPRUN [I] Preparing parent device 6\n04:23:00.689 3905692 POPRUN [D] Device 6 ipuLinkDomainSize=64, ipuLinkConfiguration=Default, ipuLinkTopology=Mesh, gatewayMode=true, instanceSize=4\n[1,0]&lt;stdout&gt;:[INFO] Total replicas: 4\n[1,0]&lt;stdout&gt;:[INFO] Global batch size: 16416\n[1,0]&lt;stdout&gt;:[INFO] Number of IPUs required: 4\n[1,0]&lt;stdout&gt;:[INFO] Loading the data\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [06:26&lt;00:00][1,0]&lt;stderr&gt;:WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile-tile it was set to 1471)\n[1,0]&lt;stderr&gt;:2023-04-27T04:30:33.475912Z PO:ENGINE   3906481.3906481 W: WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile-tile it was set to 1471)\n[1,0]&lt;stderr&gt;:2023-04-27T04:30:36.928499Z popart:session 3906481.3906481 W: Rng state buffer was not serialized.You did not load poplar Engine.Remember that if you would like to run the model using the model runtime then you have to create your own buffer and callback in your model runtime application for rngStateTensor.\n[1,0]&lt;stderr&gt;:\nLoss:6.7615 | Accuracy:0.57%:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 75/78 [11:07&lt;00:10,  3.62s/it][1,0[1,0]&lt;stdout&gt;:[INFO] Epoch 1\n[1,0]&lt;stdout&gt;:[INFO] loss: 6.7508,\n[1,0]&lt;stdout&gt;:[INFO] accuracy: 0.61 %\n[1,0]&lt;stdout&gt;:[INFO] throughput: 1886.4 samples/sec\n[1,0]&lt;stdout&gt;:[INFO] Epoch 2/2\nLoss:6.7508 | Accuracy:0.61%: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [11:18&lt;00:00,  8.70s/it][1,0]&lt;stderr&gt;:\nLoss:6.2860 | Accuracy:2.41%:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 75/7[1,0]&lt;stdout&gt;:[INFO] Epoch 2,0]&lt;stderr&gt;:\n[1,0]&lt;stdout&gt;:[INFO] loss: 6.2747,\n[1,0]&lt;stdout&gt;:[INFO] accuracy: 2.48 %\n[1,0]&lt;stdout&gt;:[INFO] throughput: 4476.7 samples/sec\n[1,0]&lt;stdout&gt;:[INFO] Finished training. Time: 2023-04-27 04:40:05.821555. It took: 0:16:04.818638\nLoss:6.2747 | Accuracy:2.48%: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [04:46&lt;00:00,  3.67s/it][1,0]&lt;stderr&gt;:\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#gpt-2-pytorch-pod16-run","title":"GPT-2 PyTorch - POD16 run","text":"<p>The scripts to train a GPT-2 pytorch model on the POD16 are located at https://github.com/graphcore/examples/tree/master/nlp/gpt2/pytorch</p> <p>In order to run the GPT-2 Pytorch model, create a new popTorch virtual environment poptorch31_gpt2 as described in the virtual environment section and activate it.</p> <pre><code>source ~/venvs/graphcore/poptorch31_gpt2/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements_3","title":"Install Requirements","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/nlp/gpt2/pytorch\npip3 install -r requirements.txt\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-gpt2-on-16-ipus","title":"Run GPT2 on 16 IPUs","text":"<p>The command for the GPT2 model is as follows is as follows. <pre><code>/opt/slurm/bin/srun --ipus=16 python /home/$USER/graphcore/examples/nlp/gpt2/pytorch/train_gpt2.py --model gpt2 --ipus-per-replica 4 --replication-factor 4 --gradient-accumulation 2048 --device-iterations 8 --batch-size 1 --layers-per-ipu 0 4 4 4 --matmul-proportion 0.15 0.15 0.15 0.15 --max-len 1024 --optimizer AdamW --learning-rate 0.00015 --lr-schedule cosine --lr-warmup 0.01 --remap-logit True --enable-sequence-serialized True --embedding-serialization-factor 4 --recompute-checkpoint-every-layer True --enable-half-partials True --replicated-tensor-sharding True --dataset 'generated' --epochs 1\n</code></pre> It runs a <code>gpt2</code> model that fits on 4 IPUS indicated by <code>--ipus-per-replica</code>. The <code>--replication-factor</code> indicates how many times the model is replicated in a data parallel manner (4 in the above example). Hence the total number of IPUs used in this example is 16.</p> <p>The effective global batch size in this example is (micro)batch-size * gradient-accumulation * replication-factor = 1 x 2048 x 4 = 8192.  The device iterations indicates the total number samples loaded in 1 training step = global batch size * device iterations = 8192*8 = 65536. To learn more about these parameters and in general batching of IPUs refer IPU batching .</p> <p>The above example is running with <code>generated</code> or <code>synthetic data</code>. To use the same example with a real world dataset, refer to data setup.</p>"},{"location":"ai-testbed/graphcore/example-programs/#output_3","title":"Output","text":"<pre><code>Building (if necessary) and loading remap_tensor_ce.\nFailed to find compiled extension; rebuilding.\nBuilding (if necessary) and loading residual_add_inplace_pattern.\nModel initializing\n-------------------- Device Allocation --------------------\nEmbedding  --&gt; IPU 0\nLayer 0  --&gt; IPU 1\nLayer 1  --&gt; IPU 1\nLayer 2  --&gt; IPU 1\nLayer 3  --&gt; IPU 1\nLayer 4  --&gt; IPU 2\nLayer 5  --&gt; IPU 2\nLayer 6  --&gt; IPU 2\nLayer 7  --&gt; IPU 2\nLayer 8  --&gt; IPU 3\nLayer 9  --&gt; IPU 3\nLayer 10 --&gt; IPU 3\nLayer 11 --&gt; IPU 3\nLM_head --&gt; IPU 0\nArguments: Namespace(async_dataloader=False, auto_loss_scaling=False, batch_size=1, checkpoint_input_dir='', checkpoint_output_dir=None, compile_only=False, custom_ops=True, dataset='generated', device_iterations=8, embedding_serialization_factor=4, enable_half_partials=True, enable_sequence_serialized=True, epochs=1, executable_cache_dir=None, gradient_accumulation=2048, input_files=None, ipus_per_replica=4, layers_per_ipu=[0, 4, 4, 4], learning_rate=0.00015, log_steps=1, loss_scaling=50000.0, lr_decay_steps=None, lr_schedule='cosine', lr_warmup=0.01, lr_warmup_steps=None, matmul_proportion=[0.15, 0.15, 0.15, 0.15], max_len=1024, model='gpt2', num_workers=4, optimizer='AdamW', optimizer_state_offchip=True, recompute_checkpoint_every_layer=True, recompute_checkpoint_layers=None, remap_logit=True, replicated_tensor_sharding=True, replication_factor=4, resume_training_from_checkpoint=False, save_per_epochs=1, save_per_steps=None, seed=1234, serialized_seq_len=128, stride=128, training_steps=10000, use_popdist=False, use_wandb=False, val_num=0, weight_decay=0.0)\nModel config: GPT2Config {\n  \"activation_function\": \"gelu\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50272,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50272,\n  \"gradient_checkpointing\": false,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"output_past\": true,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 400\n    }\n  },\n  \"transformers_version\": \"4.26.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50272\n}\n------------------- Data Loading Started ------------------\nloading training dataset and validating dataset\nSamples per epoch: 262144\nSteps per epoch: 4\nData loaded in 2.358953586081043 secs\n-----------------------------------------------------------\n--------------------- Training Started --------------------\nGraph compilation:   4%|\u258d         | 4/100 [00:29&lt;11:57]2023-04-27T03:39:53.291853Z PL:POPLIN    3888383.3888383 W: poplin::preplanMatMuls() is deprecated! Use poplin::preplan() instead\nMatMuls() is deprecated! Use poplin::preplan() instead\n2023-04-27T03:39:55.159194Z PL:POPLIN    3888383.3888383 W: poplin::preplanMatMuls() is deprecated! Use poplin::preplan() instead\n2023-04-27T03:39:56.958834Z PL:POPLIN    3888383.3888383 W: poplin::preplanMatMuls() is deprecated! Use poplin::preplan() instead\n2023-04-27T03:39:58.748727Z PL:POPLIN    3888383.3888383 W: poplin::preplanMatMuls() is deprecated! Use poplin::preplan() instead\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [28:04&lt;00:00]WARNING: The compile time engine option debug.branchRecordTile is set to \"23551\" when creating the Engine. (At compile-tile it was set to 5887)\n2023-04-27T04:07:29.993259Z PO:ENGINE   3888383.3888383 W: WARNING: The compile time engine option debug.branchRecordTile is set to \"23551\" when creating the Engine. (At compile-tile it was set to 5887)\n2023-04-27T04:07:42.941039Z popart:session 3888383.3888383 W: Rng state buffer was not serialized.You did not load poplar Engine.Remember that if you would like to run the model using the model runtime then you have to create your own buffer and callback in your model runtime application for rngStateTensor.\n[04:09:02.177] [poptorch::python] [warning] Ignoring unexpected optimizer attribute in ADAMW_NO_BIAS optimizer: ['step', '_step_count']\nIgnoring unexpected optimizer attribute in ADAMW_NO_BIAS optimizer: ['step', '_step_count']\n[04:09:02.179] [poptorch::python] [warning] Ignoring unexpected group 0 attribute in ADAMW_NO_BIAS optimizer: ['initial_lr']\nIgnoring unexpected group 0 attribute in ADAMW_NO_BIAS optimizer: ['initial_lr']\n[04:09:02.179] [poptorch::python] [warning] Ignoring unexpected group 1 attribute in ADAMW_NO_BIAS optimizer: ['initial_lr']\nIgnoring unexpected group 1 attribute in ADAMW_NO_BIAS optimizer: ['initial_lr']\nstep 0 of epoch 0, loss: 10.913212776184082, acc: 2.0116567611694336e-05, lr: 0.00012803300858899104, throughput: 36.69187444207895 samples/sec\nstep 1 of epoch 0, loss: 10.836352348327637, acc: 1.9758939743041992e-05, lr: 7.5e-05, throughput: 1064.3232077940409 samples/sec\nstep 2 of epoch 0, loss: 10.83123779296875, acc: 2.0459294319152832e-05, lr: 2.1966991411008938e-05, throughput: 1064.3064018230857 samples/sec\nstep 3 of epoch 0, loss: 10.829036712646484, acc: 1.9878149032592773e-05, lr: 0.0, throughput: 1064.4397806661352 samples/sec\n</code></pre> <p>Note: The graph compilation for a large model like GPT-2 takes about half an hour. </p>"},{"location":"ai-testbed/graphcore/getting-started/","title":"Getting Started","text":"<p>Connection to a Graphcore node is a two-step process.</p> <p>The first step is to ssh from a local machine to the login node.</p> <p>The second step is to log in to a Graphcore node from the login node.</p> <p></p>"},{"location":"ai-testbed/graphcore/getting-started/#log-in-to-login-node","title":"Log in to Login Node","text":"<p>Login to the Graphcore login node from your local machine using the below command. This uses the ALCF account ID that uses the password generated from the MobilePASS+.</p> <p>Note:  In the examples below, replace ALCFUserID with your ALCF user id.</p> <pre><code>ssh ALCFUserID@gc-login-01.ai.alcf.anl.gov\n# or\nssh ALCFUserID@gc-login-02.ai.alcf.anl.gov\n</code></pre> <p>Note: Use the ssh \"-v\" option in order to debug any ssh problems.</p>"},{"location":"ai-testbed/graphcore/getting-started/#log-in-to-a-graphcore-node","title":"Log in to a Graphcore Node","text":"<p>Once you are on the login node, ssh to one of the Graphcore nodes.</p> <pre><code>ssh gc-poplar-02.ai.alcf.anl.gov\n# or\nssh gc-poplar-03.ai.alcf.anl.gov\n# or\nssh gc-poplar-04.ai.alcf.anl.gov\n</code></pre> <p>**Note: <code>ssh gc-poplar-01.ai.alcf.anl.gov</code> is not accessible to users. However, its IPU resources are assigned by the slurm tasks.</p>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/","title":"Job Queueing and Submission","text":""},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#introduction","title":"Introduction","text":"<p>ALCF's Graphcore POD64 system uses Slurm for job submission and queueing. Below are some of the important commands for using Slurm. For more information refer to Slurm Documentation.</p> <p>NOTE: Jobs that require IPUs will fail unless launched with <code>srun</code> or <code>sbatch</code>. NOTE: There is a single Slurm scheduler for the Graphcore POD64.</p>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#srun","title":"SRun","text":"<p>The Slurm command <code>srun</code> can be used to run individual Python scripts (or other programs) in parallel with other scripts on a cluster managed by Slurm. An example of <code>srun</code> usage is shown below. Use the <code>--ipus=</code> option to specify the number of IPUs required for the run.</p> <p>Example:</p> <pre><code>srun --ipus=1 python mnist_poptorch.py\n</code></pre>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#sbatch","title":"SBatch","text":"<p>Alternatively, these jobs can be submitted to the Slurm workload manager through a batch script by using the <code>sbatch</code> command. To do this, create a bash script (submit-mnist-poptorch-job.sh here as an example) with the commands that you want to execute.</p> <pre><code>#!/bin/sh\npython mnist_poptorch.py\n</code></pre> <p>Then pass the bash script as an input to the <code>sbatch</code> command as shown below, requesting the number of IPUs required:</p> <pre><code>sbatch --ipus=1 --output=mnist-poptorch-output.log submit-mnist-poptorch-job.sh\n</code></pre>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#squeue","title":"SQueue","text":"<p>The <code>squeue</code> command provides information about jobs located in the Slurm scheduling queue.</p> <pre><code>$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n              2572       p64 Graphcor username  R       1:12      1 gc-poplar-02\n</code></pre>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#sinfo","title":"SInfo","text":"<p>SInfo is used to view partition and node information for a system running Slurm.</p> <pre><code>$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\np64*         up   infinite      3   idle gc-poplar-[02-04]\n</code></pre> <p>For more information, see SInfo.</p>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#scancel","title":"SCancel","text":"<p>SCancel is used to signal or cancel jobs, job arrays, or job steps.</p> <pre><code>scancel job_id\n</code></pre>"},{"location":"ai-testbed/graphcore/miscellaneous/","title":"Miscellaneous","text":""},{"location":"ai-testbed/graphcore/miscellaneous/#status","title":"Status","text":""},{"location":"ai-testbed/graphcore/miscellaneous/#gc-monitor","title":"GC-Monitor","text":"<p>The command <code>gc-monitor</code> is Graphcore's device usage monitor. Run it as follows for ordinary monitoring. See <code>gc-monitor --help</code> for other options.</p> <pre><code>export IPUOF_VIPU_API_HOST=10.1.3.101\ngc-monitor --no-card-info --all-partitions\n# or watch gc-monitor --no-card-info --all-partitions\n</code></pre> <p>Note: if there are no partitions active, gc-monitor will core dump: <code>Segmentation fault (core dumped)</code></p> <p>The output will look something like:</p> <pre><code>+--------------------------------------------------------------+-----------------------+\n|      IPUs in slurm_2616 attached from other namespaces       |         Board         |\n+----+------------------------------+--------------+-----------+-----------+-----------+\n| ID |       Application host       |    Clock     |   Temp    |   Temp    |   Power   |\n+----+------------------------------+--------------+-----------+-----------+-----------+\n| 0  |         gc-poplar-02         |   1850MHz    |  24.2 C   |  21.1 C   |  92.3 W   |\n+----+------------------------------+--------------+-----------+-----------+-----------+\n</code></pre>"},{"location":"ai-testbed/graphcore/miscellaneous/#gc-info","title":"GC-Info","text":"<p>The command <code>gc-info</code> is used to display device information. See <code>gc-info --help</code> for more options.</p> <p>To list devices,  <pre><code>gc-info -l\n</code></pre></p> <p>The command <code>gc-info</code> lists the partition and different IPU Id's along with the multi-IPU configuration IDs.</p> <pre><code>-+- Id:  [0], target: [Fabric], IPU-M host:  [10.1.5.1], IPU#: [3]\n-+- Id:  [1], target: [Fabric], IPU-M host:  [10.1.5.1], IPU#: [2]\n-+- Id:  [2], target: [Fabric], IPU-M host:  [10.1.5.1], IPU#: [1]\n</code></pre> <p>One may also display detailed information for a specific device.  The devices are numbered 0-63.  For example,</p> <pre><code>gc-info --device-id 0 --device-info\n</code></pre> <p>See <code>gc-info --help</code> for more information.</p>"},{"location":"ai-testbed/graphcore/miscellaneous/#how-busy-is-the-system","title":"How busy is the system?","text":"<p>Use one of</p> <pre><code>top\nhtop\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/","title":"Steps to Run a Model/Program","text":"<p>Note:  Please be mindful of how you are using the system. For example, consider running larger jobs in the evening or on weekends.</p> <p>Running of any model or application includes graph compilation of the model that is then deployed on the IPUs. Below is the description of training a neural network for classification on the MNIST dataset using the PopTorch (pytorch framework optimized for IPU).</p>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#examples-repo","title":"Examples Repo","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git.</p> <p>Clone the examples repository to your personal directory structure, and checkout the v3.1.0 release:</p> <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\ncd examples\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#mnist","title":"MNIST","text":""},{"location":"ai-testbed/graphcore/running-a-model-or-program/#activate-poptorch-environment","title":"Activate PopTorch Environment","text":"<p>Follows the steps at Poptorch environment setup to enable the Poplar SDK.</p> <pre><code>source ~/venvs/graphcore/poptorch31_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#install-requirements","title":"Install Requirements","text":"<p>Change directory and install packages specific to the MNIST model:</p> <pre><code>cd ~/graphcore/examples/tutorials/simple_applications/pytorch/mnist\npython -m pip install torchvision==0.14.0\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#run-mnist","title":"Run MNIST","text":"<p>Execute the command:</p> <pre><code>/opt/slurm/bin/srun --ipus=1 python mnist_poptorch.py\n</code></pre> <p>All models are run using Slurm, with the <code>--ipus</code> indicating how many IPUs are need to be allocated for the model being run. This example uses a batchsize of 8, and run for 10 epochs. It also set the device iteration to 50 which is the number of iterations the device should run over the data before returning to the user.  The dataset used in the example is derived from the TorchVision and the PopTorch dataloader is used to load the data required for the 50 device iterations from the host to the device in a single step.</p> <p>The model used here is a simple CNN based model with an output from a classifier (softmax layer). A simple Pytorch model is translated to a PopTorch model using <code>poptorch.Options()</code>. <code>poptorch.trainingModel</code> is the model wrapping function on the Pytorch model. The first call to <code>trainingModel</code> will compile the model for the IPU. You can observe the compilation process as part of output of the above command.</p> <pre><code>Graph compilation:   3%|\u258e         | 3/100 [00:00&lt;00:03]2023-04-26T16:53:21.225944Z PL:POPLIN    3680893.3680893 W: poplin::preplanMatMuls() is deprecated! Use poplin::preplan() instead\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:20&lt;00:00]2023-04-26T16:53:38.241395Z popart:session 3680893.3680893\n</code></pre> <p>The artifacts from the graph compilations is cached in the location set by the flag <code>POPTORCH_CACHE_DIR</code>, where the <code>.popef</code> file corresponding to the model under consideration is cached.</p>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#output","title":"Output","text":"<p>The expected output will start with downloads followed by and we can observe the model used by the model, the progress bar of the compilation process, and the training progress bar.</p> <pre><code>srun: job 2623 queued and waiting for resources\nsrun: job 2623 has been allocated resources\n/home/arnoldw/workspace/poptorch31.env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n  warn(f\"Failed to load image Python extension: {e}\")\nEpochs:   0%|          | 0/10 [00:00&lt;?,[16:58:56.683] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 10\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:20&lt;00:00]\nEpochs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:35&lt;00:00,  9.57s/it]\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:13&lt;00:00]\nTrainingModelWithLoss(%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 97/100 [00:13&lt;00:01]\n  (model): Network(\n    (layer1): Block(\n      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer2): Block(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer3): Linear(in_features=1600, out_features=128, bias=True)\n    (layer3_act): ReLU()\n    (layer3_dropout): Dropout(p=0.5, inplace=False)\n    (layer4): Linear(in_features=128, out_features=10, bias=True)\n    (softmax): Softmax(dim=1)\n  )\n  (loss): CrossEntropyLoss()\n)\nAccuracy on test set: 98.59%\n</code></pre> <p>Refer to the script to learn more about this example.</p> <p>Example Programs lists the different example applications with corresponding commands for each of the above steps.</p>"},{"location":"ai-testbed/graphcore/system-overview/","title":"System Overview","text":"<p>The Graphcore Bow-Pod64 system is the latest-generation AI accelerator from Graphcore. This is a one-rack system consisting of 64 Bow-class Intelligence Processing Units (IPU) with a custom interconnect. The system provides for an aggregate 22 Petaflops/s of performance in half precision. It has a total of 57.6 GB In-Processor-Memory with a total of 94,208 IPU cores. The system consists of four servers for data-processing. </p> <p>For more details refer to the POD64 spec </p> <p> (Figure from https://www.graphcore.ai/products/poplar)</p> <p>The Graphcore software stack includes support for TensorFlow and PyTorch using the Poplar SDK. The Poplar\u00ae SDK is t is the toolchain specifically designed for creating graph software for ML applications.  It integrates with the traditional ML frameworks like PyTorch and TensorFlow allowing users to port their existing code to the IPU hardware-specific code. The various components of the poplar SDK stack are shown in the figure. It includes the PopTorch framework which is a wrapper over the PyTorch framework optimized to the IPU hardware. It also enlists the different PopLibs libraries supported, which enables to construct graphs, define tensor data and control how the code and data are mapped onto the IPU for execution.  </p>"},{"location":"ai-testbed/graphcore/virtual-environments/","title":"Virtual Environments","text":""},{"location":"ai-testbed/graphcore/virtual-environments/#poplar-sdk-setup","title":"Poplar SDK Setup","text":"<p>The Poplar SDK is downloaded onto the graphcore systems at the <code>/software/graphcore/poplar_sdk/</code> location. The default poplar version (3.1.0) is enabled automatically upon logging into a graphcore node.</p> <p>Check if Poplar is setup correctly:</p> <pre><code>popc --version\n</code></pre> <p>One should see:</p> <pre><code>POPLAR version 3.1.0 (e12d5f9f01)\nclang version 15.0.0 (bab932b4fc4cdb58bb009370384b2c41579bd9d9)\n</code></pre> <p>If the Poplar SDK is not enabled, it can be enabled with <pre><code>source /software/graphcore/poplar_sdk/3.1.0/enable\n</code></pre></p> <p>To disable the current Poplar SDK, e.g. if one wants to use a different Poplar SDK,</p> <pre><code>unset POPLAR_SDK_ENABLED\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#miscellaneous-environment-variables","title":"Miscellaneous Environment Variables","text":"<pre><code>mkdir ~/tmp\nexport TF_POPLAR_FLAGS=--executable_cache_path=~/tmp\nexport POPTORCH_CACHE_DIR=~/tmp\n\nexport POPART_LOG_LEVEL=WARN\nexport POPLAR_LOG_LEVEL=WARN\nexport POPLIBS_LOG_LEVEL=WARN\n\nexport PYTHONPATH=/software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/python:$PYTHONPATH\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#poptorch-environment-setup","title":"PopTorch Environment Setup","text":"<p>PopTorch is an extension of the Pytorch framework that is optimized for the IPU specific functionality. To activate the PopTorch environment, first create a virtual environment and activate it.</p> <pre><code>mkdir -p ~/venvs/graphcore\nvirtualenv ~/venvs/graphcore/poptorch31_env\nsource ~/venvs/graphcore/poptorch31_env/bin/activate\n</code></pre> <p>Use the following commands to install the PopTorch environment.</p> <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.1.0\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\npip install $POPLAR_SDK_ROOT/poptorch-3.1.0+98660_0a383de63f_ubuntu_20_04-cp38-cp38-linux_x86_64.whl\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#tensorflow-2-environment-setup","title":"TensorFlow 2 Environment Setup","text":"<p>The Poplar SDK provides TensorFlow and Keras wheels built on 2.6 that includes the IPU specific functionality and optimized for the AMD processors. It can be installed as follows.</p> <p>Create virtual environment.</p> <pre><code>virtualenv ~/venvs/graphcore/tensorflow2_31_env\nsource ~/venvs/graphcore/tensorflow2_31_env/bin/activate\n</code></pre> <p>Install the TensorFlow and Keras wheels.</p> <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.1.0\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\npip install $POPLAR_SDK_ROOT/tensorflow-2.6.3+gc3.1.0+246224+2b7af067dae+amd_znver1-cp38-cp38-linux_x86_64.whl\npip install $POPLAR_SDK_ROOT/keras-2.6.0+gc3.1.0+246230+88e2debf-py2.py3-none-any.whl\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#verify-installation","title":"Verify Installation","text":"<pre><code>python -c \"from tensorflow.python import ipu\"\n</code></pre> <p>You should see:</p> <pre><code>2023-04-24 23:13:36.091496: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 3.1.0 (e12d5f9f01) Poplar package: 9c103dc348\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#installing-packages","title":"Installing Packages","text":"<p>Install packages in the normal manner such as:</p> <pre><code>python3 -m pip install \"some_package\"\n</code></pre> <p>For more details see Use pip for installing.</p> <p>To install a different version of a package that is already installed in one's environment, one can use:</p> <pre><code>pip install --ignore-installed  ... # or -I\n</code></pre> <p>Note: Conda is not supported on the Graphcore system.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/","title":"Scaling ResNet50","text":"<p>Follow all the instructions in Getting Started to log into a Graphcore node.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#examples-repo","title":"Examples Repo","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git.</p> <p>Clone the examples repository to your personal directory structure:</p> <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#environment-setup","title":"Environment Setup","text":"<p>Establish a virtual environment.</p> <pre><code>mkdir -p ~/venvs/graphcore\nrm -rf ~/venvs/graphcore/poptorch31_rn50_env\nvirtualenv ~/venvs/graphcore/poptorch31_rn50_env\nsource ~/venvs/graphcore/poptorch31_rn50_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#install-poptorch","title":"Install PopTorch","text":"<p>Install PopTorch.</p> <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.1.0\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\npip install $POPLAR_SDK_ROOT/poptorch-3.1.0+98660_0a383de63f_ubuntu_20_04-cp38-cp38-linux_x86_64.whl\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#environment-variables","title":"Environment Variables","text":"<p>Establish the following environment variables.</p> <pre><code>mkdir ${HOME}/tmp\nexport TF_POPLAR_FLAGS=--executable_cache_path=${HOME}/tmp\nexport POPTORCH_CACHE_DIR=${HOME}/tmp\nexport POPART_LOG_LEVEL=WARN\nexport POPLAR_LOG_LEVEL=WARN\nexport POPLIBS_LOG_LEVEL=WARN\nexport PYTHONPATH=/software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/python:$PYTHONPATH\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#install-requirements","title":"Install Requirements","text":"<pre><code>cd ${HOME}/graphcore/examples/vision/cnns/pytorch/\nmake install\nmake install-turbojpeg\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#one-time-per-user-ssh-key-set-up","title":"One-time per user ssh key set up","text":"<p>Set up the ssh key on gc-poplar-01.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#gc-poplar-01","title":"Gc-poplar-01","text":"<p>On gc-poplar-01:</p> <pre><code>mkdir ~/.ssh\ncd ~/.ssh\nssh-keygen -t rsa -b 4096\n#Accecpt default filename of id_rsa\n#Enter passphrase (empty for no passphrase):\n#Enter same passphrase again:\ncat id_rsa.pub &gt;&gt; authorized_keys\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-01 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-02 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-03 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-04 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#benchmarksyml","title":"<code>benchmarks.yml</code>","text":"<p>Update ${HOME}/graphcore/examples/vision/cnns/pytorch/train/benchmarks.yml with your favorite editor to match benchmarks.yml.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#configsyml","title":"<code>configs.yml</code>","text":"<p>Update ${HOME}/graphcore/examples/vision/cnns/pytorch/train/configs.yml with your favorite editor.  At about line 30, change use_bbox_info: true to use_bbox_info: false.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#scale-resnet50","title":"Scale ResNet50","text":"<p>Scale and benchmark ResNet50.</p> <p>Note: The number at the end of each line indicates the number of IPUs.</p> <p>Note: Use screen because every run is long.</p> <p>\"PopRun exposes this control with the --process-placement flag and provides multiple pre-defined strategies. By default (and with --process-placement spreadnuma), PopRun is designed to be NUMA-aware. On each host, all the available NUMA nodes are divided among the instances. This means that each instance is bound to execute on and allocate memory from its assigned NUMA nodes, ensuring memory access locality. This strategy maximises memory bandwidth and is likely to yield optimal performance for most of the data loading workloads in machine learning.\" [Multi-Instance Multi-Host(https://docs.graphcore.ai/projects/poprun-user-guide/en/latest/launching.html#multi-instance-multi-host)</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#setup","title":"Setup","text":"<p>Move to the correct directory and establish the datasets directory.</p> <pre><code>cd ${HOME}/graphcore/examples/vision/cnns/pytorch/train\nexport DATASETS_DIR=/mnt/localdata/datasets/\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#scaling-to-16-ipus","title":"Scaling to 16 IPUs","text":"<p>One may use any of the following commands to run ResNet50 on one to sixteen IPUs.</p> <pre><code>python3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_1\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_2\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_4\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_8\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod16\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#scaling-to-64-ipus","title":"Scaling to 64 IPUs","text":"<p>Note: One must complete the instructions on Multi-node Setup before running this example.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#establish-environment-variables","title":"Establish Environment Variables","text":"<pre><code>HOST1=`ifconfig eno1 | grep \"inet \" | grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' | head -1`\nOCT123=`echo \"$HOST1\" | cut -d \".\" -f 1,2,3`\nOCT4=`echo \"$HOST1\" | cut -d \".\" -f 4`\nHOST2=$OCT123.`expr $OCT4 + 1`\nHOST3=$OCT123.`expr $OCT4 + 2`\nHOST4=$OCT123.`expr $OCT4 + 3`\nexport HOSTS=$HOST1,$HOST2,$HOST3,$HOST4\nexport CLUSTER=c16\nexport IPUOF_VIPU_API_PARTITION_ID=p64\nexport TCP_IF_INCLUDE=$OCT123.0/8\nexport IPUOF_VIPU_API_HOST=$HOST1\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#64-ipu-run","title":"64 IPU Run","text":"<p>This runs to convergence.  It uses all 64 IPUs for more than 12 hours.</p> <p>Note: This should only be used if absolutely required.</p> <p>Execute:</p> <pre><code>python3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod64\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod64_conv\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#benchmark-results","title":"Benchmark Results","text":""},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#one-ipu","title":"One IPU","text":"<pre><code>[INFO] 2022-12-16 17:07:32: Total runtime: 3956.836479 seconds\n[INFO] 2022-12-16 17:07:32:    throughput = '7527.626315789474'\n[INFO] 2022-12-16 17:07:32:    accuracy = '57.41'\n[INFO] 2022-12-16 17:07:32:    loss = '2.8153'\n[INFO] 2022-12-16 17:07:33:    Total compile time: 429.59 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#two-ipus","title":"Two IPUs","text":"<pre><code>[INFO] 2022-12-16 15:56:23: Total runtime: 5866.494071 seconds\n[INFO] 2022-12-16 15:56:23:    throughput = '4798.778947368421'\n[INFO] 2022-12-16 15:56:23:    accuracy = '68.23'\n[INFO] 2022-12-16 15:56:23:    loss = '2.3148'\n[INFO] 2022-12-16 15:56:24:    Total compile time: 418.75 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#four-ipus","title":"Four IPUs","text":"<pre><code>[INFO] 2022-12-16 04:05:28: Total runtime: 3070.994553 seconds\n[INFO] 2022-12-16 04:05:28:    throughput = '9959.821052631578'\n[INFO] 2022-12-16 04:05:28:    accuracy = '67.76'\n[INFO] 2022-12-16 04:05:28:    loss = '2.338'\n[INFO] 2022-12-16 04:05:29:    Total compile time: 377.4 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#eight-ipus","title":"Eight IPUs","text":"<pre><code>[INFO] 2022-12-16 02:46:45: Total runtime: 1831.437598 seconds\n[INFO] 2022-12-16 02:46:45:    throughput = '19865.263157894733'\n[INFO] 2022-12-16 02:46:45:    accuracy = '64.94'\n[INFO] 2022-12-16 02:46:45:    loss = '2.4649'\n[INFO] 2022-12-16 02:46:46:    Total compile time: 386.27 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#sixteen-ipus","title":"Sixteen IPUs","text":"<p>Epochs: 20</p> <pre><code>[INFO] 2022-12-15 22:01:14: Total runtime: 1297.274336 seconds\n[INFO] 2022-62:01:14:    throughput = '39057.447368421046'\n[INFO] 2022-12-15 22:01:14:    accuracy = '57.43'\n[INFO] 2022-12-15 22:01:14:    loss = '2.8162'\n[INFO] 2022-12-15 22:01:16:    Total compile time: 397.08 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#sixty-four-ipus","title":"Sixty-Four IPUs","text":"<pre><code>[1,0]&lt;stdout&gt;:[INFO] loss: 4.8367,\n[1,0]&lt;stdout&gt;:[INFO] accuracy: 18.83 %\n[1,0]&lt;stdout&gt;:[INFO] throughput: 51368.5 samples/sec\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/","title":"CosmicTagger Conversion","text":"<p>The intent of this page is to show conceptually how to convert a model to run on the Graphcore system. It is not necessary to convert CosmicTagger because it has already been converted and is located at CosmicTagger on the Graphcore branch. The original is located at CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#run-model-on-cpu","title":"Run Model on CPU","text":"<p>The first step to converting a model is to verify that it runs on the CPU.  This step has been verified for CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#configpy","title":"Config.py","text":"<p>CosmicTagger can run on multiple machines.  As such, it is necessary to specify the architecture that one is using.  For example, CPU or GPU.  The architecture is stored in the ComputeMode class.</p> <p>Edit src/config/config.py.  Add IPU to the ComputeMode class.</p> <pre><code>class ComputeMode(Enum):\nCPU   = 0\n#...\nIPU   = 5\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#trainerpy","title":"Trainer.py","text":"<p>Edit src/utils/torch/trainer.py.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#import-poptorch","title":"Import PopTorch","text":"<p>PopTorch is Graphcore's extension of PyTorch.</p> <p>Import poptorch at the top of the file.</p> <pre><code>import poptorch\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#wrap-model","title":"Wrap Model","text":"<p>Wrap the model using poptorch.trainingModel() so that it may be ran on IPUs for training.</p> <p>Wrap the model using poptorch.inferenceModel() when not training.</p> <p>Find the following code around line 90 in the init_network method.</p> <pre><code>        # Foregoing any fusions as to not disturb the existing ingestion pipeline\nif self.is_training() and self.args.mode.quantization_aware:\nself._raw_net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\nself._net = torch.quantization.prepare_qat(self._raw_net)\nelse:\nself._net = self._raw_net\n</code></pre> <p>After the above code, add:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\nif self.is_training():\nopts = poptorch.Options()\nself._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\nelse:\nself._net = poptorch.inferenceModel(self._net)\n</code></pre> <p>See poptorch.trainingModel() and poptorch.inferenceModel() for more information.</p> <p>There is also a Build the Model tutorial.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-optimizer","title":"Update Optimizer","text":"<p>Update init_optimizer() to use the poptorch class instead of the torch class as needed.</p> <p>Change:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\n            self._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n        else:\n            self._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre> <p>to:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\nif self.args.run.compute_mode == ComputeMode.IPU:\nself._opt = poptorch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\nelse:\nself._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\nelse:\nif self.args.run.compute_mode == ComputeMode.IPU:\nself._opt = poptorch.optim.Adam(self._net.parameters(), 1.0)\nelse:\nself._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-the-forward-pass","title":"Update the Forward Pass","text":"<p>Putting the loss calculation in forward_pass() allows the loss computation to be performed on the IPUs. This will be faster because the data will not need to be transfered round-trip to the CPU.</p> <p>Change forward_pass():</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#original","title":"Original","text":"<pre><code>            if net is None:\nlogits_image = self._net(minibatch_data['image'])\nelse:\nlogits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#updated","title":"Updated","text":"<p>The following code changes are to account for the loss function, i.e., self.loss_calculator, and the image labels, i.e., labels_image, to be passed to the model's forward_pass method.  Additionally, the calculated loss is returned from the forward_pass method.</p> <pre><code>            if net is None:\nif self.args.run.compute_mode == ComputeMode.IPU:\nlogits_image, labels_image, loss = self._net(minibatch_data['image'], self.loss_calculator, labels_image)\nreturn logits_image, labels_image, loss\nelse:\nlogits_image = self._net(minibatch_data['image'])\nelse:\nif self.args.run.compute_mode == ComputeMode.IPU and self.args.mode.name != ModeKind.inference:\nlogits_image, labels_image, loss = net(minibatch_data['image'], self.loss_calculator, labels_image)\nreturn logits_image, labels_image, loss\nelse:\nlogits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-the-training-step","title":"Update the Training Step","text":"<p>Receive the extra loss variable from the forward_pass method.</p> <p>Update the train_step method.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#original-training-step","title":"Original Training Step","text":"<pre><code>                    with self.timing_context(\"forward\"):\nif self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\nwith torch.cuda.amp.autocast():\nlogits_image, labels_image = self.forward_pass(minibatch_data)\nelse:\nlogits_image, labels_image = self.forward_pass(minibatch_data)\nverbose = False\n# Compute the loss based on the logits\nwith self.timing_context(\"loss\"):\nloss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#updated-training-step","title":"Updated Training Step","text":"<p>The forward_pass() method was changed to return the extra variable loss in the previous section.  It is now received conditionally when using an IPU(s).</p> <p>In the with self.timing_context(\"loss\"): section, only calculate loss if not using an IPU(s).</p> <pre><code>                    with self.timing_context(\"forward\"):\nif self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\nwith torch.cuda.amp.autocast():\nlogits_image, labels_image = self.forward_pass(minibatch_data)\nelse:\nif self.args.run.compute_mode == ComputeMode.IPU:\nlogits_image, labels_image, loss = self.forward_pass(minibatch_data)\nelse:\nlogits_image, labels_image = self.forward_pass(minibatch_data)\nverbose = False\n# Compute the loss based on the logits\nwith self.timing_context(\"loss\"):\nif self.args.run.compute_mode == ComputeMode.IPU:\nloss = loss\nelse:\nloss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-validation-step","title":"Update Validation Step","text":"<p>Update the val_step method.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#original-validation-step-code","title":"Original Validation Step Code","text":"<p>Find this code.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\nwith torch.cuda.amp.autocast():\nlogits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\nelse:\nlogits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n# Compute the loss based on the logits\nloss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#updated-validation-step-code","title":"Updated Validation Step Code","text":"<p>Change the code to the following.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\nwith torch.cuda.amp.autocast():\nlogits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n# Compute the loss based on the logits\nloss = self.loss_calculator(labels_image, logits_image)\nelse:\nif self.args.run.compute_mode == ComputeMode.IPU:\nlogits_image, labels_image, loss = self.forward_pass(minibatch_data, net=val_net)\nelse:\nlogits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n# Compute the loss based on the logits\nloss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#uresnet2d-model","title":"UResNet2D Model","text":""},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-model","title":"Update Model","text":"<p>The Graphcore system is more computationally efficient if the loss function is on the IPU.  This is accomplished by using the loss function within the model's forward method.</p> <p>Edit src/networks/torch/uresnet2D.py.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-the-forward-declaration","title":"Update the Forward Declaration","text":"<p>Find the forward method.</p> <pre><code>def forward(self, input_tensor):\n</code></pre> <p>Update the argument list to include the loss function, i.e., loss_calculator and the image labels, i.e., labels_image.</p> <pre><code>def forward(self, input_tensor, loss_calculator=None, labels_image=None):\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#add-loss-calculation","title":"Add Loss Calculation","text":"<p>Add the loss calculation just before the forward method returns.</p> <pre><code>        if loss_calculator is not None:\nlabels_image = labels_image.long()\nlabels_image = torch.chunk(labels_image, chunks=3, dim=1)\nshape =  labels_image[0].shape\nlabels_image = [ _label.view([shape[0], shape[-2], shape[-1]]) for _label in labels_image ]\nloss = loss_calculator(labels_image, x)\nimport poptorch\nloss = poptorch.identity_loss(loss , reduction=\"mean\")\nreturn x, labels_image, loss\n# This return already exists.\nreturn x\n</code></pre> <p>The poptorch.identity_loss method takes a single PyTorch tensor and will backpropagate a gradient of ones through it.  You may find an example at here</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#binexecpy","title":"bin/exec.py","text":"<p>The following is included for completeness.  One will not likely find this in other code.</p> <p>Open bin/exec.py in your favorite editor.  Change:</p> <pre><code>@hydra.main(version_base=None, config_path=\"../src/config\", config_name=\"config\")\n</code></pre> <p>to</p> <pre><code>@hydra.main(config_path=\"../src/config\", config_name=\"config\")\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/","title":"CosmicTagger Conversion","text":"<p>The intent of this page is to show conceptually how to convert a Graphcore model to run on Distributed Data Parallel using PopDist. It is not necessary to convert CosmicTagger because it has already been converted and is located at CosmicTagger on the GraphcoreDDP branch. The original is located at CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#run-model-on-cpu","title":"Run Model on CPU","text":"<p>The first step to converting a model is to verify that it runs on the CPU.  This step has been verified for CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#starter-code","title":"Starter Code","text":"<p>You may use the code at CosmicTagger on the Graphcore branch.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#trainerpy","title":"Trainer.py","text":"<p>Edit src/utils/torch/trainer.py.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#import-poplar-packages","title":"Import Poplar Packages","text":"<p>PopTorch is Graphcore's extension of PyTorch.</p> <p>PopDist is Graphcore's distributed processing package.</p> <p>Import poptorch and popdist at the top of the file.</p> <pre><code>try:\nimport poptorch\nimport popdist\nimport popdist.poptorch\nexcept:\npass\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#initialization","title":"Initialization","text":"<p>Initialize popdist for distributed computing.</p> <p>Establish a class variable name instance.  This is used to differentiate between different model instances that will be saved.</p> <p>Add the following line at the bottom of init().</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU and popdist.isPopdistEnvSet():\npopdist.init()\nself._instance = popdist.getInstanceIndex()\nelse:\nself._instance = 0\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#use-instance-variable","title":"Use Instance Variable","text":"<p>Use the instance variable for the model file name.</p> <p>Find def get_model_filepath.</p> <p>Change:</p> <pre><code>        name = file_path + 'model-{}.ckpt'.format(self._global_step)\n</code></pre> <p>To:</p> <pre><code>        name = file_path + f'model-{self._global_step}-{self._instance}.ckpt'\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#establish-logging-method","title":"Establish Logging Method","text":"<p>Add a helper function to log data at the bottom of the file.</p> <pre><code>    def log_in_single_instance(self, string):\nif self.args.run.compute_mode == ComputeMode.IPU:\nif not popdist.isPopdistEnvSet() or popdist.getInstanceIndex() == 0:\nlogging.info(string)\nelse:\nlogging.info(string)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#update-init_network","title":"Update Init_network()","text":"<p>PopTorch has an Option() method which returns values that get passed to poptorch.trainingModel. The returned values are stored in opts in this example.</p> <p>Find:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\nif self.is_training():\nopts = poptorch.Options()\nself._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\nelse:\nself._net = poptorch.inferenceModel(self._net)\n</code></pre> <p>Replace it with:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\nif popdist.isPopdistEnvSet():\nopts = popdist.poptorch.Options()\n# When using the dataloader with 'auto_distributed_partitioning=True'\n# and 'shuffle=True' we must set the random seed to ensure that tensors\n# are in the same order in all processes.\nopts.randomSeed(42)\n# Replication factor is already set via PopRun so\n# we ignore 'args.num_replicas'.\nlogging.info(f\"Num of local replicas: {popdist.getNumLocalReplicas()}\")\nelse:\nopts = poptorch.Options()\nopts.replicationFactor(self.args.num_replicas)\nif self.is_training():\nself._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\nelse:\nself._net = poptorch.inferenceModel(self._net)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#run-the-code","title":"Run The Code","text":"<p>See instructions in README_GRAPHCORE.md.</p>"},{"location":"ai-testbed/graphcore/unused/multi-node-setup/","title":"Multi-node Setup","text":"<p>These steps only need to be executed once per user.</p> <p>Running on multiple nodes is a three step process.</p> <ol> <li> <p>Create a Key</p> <pre><code>cd ~/.ssh\nssh-keygen -t rsa -b 4096\n</code></pre> </li> <li> <p>Put Key into Authorized_keys File</p> <pre><code>cat id_rsa.pub &gt;&gt; authorized_keys\n</code></pre> </li> <li> <p>Add Node IP Addresses to Known_hosts File</p> <pre><code>ssh-keyscan -H 10.1.3.101 &gt;&gt; ~/.ssh/known_hosts\nssh-keyscan -H 10.1.3.102 &gt;&gt; ~/.ssh/known_hosts\nssh-keyscan -H 10.1.3.103 &gt;&gt; ~/.ssh/known_hosts\nssh-keyscan -H 10.1.3.104 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> </li> </ol>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/","title":"Profiling MNIST","text":"<p>Follow all the instructions in Getting Started to log into a Graphcore node.</p> <p>Follow the instructions in Virtual Environments up to and including PopART Environment Setup.</p> <p>Following the instructions in Example Programs up to and including MNIST, Install Requirements.</p>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/#change-directory","title":"Change Directory","text":"<pre><code>cd ~/graphcore/tutorials/simple_applications/pytorch/mnist\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/#set-poplar-options","title":"Set Poplar Options","text":"<p>Set the option to generate all reports, i.e., \"autoReport.all\":\"true\".</p> <p>Set the reports directory, i.e., \"autoReport.directory\":\"./reports\".</p> <p>Do so by running the following commands:</p> <pre><code>export POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\", \"autoReport.directory\":\"./reports\"}'\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/#run-mnist","title":"Run MNIST","text":"<p>Do so by running the following command:</p> <pre><code>python mnist_poptorch.py\n</code></pre> <p>When MNIST has finished running, see Profiling to use Graph Analyser.</p>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/","title":"Profiling ResNet50","text":"<p>Follow all the instructions in Getting Started to log into a Graphcore node.</p> <p>Follow the instructions in Virtual Environments up to and including PopART Environment Setup.</p>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#examples-repo","title":"Examples Repo","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git.</p> <p>Clone the examples repository to your personal directory structure:</p> <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#install-requirements","title":"Install Requirements","text":"<p>Change directory</p> <pre><code>cd ~/graphcore/examples/vision/cnns/pytorch\npython -m pip install -r requirements.txt\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#export-variables","title":"Export Variables","text":"<p>Export the datasets directory.</p> <pre><code>export POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\", \"autoReport.directory\":\"./reports\"}'\nexport DATASETS_DIR=/software/datasets\nHOST1=`ifconfig eno1 | grep \"inet \" | grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' | head -1`\nOCT123=`echo \"$HOST1\" | cut -d \".\" -f 1,2,3`\nOCT4=`echo \"$HOST1\" | cut -d \".\" -f 4`\nHOST2=$OCT123.`expr $OCT4 + 1`\nHOST3=$OCT123.`expr $OCT4 + 2`\nHOST4=$OCT123.`expr $OCT4 + 3`\nexport HOSTS=$HOST1,$HOST2,$HOST3,$HOST4\nexport CLUSTER=c16\nVIPU_SERVER=${VIPU_SERVER:=$HOST1}\nFIRST_PARTITION=`vipu-admin list partitions --api-host $VIPU_SERVER| grep ACTIVE | cut -d '|' -f 3 | cut -d ' ' -f 2 | head -1`\nPARTITON=${PARTITION:=$FIRST_PARTITION}\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#profile-resnet50","title":"Profile ResNet50","text":"<p>Profile ResNet50.</p> <p>Note: Use screen because every run is long.</p> <pre><code>cd train\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod16\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#profile-results","title":"Profile Results","text":"<p>When ResNet50 has finished running, see Profiling to use Graph Analyser.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/","title":"Profiling","text":"<p>This is an adaptation of Capturing IPU Reports.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#reports","title":"Reports","text":""},{"location":"ai-testbed/graphcore/unused/profiling/#capturing-ipu-reports","title":"Capturing IPU Reports","text":"<p>See Capturing IPU Reports for more information.</p> <p>This section describes how to generate the files that the Graph Analyser can analyze. The Graph Analyser uses report files generated during compilation and execution by the Poplar SDK.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#ipu-memory-overhead","title":"IPU Memory Overhead","text":"<p>Because of all these extra memory requirements, a model with high memory consumption may go out of memory when profiling is enabled. Depending on the model, you can adjust its parameters to leave space for the instrumentation. For example, you can try decreasing the batch size. In TensorFlow BERT you can adjust the micro batch-size.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#host-computing-overhead","title":"Host Computing Overhead","text":"<p>It is essential that you also try to reduce the iterations on each run. For instance, by reducing the number of steps or the number of batches per step you can get a lighter execution profile. This will not only reduce the host computation overhead but will also speed up visualization in the Graph Analyser.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#download-popvision","title":"Download PopVision","text":"<ol> <li> <p>Download PopVision Tools.</p> </li> <li> <p>Click Download Now button.</p> </li> <li> <p>In the Graph Analyser section, select you operating system.</p> </li> <li> <p>Install per selected operating system.</p> </li> </ol>"},{"location":"ai-testbed/graphcore/unused/profiling/#create-ssh-session","title":"Create SSH Session","text":"<p>Use ssh from your development system.</p> <p>The ssh command will use a jumphost and port forwarding.  The format is as follows:</p> <pre><code>ssh -J ALCFUserID@gc-login-dd.ai.alcf.anl.gov ALCFUserID@gc-poplar-DD -L 8090:127.0.0.1:22\nssh -J wilsonb@gc-login-01.ai.alcf.anl.gov wilsonb@gc-poplar-02.ai.alcf.anl.gov -L 8090:127.0.0.1:22\n</code></pre> <p>Where:</p> Argument Help ALCFUserID Is your ALCF user identification. dd Is the Graphcore login node to use, i.e., 01 or 02 DD Is the Graphcore node to use, i.e., 01, 02, 03, or 04. 8090 Is the port on your local machine. 127.0.0.1:22 Is the local IP address and port on the remote machine. <p>You will receive a prompt.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#launch-graph-analyser","title":"Launch Graph Analyser","text":"<p>Continue on your development machine.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#operating-system","title":"Operating System","text":""},{"location":"ai-testbed/graphcore/unused/profiling/#ubuntu","title":"Ubuntu","text":"<pre><code>cd /path/to/graph/analyser/directory\n./popvision-graph-analyser-3.11.6.AppImage\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling/#user-interface","title":"User Interface","text":"<ol> <li>Click Open a report...;</li> <li>Click the remote tab;</li> <li>Enter your ALCFUserID for remote machine;</li> <li>Enter the Hostname of your local machine, i.e., 127.0.0.1;</li> <li>Enter your Port address used in the ssh command, e.g., 8090;</li> <li>Click Connect;</li> <li>Navigate to your reports directory;</li> <li>Select the training directory;</li> <li>Select archive.a file; and</li> <li>Click Open button.</li> </ol> <p>The Summary Report will be displayed.</p>"},{"location":"ai-testbed/sambanova_gen1/TODO/","title":"TODO","text":"<ul> <li> Simple how to run data parallel on SN.</li> <li> On first glance, It would be good to include information of what model is being run here.</li> <li> Can users just take an existing compiled model and run? No, they need to recompile here for this, right? Users coming from the GPU land would falter here otherwise.</li> <li> How is data parallel implemented in a single sentence would be good.</li> <li> What does intelligently split data mean? Is there a special or particular way it is done or can be done that is different from what is done, say, on a GPU or other systems today?</li> <li> What happens if I set ws = 3</li> <li> Why are we setting OMP_NUM_THREADS=8? What happens if we set it lower. If this is important, we should highlight it</li> <li> Mention that MPI is used to do data-parallel training. Number of MPI ranks should be equal to RDUs requested</li> <li> What does reduce on rdu imply. Please provide more details here.</li> <li> How does someone who has worked with Horovod or DDP move to use data parallel on SN?</li> <li> <p> Does bs=1 mean local batch.</p> </li> <li> <p> docs/sambanova/Best-Practices-and-FAQs.md ## MPI -- TODO -- this needs to be redone - may be part of data parallel page.</p> </li> <li> Edit Anatomy... to be a technical doc</li> <li> Edit DataParallel.md to be a technical doc</li> <li>[...] SN provide indexing for their docs. From training</li> <li> We need to include in the documentation where the SN docs are located,</li> <li> what the contents are,</li> <li> and how to view them.</li> <li> As part of what the contents are, we need some readme.md with the PDFs listed</li> <li> <p> SN PDF docs copy them locally. and then view them.  VV has a tar file also.</p> </li> <li> <p> Move snconfig from overview to Miscellaneous</p> </li> <li> Ask SN for a diagram.</li> <li> Use SN diagram.</li> </ul> <pre><code>## [SambaNova](https://github.com/argonne-lcf/ai-testbed-userdocs/tree/main/docs/sambanova)\n**DONE**1. [System Overview](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/System-Overview.md)\n    - **DONE** \"half-rack system\" ? **Yes**\n    - **DONE**Configuration section says\n        &gt; SambaNova node can be accessed as sm-01.ai.alcf.anl.gov. **This is the actual SN node.**\n\n       but page on [How to setup your base environment](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/How-to-setup-your-base-environment.md)says\n        ```bash\n        ssh ALCFUserID@sambanova.alcf.anl.gov\n        ```\n        **This is the log in node.**\n\n2. [How to setup your base environment](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/System-Overview.md)\n    **DONE**1. Page title reads like an instruction, whereas most of the others read like section titles **docs/sambanova/Logging-into-a-SambaNova-Node.md**\n    **SKIPPED**2. Could combine this page with [Using virtual environments to customize environment](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Using-virtual-environments-to-customize-environment.md) into a single **Environment Setup** page??\n\n3. [Using virtual environments to customize environment](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Using-virtual-environments-to-customize-environment.md)\n    **DONE** **As script**1. Might be worth mentioning what packages are included in `--system-site-packages`\n    2. What commonly used packages are missing / might need to be installed manually?\n4. [Steps to run a model / program](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Steps-to-run-a-model-or-program.md)\n    - Might be worth mentioning that the [Example Programs](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Example-Programs.md) page has instructions for making a local copy of the examples used in the documentation\n    - \"The SambaNova workflow includes the following main steps to run a model\"\n        - Maybe list steps explicitly and then expand on them in their respective sections? e .g.\n        - The SambaNova workflow includes the following main steps to run a model:\n            1. Compile\n            2. Run\n            3. Test (optional)\n        - Seems a bit unnecessary to list them as steps if the Test step is optional?\n    - **DONE**Where is the `pef` directory coming from in the `srun python lenet.py run --pef=\"pef/lenet/lenet.pef `command?\n\n11. [Miscellaneous](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Miscellaneous.md)\n    **VV seems fine with the original**1. Might be a good idea to include the links under Resources somewhere on the main page?\n12. \n\n- [ ] [Anatomy of SambaNova PyTorch Models](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Anatomy-of-SambaNova-PyTorch-Models.md)\n    1. Updates section references https://git.cels.anl.gov/ai-testbed-apps/sambanova/sambanova_starter.git but that link requires special permission to view\n\n- [X] **There is already a link to the Example Programs.**Page Steps to Run a Model/Program : Add \u201c cd ~/apps/starters/\u201c before the example \n- [X] **I would have to set up a new repo from SC22 paper.** Page Performance Tools : Measure TFLOPs : Conv2D forward pass example file location can be provided\n- [X] Page Example Programs : section UNet : Remove question mark here --batch-size=1? python unet.py run --do-train  --in-channels=3  --in-width=32  --in-height=32 --init-features 32 --batch-size=1? --data-dir $DATADIR --log-dir ${OUTDIR}/log_dir_unet32_train --epochs 5 --pef=${OUTDIR}/unet_train/unet_train.pef\n\nPage Best Practices and FAQs : Section Data Parallel : Add unet_main.py file location\n- [X] Page Best Practices and FAQs : Section Data Parallel : Remove \u201c- \u2014\u201c near python unet_main.py compile - --batch-size=48\n- [X] Page Best Practices and FAQs : Section Data Parallel : Remove \u201c- \u2014\u201c  unet_main.py run - -p . \n- [Doc'd elsewhere] Also set export - [ SOFTWARE_HOME before compile and add to \u2014pef-name \n- [X] Argonne SambaNova-Training-June2021 and Human Decisions Files notes link is broken \n\nTypos : Page :Anatomy of SambaNova PyTorch Model  typo at\n- [X] \u201cThen once every 10,000 epics, print the status\u201d\n- [X] \u201cContains the train and test methods and a littile more.\u201d\n- [X] \u201cinstantiated instantiate\u201d\n- [X] \u201cthe the bigger systems \u201c\n- [X] \u201ct the the various \u201c\n\nPage Best Practices and FAQs: \n- [UTL] \u201ct and environment variable\u201d \n- [X] \u201cwhere the loss is calculating across - calculated\u201c \n\n- [X] Page Steps to Run a Model/Program: \u201cTest (Optional) This commands - command\u201c\n- [X] Page How to Setup Your Base Environment - \u201c request an acccount a\u201d \u201cgenerted - generated\u201d \n\n- [X] ModelZoo to /software/sambanova/apps/ and also use symbolic link for latest.\n\n- [X] How to do DataParallel.  add page.  See Confluence and maybe video and its .pptx\n\n- [X] How to spec 2 or more RDUs on SN?  Queue and Sub also.  \n- [model parallel] can do in compile phase v. run can be different.\n\n- [X] Move SN starter code to /software/sambanova/tutorials\n\n- [OK] just fyi, here's how to serve html from the bastion node (e.g. cerebras.alcf.anl.gov):\n\n```bash\nssh -t -L localhost:8089:localhost:8089 arnoldw@cerebras.alcf.anl.gov  \"cd /software/cerebras/docs/V1.1/;python3 -m http.server 8089\"\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/TODO/#system-overview","title":"System Overview","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#how-to-setup-your-base-environment","title":"How to setup your base environment","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft 4/18</li> <li> Second draft reviewed</li> <li> Third draft 4/20</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#using-virtual-environments-to-customize-environment","title":"Using Virtual Environments to Customize Environment","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft 4/18</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#steps-to-run-a-modelprogram","title":"Steps to Run a Model/Program","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft 4/19</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#job-queueing-and-submission","title":"Job Queueing and Submission","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft 4/18</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#example-programs","title":"Example Programs","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft 4/19</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#performance-tools","title":"Performance Tools","text":"<ul> <li> First draft 4/19</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#best-practices-and-faqs","title":"Best Practices and FAQs","text":"<ul> <li> First draft 4/19</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#tunneling-and-forwarding-ports","title":"Tunneling and Forwarding Ports","text":"<ul> <li> First draft 4/19</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#system-and-storage-policies","title":"System and Storage Policies","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#miscellaneous","title":"Miscellaneous","text":"<ul> <li> First draft 4/19</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/TODO/#anatomy-of-sambanova-pytorch-models","title":"Anatomy of SambaNova PyTorch Models","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"ai-testbed/sambanova_gen1/documentation/","title":"Documentation","text":"<p>The SambaNova documentation is housed at <code>/software/sambanova/docs/latest/</code> accessible via login node.</p> <pre><code>getting-started.pdf             # Getting Started with SambaFlow\naccuracy-debugging-tools.pdf    # Introduction to the model accuracy debugging tools.\ncompiler-options.pdf            # Provides advanced compiler options for the compile command.\nconversion-to-sambaflow.pdf     # Converting existing models to SambaFlow\nintermediate-tutorial.pdf       # SambaFlow intermediate model\nintro-tutorial-pytorch.pdf      # A peek into the code of the above example program.\nrelease-notes.pdf               # Provide new feature and bug fixes updates for each release version.\nrun-examples-language.pdf       # Run BERT on RDU\nrun-examples-pytorch.pdf        # Run Power PCA and micro models like GEMM on RDU\nrun-examples-vision.pdf         # Run UNET on RDU.\nusing-layernorm.pdf             # Example to use LayerNorm instead of BatchNorm\nusing-venvs.pdf                 # Python Virtual Environment.\n\nlatest\\\n    accuracy-debugging-tools.pdf\n    compiler-options.pdf\n    conversion-to-sambaflow.pdf\n    getting-started.pdf\n    intermediate-tutorial.pdf\n    intro-tutorial-pytorch.pdf\n    release-notes.pdf\n    run-examples-language.pdf\n    run-examples-pytorch.pdf\n    run-examples-vision.pdf\n    using-layernorm.pdf\n    using-venvs.pdf\n</code></pre> <p>The documentation can be viewed on your local system by copying the files from the login node.</p> <pre><code>cd &lt;your docs destination&gt;\nscp -r ALCFUserID@sambanova.alcf.anl.gov:/software/sambanova/docs/latest/* .\n</code></pre> <p>View the PDFs in your favorite viewer or web browser on your local machine.</p>"},{"location":"ai-testbed/sambanova_gen1/example-multi-node-programs/","title":"Example Multi-Node Programs","text":"<p>SambaNova provides examples of some well-known AI applications under the path: <code>/opt/sambaflow/apps/starters</code>, on both SambaNova compute nodes. Make a copy of this to your home directory:</p> <p>Copy starters to your personal directory structure if you have not already done so.</p> <pre><code>cd ~/\nmkdir apps\ncp -r /opt/sambaflow/apps/starters apps/starters\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/example-multi-node-programs/#unet","title":"UNet","text":""},{"location":"ai-testbed/sambanova_gen1/example-multi-node-programs/#set-up","title":"Set-up","text":"<p>Copy files and change directory if you have not already done so.</p> <pre><code>cp -r /opt/sambaflow/apps/image ~/apps/image\ncd ~/apps/image\ncp /software/sambanova/apps/image/pytorch/unet/*.sh .\n</code></pre> <p>You just copied two bash scripts.  They are:</p> <ol> <li> <p>unet_all.sh</p> <ul> <li>Compiles UNet and then submits a batch job to run the model.</li> </ul> </li> <li> <p>unet_batch.sh</p> <ul> <li>Runs Unet.</li> </ul> </li> </ol>"},{"location":"ai-testbed/sambanova_gen1/example-multi-node-programs/#unet-all","title":"Unet All","text":"<p>Here is a breakdown of unet_all.sh.</p> <p>The argument -x is used to specify that each executed line is to be displayed.</p> <p>The second line is to stop on error.</p> <p>Lastly, set total time, SECONDS, to zero.</p> <pre><code>#! /bin/bash -x\nset -e\n#\n# Usage: ./unet_all.sh 256 256\n#\nSECONDS=0\n</code></pre> <p>Set variables.</p> <pre><code># IMage size.\nIM=${1}\n# Batch Size\nBS=${2}\nNUM_WORKERS=1\nexport OMP_NUM_THREADS=16\n</code></pre> <p>Activate the virtual environment.  And, establish the UNet directory.</p> <pre><code>source /opt/sambaflow/venv/bin/activate\nUNET=$(pwd)/unet\n</code></pre> <p>Display model name and time.</p> <pre><code>echo \"Model: UNET\"\necho \"Date: \" $(date +%m/%d/%y)\necho \"Time: \" $(date +%H:%M)\necho \"COMPILE\"\n</code></pre> <p>This section will compile the model for multiple RDUs if it does not exist.</p> <p>A log file will be created at compile_${BS}_${IM}_NN.log.</p> <pre><code># Compile for parallel RDUs\nif [ ! -e out/unet_train_${BS}_${IM}_NN/unet_train_${BS}_${IM}_NN.pef ] ; then\npython ${UNET}/unet.py compile -b ${BS} --in-channels=3 --in-width=${IM} --in-height=${IM} --enable-conv-tiling --mac-v2 --compiler-configs-file ${UNET}/jsons/compiler_configs/unet_compiler_configs_no_inst.json --pef-name=\"unet_train_${BS}_${IM}_NN\"  --data-parallel -ws 2 &gt; compile_${BS}_${IM}_NN.log 2&gt;&amp;1\nfi\n</code></pre> <p>Here, a batch job is submitted for the multi-node run.</p> <p>Sbatch argument definitions:</p> <ul> <li> <p>--gres=rdu:1</p> <p>This indicates that the model fits on a single RDU.</p> </li> <li> <p>--tasks-per-node 8</p> <p>All eight RDUs per node are to be used.  Valid options are 1 through 8.</p> </li> <li> <p>--nodes 2</p> <p>The number of nodes to use.  Currently there are two nodes.</p> </li> <li> <p>--nodelist sm-02,sm-01</p> <p>The node names to use.</p> </li> <li> <p>--cpus-per-task=16</p> <p>CPUs per model.</p> </li> <li> <p>unet_batch.sh</p> <p>The bash script to be batched.</p> </li> </ul> <p>Unet_batch.sh argument definitions:</p> <ul> <li> <p>NN</p> <p>Number of nodes.</p> </li> </ul> <pre><code># Run Multi-Node, Data Parallel\nNN=2\necho \"RUN\"\necho \"NN=${NN}\"\nsbatch --gres=rdu:1 --tasks-per-node 8  --nodes 2 --nodelist sm-02,sm-01 --cpus-per-task=16 ./unet_batch.sh ${NN} ${NUM_WORKERS}\necho \"Duration: \" $SECONDS\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/example-multi-node-programs/#unet-batch","title":"Unet Batch","text":"<p>Here is a description of unet_batch.sh.  This script is automatically run by unet_all.sh.</p> <p>This block is the same as above.</p> <pre><code>#! /bin/bash -x\nset -e\n#\n# Usage: ./unet_batch.sh 2 1\n#\nSECONDS=0\n</code></pre> <p>Establish variables.</p> <pre><code># Batch Size\nBS=256\n# IMage size\nIM=256\nNN=${1}\nNUM_WORKERS=${2}\nexport OMP_NUM_THREADS=16\nDATADIR=/software/sambanova/dataset/kaggle_3m\nUNET=$(pwd)/unet\nexport SAMBA_CCL_USE_PCIE_TRANSPORT=0\n</code></pre> <p>Activate virtual environment.</p> <pre><code>source /opt/sambaflow/venv/bin/activate\n</code></pre> <p>Display an informative banner.</p> <pre><code>echo \"Model: UNET_TRAIN\"\necho \"Date: \" $(date +%m/%d/%y)\necho \"Time: \" $(date +%H:%M)\n</code></pre> <p>Run Unet</p> <pre><code>srun --mpi=pmi2 python ${UNET}/unet_hook.py  run --do-train --in-channels=3 --in-width=${IM} --in-height=${IM} --init-features 32 --batch-size=${BS} --epochs 2   --data-dir ${DATADIR} --log-dir log_dir_unet_${NN}_train_kaggle --pef=$(pwd)/out/unet_train_${BS}_${IM}_NN/unet_train_${BS}_${IM}_NN.pef --data-parallel --reduce-on-rdu --num-workers=${NUM_WORKERS}\n</code></pre> <p>Display total execution time.</p> <pre><code>echo \"Duration: \" $SECONDS\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/example-multi-node-programs/#compile-and-run","title":"Compile and Run","text":"<p>Change directory:</p> <pre><code>cd ~/apps/image/\n</code></pre> <p>Compile and run UNet:</p> <pre><code>./unet_all.sh 256 256\n</code></pre> <p>Squeue will give you the queue status.</p> <pre><code>squeue\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/example-programs/","title":"Example Programs","text":"<p>SambaNova provides examples of some well-known AI applications under the path: <code>/opt/sambaflow/apps/starters</code>, on both SambaNova compute nodes. Make a copy of this to your home directory:</p> <p>Copy starters to your personal directory structure:</p> <pre><code>cd ~/\nmkdir apps\ncp -r /opt/sambaflow/apps/starters apps/starters\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/example-programs/#lenet","title":"LeNet","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/lenet\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/example-programs/#common-arguments","title":"Common Arguments","text":"<p>Below are some of the common arguments used across most of the models in the example code.</p> Argument Default Help -b 1 Batch size for training -n, 100 Number of iterations to run --num-iterations the pef for -e, 1 Number epochs for training --num-epochs --log-path 'check Log path points' --num-workers 0 Number of workers --measure-train- None Measure training performance performance"},{"location":"ai-testbed/sambanova_gen1/example-programs/#lenet-arguments","title":"LeNet Arguments","text":"Argument Default Help --lr 0.01 Learning rate for training --momentum 0.0 Momentum value for training --weight-decay 0.01 Weight decay for training --data-path './data' Data path --data-folder 'mnist_ Folder containing mnist data data' <p>NOTE:  If you receive an \\\"HTTP error\\\" message on any of the following commands, run the command again. Such errors (e.g 503) are commonly an intermittent failure to download a dataset.</p> <p>Run these commands:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\nsrun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>To use Slurm sbatch, create submit-lenet-job.sh with the following contents:</p> <pre><code>#!/bin/sh\npython lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\npython lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Then</p> <pre><code>sbatch --output=pef/lenet/output.log submit-lenet-job.sh\n</code></pre> <p>Squeue will give you the queue status.</p> <pre><code>squeue\n# One may also...\nwatch squeue\n</code></pre> <p>The output file will look something like this:</p> <pre><code>[Info][SAMBA][Default] # Placing log files in\npef/lenet/lenet.samba.log\n\n[Info][MAC][Default] # Placing log files in\npef/lenet/lenet.mac.log\n\n[Warning][SAMBA][Default] #\n\n--------------------------------------------------\n\nUsing patched version of torch.cat and torch.stack\n\n--------------------------------------------------\n\n[Warning][SAMBA][Default] # The dtype of \"targets\" to\nCrossEntropyLoss is torch.int64, however only int16 is currently\nsupported, implicit conversion will happen\n\n[Warning][MAC][GraphLoweringPass] # lenet__reshape skip\nset_loop_to_air\n\n[Warning][MAC][GraphLoweringPass] # lenet__reshape_bwd skip\nset_loop_to_air\n\n...\n\nEpoch [1/1], Step [59994/60000], Loss: 0.1712\n\nEpoch [1/1], Step [59995/60000], Loss: 0.1712\n\nEpoch [1/1], Step [59996/60000], Loss: 0.1712\n\nEpoch [1/1], Step [59997/60000], Loss: 0.1712\n\nEpoch [1/1], Step [59998/60000], Loss: 0.1712\n\nEpoch [1/1], Step [59999/60000], Loss: 0.1712\n\nEpoch [1/1], Step [60000/60000], Loss: 0.1712\n\nTest Accuracy: 98.06 Loss: 0.0628\n\n2021-6-10 10:52:28 : [INFO][SC][53607]: SambaConnector: PEF File:\npef/lenet/lenet.pef\n\nLog ID initialized to: [ALCFUserID][python][53607] at\n/var/log/sambaflow/runtime/sn.log\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/example-programs/#mnist-feed-forward-network","title":"MNIST - Feed Forward Network","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/ffn_mnist/\n</code></pre> <p>Commands to run MNIST example:</p> <pre><code>srun python ffn_mnist.py compile --pef-name=\"ffn_mnist\" --output-folder=\"pef\"\nsrun python ffn_mnist.py run --pef=\"pef/ffn_mnist/ffn_mnist.pef\" --data-path mnist_data\n</code></pre> <p>To run the same using Slurm sbatch, create and run the submit-ffn_mnist-job.sh with the following contents.</p> <pre><code>#!/bin/sh\npython ffn_mnist.py compile --pef-name=\"ffn_mnist\" --output-folder=\"pef\"\npython ffn_mnist.py run --pef=\"pef/ffn_mnist/ffn_mnist.pef\" --data-path mnist_data\n</code></pre> <pre><code>sbatch --output=pef/ffn_mnist/output.log submit-ffn_mnist-job.sh\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/example-programs/#logistic-regression","title":"Logistic Regression","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/logreg\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/example-programs/#logistic-regression-arguments","title":"Logistic Regression Arguments","text":"<p>This is not an exhaustive list of arguments.</p> <p>Arguments</p> Argument Default Help Step --lr 0.001 Learning rate for training Compile --momentum 0.0 Momentum value for training Compile --weight-decay 1e-4 Weight decay for training Compile --num-features 784 Number features for training Compile --num-classes 10 Number classes for training Compile --weight-norm na Enable weight normalization Compile <p>Run these commands:</p> <pre><code>srun python logreg.py compile --pef-name=\"logreg\" --output-folder=\"pef\"\nsrun python logreg.py test --pef=\"pef/logreg/logreg.pef\"\nsrun python logreg.py run --pef=\"pef/logreg/logreg.pef\"\n</code></pre> <p>To use Slurm, create submit-logreg-job.sh with the following contents:</p> <pre><code>#!/bin/sh\npython logreg.py compile --pef-name=\"logreg\" --output-folder=\"pef\"\npython logreg.py test --pef=\"pef/logreg/logreg.pef\"\npython logreg.py run --pef=\"pef/logreg/logreg.pef\"\n</code></pre> <p>Then</p> <pre><code>sbatch --output=pef/logreg/output.log submit-logreg-job.sh\n</code></pre> <p>The output, pef/logreg/output.log, will look something like this:</p> <pre><code>[Info][SAMBA][Default] # Placing log files in\npef/logreg/logreg.samba.log\n[Info][MAC][Default] # Placing log files in\npef/logreg/logreg.mac.log\n[Warning][SAMBA][Default] #\n--------------------------------------------------\nUsing patched version of torch.cat and torch.stack\n--------------------------------------------------\n\n[Warning][SAMBA][Default] # The dtype of \"targets\" to\nCrossEntropyLoss is torch.int64, however only int16 is currently\nsupported, implicit conversion will happen\n[Warning][MAC][MemoryOpTransformPass] # Backward graph is trimmed\naccording to requires_grad to save computation.\n[Warning][MAC][WeightShareNodeMergePass] # Backward graph is\ntrimmed according to requires_grad to save computation.\n[Warning][MAC][ReduceCatFaninPass] # Backward graph is trimmed\naccording to requires_grad to save computation.\n[info ] [PLASMA] Launching plasma compilation! See log file:\n/home/ALCFUserID/apps/starters/pytorch/pef/logreg//logreg.plasma_compile.log\n...\n\n[Warning][SAMBA][Default] # The dtype of \"targets\" to\nCrossEntropyLoss is torch.int64, however only int16 is currently\nsupported, implicit conversion will happen\nEpoch [1/1], Step [10000/60000], Loss: 0.4763\nEpoch [1/1], Step [20000/60000], Loss: 0.4185\nEpoch [1/1], Step [30000/60000], Loss: 0.3888\nEpoch [1/1], Step [40000/60000], Loss: 0.3721\nEpoch [1/1], Step [50000/60000], Loss: 0.3590\nEpoch [1/1], Step [60000/60000], Loss: 0.3524\nTest Accuracy: 90.07 Loss: 0.3361\n2021-6-11 8:38:49 : [INFO][SC][99185]: SambaConnector: PEF File:\npef/logreg/logreg.pef\nLog ID initialized to: [ALCFUserID][python][99185] at\n/var/log/sambaflow/runtime/sn.log\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/example-programs/#unet","title":"UNet","text":"<p>Change directory and copy files.</p> <pre><code>cp -r /opt/sambaflow/apps/image ~/apps/image\ncd ~/apps/image/unet\n</code></pre> <p>Using the contents of unet_compile_run_inf_rl.sh, create a file in the current directory with the same name.</p> <p>Export the path to the dataset which is required for the training.</p> <pre><code>export OUTDIR=~/apps/image/unet\nexport DATADIR=/software/sambanova/dataset/kaggle_3m\n</code></pre> <p>Run these commands for training (compile + train):</p> <pre><code>sbatch unet_compile_run_inf_rl.sh compile 32 1  # Takes over 15 minutes.\nsbatch unet_compile_run_inf_rl.sh test 32 1     # Very fast.\nsbatch unet_compile_run_inf_rl.sh run 32 1      #\n</code></pre> <p>The output files are named slurm-\\&lt;batch ID&gt;.out.</p> <p>Using SLURM:  To use Slurm, create submit-unet-job.sh with the following contents:</p> <pre><code>#!/bin/sh\nexport OUTDIR=~/apps/image/unet\nexport DATADIR=/software/sambanova/dataset/kaggle_3m\n./unet_compile_run_inf_rl.sh compile 32 1\n./unet_compile_run_inf_rl.sh test 32 1\n./unet_compile_run_inf_rl.sh run 32 1\n</code></pre> <p>Then</p> <pre><code>sbatch submit-unet-job.sh\n</code></pre> <p>Squeue will give you the queue status.</p> <pre><code>squeue\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/getting-started/","title":"Getting Started","text":""},{"location":"ai-testbed/sambanova_gen1/getting-started/#on-boarding","title":"On-Boarding","text":"<p>See Get Started to request an account and additional information.</p>"},{"location":"ai-testbed/sambanova_gen1/getting-started/#setup","title":"Setup","text":""},{"location":"ai-testbed/sambanova_gen1/getting-started/#system-view","title":"System View","text":"<p>Connection to a SambaNova node is a two-step process. The first step is to ssh to the login node. This step requires an MFA passcode for authentication - an eight-digit passcode generated by an app on your mobile device, e.g., mobilePASS+. The second step is to log in to a SambaNova node from the login node.</p> <p></p>"},{"location":"ai-testbed/sambanova_gen1/getting-started/#log-in-to-login-node","title":"Log in to Login Node","text":"<p>Login to the SambaNova login node from your local machine using the below command. This uses the MobilPass+ token generated every time you log in to the system. This is the same passcode used to authenticate into other ALCF systems, such as Theta and Cooley.</p> <p>In the examples below, replace ALCFUserID with your ALCF user id.</p> <pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\nPassword: &lt; MobilPass+ code &gt;\n</code></pre> <p>Note: Use the ssh \"-v\" option in order to debug any ssh problems.</p>"},{"location":"ai-testbed/sambanova_gen1/getting-started/#log-in-to-a-sambanova-node","title":"Log in to a SambaNova Node","text":"<p>Once you are on the login node, the SambaNova system can be accessed using the alias sm-01 or sm-02.</p> <pre><code>ssh sm-01\n# or\nssh sm-02\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/getting-started/#sdk-setup","title":"SDK setup","text":"<p>The SambaNova system has a bash shell script to set up the required software environment. This sets up the SambaFlow software stack, and the associated environmental variables and starts a pre-configured virtual environment.</p> <p>Use</p> <pre><code>ALCFUserID@sm-01:~$ source /software/sambanova/envs/sn_env.sh\n(venv) ALCFUserID@sm-01:~$\n</code></pre> <p>The contents of the sn_env.sh script is shown below for convenience.</p> <pre><code>alias snpath='export PATH=$PATH:/opt/sambaflow/bin' # This is the path to SambaFlow which is the software stack running on SambaNova systems. This stack includes the Runtime, the compilers, and the SambaFlow Python SDK which is used to create and run models.\nalias snthreads='export OMP_NUM_THREADS=16' # The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel regions. The value of this environment variable must be a list of positive integer values. The values of the list set the number of threads to use for parallel regions at the corresponding nested levels. For the SambaNova system, it is usually set to 1.\nalias snvenv='source /opt/sambaflow/venv/bin/activate' # This starts the pre-configured virtual environment that consists of sambaflow and other built-in libraries.\n</code></pre> <p>NOTE:  SambaNova operations will fail unless the SambaNova venv is set up.</p> <p>You may deactivate the environment if finished.</p> <pre><code>deactivate\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/job-queuing-and-submission/","title":"Job Queueing and Submission","text":""},{"location":"ai-testbed/sambanova_gen1/job-queuing-and-submission/#introduction","title":"Introduction","text":"<p>SambaNova uses Slurm for job submission and queueing. Below are some of the important commands for using Slurm. For more information refer to Slurm Documentation.</p> <p>NOTE: Run the python scripts using srun or sbatch, to ensure that concurrent jobs do not interfere with each other.</p> <p>NOTE: There is just one scheduler for both sm-01 and sm-02.</p>"},{"location":"ai-testbed/sambanova_gen1/job-queuing-and-submission/#srun","title":"Srun","text":"<p>The Slurm command <code>srun</code> can be used to run individual python scripts in parallel with other scripts on a cluster managed by Slurm. Examples of <code>srun</code> usage are shown below.</p> <p>Slurm will assign a nodelist/host to run a job if a host is not specified.</p> <p>Example:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\nsrun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>You may specify which node/host on which to run a job.</p> <p>Reasons to specify a node list:</p> <ul> <li>One wants to test a specific node to verify function of the HW and SW  (daily smoke tests do this)</li> <li>The nodes are at different software levels and one wants to use a node that has the needed software level for one's application.</li> </ul> <p>Example:</p> <pre><code>srun --nodelist=sm-02 python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/job-queuing-and-submission/#sbatch","title":"Sbatch","text":"<p>Alternatively, these jobs can be submitted to the Slurm workload manager through a batch script by using the <code>sbatch</code> command. To do this, create a bash script (submit-lenet-job.sh here as an example) with the commands that you want to execute.</p> <pre><code>#!/bin/sh\npython lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\npython lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Then pass the bash script as an input to the <code>sbatch</code> command as shown below.</p> <pre><code>sbatch --output=pef/lenet/output.log submit-lenet-job.sh\n</code></pre> <p>In case of the need to use multiple RDUs (2 in the example shown below), the <code>sbatch</code> command would be altered as:</p> <pre><code>sbatch --gres=rdu:2 &lt;your_script.sh&gt;\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/job-queuing-and-submission/#squeue","title":"Squeue","text":"<p>The <code>squeue</code> command provides information about jobs located in the Slurm scheduling queue.</p> <pre><code>squeue\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/job-queuing-and-submission/#sinfo","title":"Sinfo","text":"<p>Sinfo is used to view partition and node information for a system running Slurm.</p> <p>Here is a suggested command:</p> <pre><code>sinfo -O AllocNodes, GresUsed, Gres, NodeList\n</code></pre> <p>For more information, see sinfo.</p>"},{"location":"ai-testbed/sambanova_gen1/job-queuing-and-submission/#scancel","title":"Scancel","text":"<p>Scancel is used to signal or cancel jobs, job arrays, or job steps.</p> <pre><code>scancel job_id\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/miscellaneous/","title":"Miscellaneous","text":""},{"location":"ai-testbed/sambanova_gen1/miscellaneous/#sdk-version","title":"SDK Version","text":"<p>To find the SDK version, run the following commands</p> <pre><code>(venv) ALCFUserID@sm-01:~$ python\nPython 3.7.6 (default, Feb 18 2020, 21:28:31) \n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import sambaflow\n&gt;&gt;&gt; sambaflow.__version__\n'1.11.5'\n&gt;&gt;&gt; \n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/miscellaneous/#omp_num_threads","title":"OMP_NUM_THREADS","text":"<p>The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel regions.</p> <p>The value of this environment variable must be a list of positive integer values. The values of the list set the number of threads to use for parallel regions at the corresponding nested levels.</p> <p>For the SambaNova system it, is usually set to one.</p> <pre><code>export OMP_NUM_THREADS=1\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/miscellaneous/#where-is-the-model","title":"Where is the Model?","text":"<p>Two copies of the model are maintained.  One in CPU memory and one in RDU memory. They do not interfere with each other unless you explicitly sync the model/parameter in between using:</p> <pre><code>SambaTensor.rdu() # Moves the CPU model to the RDU\nSambaTensor.cpu() # Moves the RDU model to the CPU\n</code></pre> <p>In order to run the model on the CPU, you can simply use the PyTorch model as if there is no RDU. In order to run the model on RDU, you would need to use session.run().</p>"},{"location":"ai-testbed/sambanova_gen1/miscellaneous/#useful-commands","title":"Useful Commands","text":""},{"location":"ai-testbed/sambanova_gen1/miscellaneous/#sn-configuration","title":"SN Configuration","text":"<pre><code>snconfig\n</code></pre> <p>The snconfig utility shows the static configuration of the system. The configuration on sm-01 for the first RDU is as follows:</p> <pre><code>Platform Name: DataScale SN10-8\nNode Name: NODE\nNumber of XRDUS: 4\nXRDU Name: XRDU_0\nNumber of RDUS: 2\nRDU name: RDU_0\nNumber of TILES: 4\nTILE Name: TILE_0\nSerial Number : N/A\n...\nNumber of PCIES: 4\nPCIE Name: PCIE_0\nBandwidth : 32 GB/s\nSpeed : 16 GT/s\nWidth : 16\nSerial Number : N/A\n...\nNumber of DDRCHs: 6\nDDR CH Name: DDRCH_0\nNumber of DIMMS: 2\nDIMM Name: DIMM_C0\nSize : 64.0 GB\nDIMM Name: DIMM_C1\nSize : 0.0 GB\nSerial Number : N/A\nCurrent utilization can be seen with sntilestat. In this example, only\nfour tiles in one RDU are in use.\nTILE %idle %exec %pload %aload %chkpt %quiesce PID USER COMMAND\n/XRDU_0/RDU_0/TILE_0 80.4 7.0 10.4 2.2 0.0 0.0 49880 arnoldw python\nres_ffn_mnist.py run --pef=pef/res_ffn_mnist/res_ffn_mnist.pef\n--num-epochs 100\n/XRDU_0/RDU_0/TILE_1 80.5 6.9 11.3 1.3 0.0 0.0 49880 arnoldw python\nres_ffn_mnist.py run --pef=pef/res_ffn_mnist/res_ffn_mnist.pef\n--num-epochs 100\n/XRDU_0/RDU_0/TILE_2 82.1 4.7 11.4 1.8 0.0 0.0 49880 arnoldw python\nres_ffn_mnist.py run --pef=pef/res_ffn_mnist/res_ffn_mnist.pef\n--num-epochs 100\n/XRDU_0/RDU_0/TILE_3 80.1 6.3 11.7 1.9 0.0 0.0 49880 arnoldw python\nres_ffn_mnist.py run --pef=pef/res_ffn_mnist/res_ffn_mnist.pef\n--num-epochs 100\n/XRDU_0/RDU_1/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_0/RDU_1/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_0/RDU_1/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_0/RDU_1/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_0/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_0/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_0/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_0/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_1/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_1/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_1/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_1/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_0/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_0/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_0/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_0/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_1/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_1/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_1/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_1/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_0/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_0/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_0/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_0/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_1/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_1/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_1/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_1/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/miscellaneous/#sambanova-daemon-service","title":"SambaNova Daemon Service","text":"<p>The following command checks if the SambaNova daemon service is running.</p> <pre><code>systemctl status snd\n</code></pre> <p>The output should look something like this:</p> <pre><code>* snd.service - SN Devices Service\n   Loaded: loaded (/usr/lib/systemd/system/snd.service; enabled; vendor preset: enabled)\n   Active: active (running) since Fri 2022-02-18 11:45:15 CST; 1 months 25 days ago\n Main PID: 3550 (snd)\n    Tasks: 10 (limit: 19660)\n   CGroup: /system.slice/snd.service\n           `-3550 /opt/sambaflow/bin/snd\n\nWarning: Journal has been rotated since the unit was started. Log output is incomplete or unavailable.\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/miscellaneous/#tile-status","title":"Tile status","text":"<pre><code>sntilestat\nwatch sntilestat\n</code></pre> <p>The output shown below is when the system is completely idle.</p> <pre><code>TILE                 %idle %exec %pload %aload %chkpt %quiesce    PID     USER COMMAND\n/XRDU_0/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/miscellaneous/#finding-hung-tiles","title":"Finding Hung Tiles","text":"<pre><code>snconfig show Node dynamic | grep perfect\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/miscellaneous/#how-busy-is-the-system","title":"How busy is the system?","text":"<p>Use one of</p> <pre><code>top\nhtop\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/running-a-model-or-program/","title":"Steps to Run a Model/Program","text":"<p>NOTE:  Please be mindful of how you are using the system. For example, consider running larger jobs in the evening or on weekends.</p> <p>NOTE: Please use only Slurm commands, i.e., srun and sbatch, to run your code. If you run your code directly using the python command, it may cause conflicts on the system.</p>"},{"location":"ai-testbed/sambanova_gen1/running-a-model-or-program/#introduction","title":"Introduction","text":"<p>The SambaNova workflow includes the following main steps to run a model.</p> <ol> <li>Compile</li> <li>Run</li> <li>Test (optional)</li> </ol> <p>The system uses the Slurm job scheduler to schedule the jobs and manage the workload on the system. For more information on Slurm, see Job Queueing and Submission.</p> <p>Example Programs lists the different example applications with corresponding commands for each of the above steps.</p>"},{"location":"ai-testbed/sambanova_gen1/running-a-model-or-program/#compile","title":"Compile","text":"<p>Compiles the model and generates a .pef file. This file contains information on how to reconfigure the hardware, how many compute and memory resources are required and how they will be used in all subsequent steps. The pef files are by default saved in the 'out' directory; the SambaNova documentation advises saving pef files in separate directories with the '--output-folder' option.</p> <p>It is necessary to re-compile only when the model changes, or parameters specific to the model graph change, including the batch size.</p> <p>Compile times can be significant. Compile of the Unet sample, for example, when using images of size 32x32 pixels, takes 358 (s), and 1844 (s) for images of size 256x256.</p> <p>Example:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre> <p>Where</p> Argument Default Help -b 1 Batch size for training"},{"location":"ai-testbed/sambanova_gen1/running-a-model-or-program/#run","title":"Run","text":"<p>This will run the application on SN nodes.</p> <pre><code>srun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>The location of the pef file generated in the compile step is passed as an argument to the run command.</p>"},{"location":"ai-testbed/sambanova_gen1/running-a-model-or-program/#test-optional","title":"Test (Optional)","text":"<p>This command is used to run the model on both the host CPU and the SambaNova node.  It compares the answers from the CPU and SambaNova RDU and will raise errors if any discrepancies are found. Pass the pef file generated as part of the compile step as the input to this command.</p> <pre><code>srun python lenet.py test --pef=\"pef/lenet/lenet.pef\"\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/system-overview/","title":"System Overview","text":""},{"location":"ai-testbed/sambanova_gen1/system-overview/#introduction","title":"Introduction","text":"<p>The SambaNova DataScale system is architected around the next-generation Reconfigurable Dataflow Unit (RDU) processor for optimal dataflow processing and acceleration. The AI Testbed's SambaNova system is a half-rack system consisting of two nodes, each of which features eight RDUs interconnected to enable model and data parallelism. SambaFlow, its software stack, extracts, optimizes, and maps dataflow graphs to the RDUs from standard machine learning frameworks, like PyTorch.</p> <p>Here is the link to the SambaNova white paper: Accelerated Computing with a Reconfigurable Dataflow Architecture</p>"},{"location":"ai-testbed/sambanova_gen1/tunneling-and-forwarding-ports/","title":"Tunneling and Forwarding Ports","text":"<p>Port forwarding is covered here.  This is specifically for TensorBoard.</p>"},{"location":"ai-testbed/sambanova_gen1/tunneling-and-forwarding-ports/#tensorboard-port-forwarding","title":"TensorBoard Port-Forwarding","text":"<p>This section describes the steps to be followed to set up port forwarding for applications, like TensorBoard, which runs on the SambaNova system and binds to one or more ports. This example uses 6006 and 16006 as port numbers. Using port numbers other than these may avoid collisions with other users.</p>"},{"location":"ai-testbed/sambanova_gen1/tunneling-and-forwarding-ports/#from-your-local-machine","title":"From your local machine","text":"<p>Run</p> <pre><code>ssh -v -N -f -L localhost:16006:localhost:16006 ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilPass+ code &gt;\n\nssh ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilPass+ code &gt;\n</code></pre> <p>replacing ALCFUserID with your ALCF User ID.</p>"},{"location":"ai-testbed/sambanova_gen1/tunneling-and-forwarding-ports/#from-sambanovaalcfanlgov","title":"From sambanova.alcf.anl.gov","text":"<p>Below are the commands specific to sm-01. You may replace sm-01 with sm-02 when using the appropriate system.</p> <p>Run</p> <p>NOTE:  The full name is sm-01.ai.alcf.anl.gov and it may also be used.</p> <pre><code>ssh -N -f -L localhost:16006:localhost:6006 ALCFUserID@sm-01\nssh ALCFUserID@sm-01\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/tunneling-and-forwarding-ports/#on-sm-01","title":"On sm-01","text":"<p>Execute the following command:</p> <pre><code>ALCFUserID@sm-01:~$ source /software/sambanova/envs/sn_env.sh\n(venv) ALCFUserID@sm-01:~$\n</code></pre> <p>Navigate to the appropriate directory for your model. Launch your model using srun or sbatch.</p> <pre><code>cd /path/to/your/project\nsbatch --output=pef/my_model/output.log submit-my_model-job.sh\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/tunneling-and-forwarding-ports/#on-another-sm-01-terminal-window","title":"On Another sm-01 Terminal Window","text":"<p>The SambaNova system has a bash shell script to setup the required software environment. This sets up the SambaFlow software stack, the associated environmental variables and activates a pre-configured virtual environment.</p> <p>Use</p> <pre><code>ALCFUserID@sm-01:~$ source /software/sambanova/envs/sn_env.sh\n(venv) ALCFUserID@sm-01:~$\n</code></pre> <p>Navigate to the appropriate directory for your model.</p> <pre><code>cd /path/to/your/project\ntensorboard --logdir /logs --port 6006\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/tunneling-and-forwarding-ports/#browser-on-local-machine","title":"Browser on Local Machine","text":"<p>Then, navigate in your browser to, in this example, http://localhost:16006 on your local machine.</p>"},{"location":"ai-testbed/sambanova_gen1/tunneling-and-forwarding-ports/#notes","title":"Notes","text":"<p>Explanation of ssh command:</p> <pre><code>-N : no remote commands\n\n-f : put ssh in the background\n\n-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt; :\n\nThe full command line will forward &lt;machine1&gt;:&lt;portA&gt; (local scope) to &lt;machine2&gt;:&lt;portB&gt; (remote scope)\n</code></pre> <p>Adapted from:  How can I run Tensorboard on a remote server?</p>"},{"location":"ai-testbed/sambanova_gen1/virtual-environment/","title":"Virtual Environments to Customize Environment","text":""},{"location":"ai-testbed/sambanova_gen1/virtual-environment/#using-a-virtual-venv","title":"Using a Virtual Venv","text":"<p>To create a virtual environment, one can use the --system-site-packages flag:</p> <pre><code>python -m venv --system-site-packages my_env\nsource my_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/virtual-environment/#system-site-packages","title":"System Site Packages","text":"<p>There are many packages available on the system. Run the following Python script to retrieve the location of the packages:</p> <pre><code>import sys\nsite_packages = next(p for p in sys.path if 'site-packages' in p)\nprint(site_packages)\n</code></pre> <p>Given the location of the packages, one may list the packages. For example:</p> <pre><code>ls -al /opt/sambaflow/venv/lib/python3.7/site-packages\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/virtual-environment/#installing-packages","title":"Installing Packages","text":"<p>Install packages in the normal manner such as:</p> <pre><code>python3 -m pip install \"SomeProject\"\n</code></pre> <p>For more details see Use pip for installing.</p> <p>To install a different version of a package that is already installed in one's environment, one can use:</p> <pre><code>pip install --ignore-installed  ... # or -I\n</code></pre> <p>Note: Conda is not supported on the SambaNova system.</p>"},{"location":"ai-testbed/sambanova_gen1/unused/performance-tools/","title":"Performance Tools","text":""},{"location":"ai-testbed/sambanova_gen1/unused/performance-tools/#tile-status","title":"Tile Status","text":"<pre><code>sntilestat\nwatch sntilestat\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/unused/performance-tools/#measure-tflops","title":"Measure TFLOPs","text":"<p>This is an example for measuring TFLOPs for Conv2D forward pass.</p> <pre><code>elif args.command == 'run':\nsamba.session.run(inputs, section_types=['fwd'])\n#samba.session.run(inputs, section_types=['bckwd'])\nn_iters = 100\nforward_pass_time = []\nprint(\"run starts\")\nstart_time_forward = time.time()\nfor loop in range(n_iters):\nsamba.session.run(inputs, section_types=['fwd'])\n#samba.session.run(inputs, section_types=['bckwd'])\n#samba.session.run(inputs, section_types=['fwd', 'bckwd'])\nend_time_forward = time.time()\nforward_pass_time.append(end_time_forward - start_time_forward)\nprint(\"run ends\")\nw_0 = (args.w + 2*args.pad_w - args.s)/args.wstride + 1\nh_0 = (args.h + 2*args.pad_h - args.r)/args.hstride + 1\ntflops = 2 * (w_0*h_0) * args.s * args.r * args.c * args.k * args.n\ntflops_forw = tflops/(sum(forward_pass_time)/n_iters/5)/(10**12) #tflops\nprint(tflops)\nprint(sum(forward_pass_time))\nprint(\"tflops: %f\"%tflops_forw)\nprint(\"SN,Training,%s,Conv2d_fwd,%d,100,1,%d,%d,%d,%d,%d,%d,%d,0.0,%f,None,%f,%f,%f\" % (\"dtype\", args.n, args.w, args.h, args.c, args.k, args.s, args.pad_w, args.wstride, (sum(forward_pass_time)/n_iters)/args.n, args.n/(sum(forward_pass_time)/n_iters), tflops_forw, (sum(forward_pass_time)/n_iters)/args.n))\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/unused/running-bert-large-on-sn10-8r/","title":"Steps to Run BERT-Large on Sambanova DataScale SN10-8R","text":""},{"location":"ai-testbed/sambanova_gen1/unused/running-bert-large-on-sn10-8r/#pretraining-in-data-parallel-mode","title":"Pretraining in data parallel mode","text":"<p>Note: for the sake of the tutorial, we have precompiled the model and lowered the number of train steps to reduce the execution time.</p> <ol> <li>Create a folder for pretraining in your home repo, and copy the bash script <code>/projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-pretrain-job-LBS1024.sh</code> to it. Then, go to that folder. Example:</li> </ol> <pre><code>cd $HOME\nmkdir pretrain\ncp /projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-pretrain-job-LBS1024.sh pretrain/\ncd pretrain/\n</code></pre> <ol> <li>Open the <code>submit-bert-pretrain-job-LBS1024.sh</code> file, and change <code>OUTDIR</code> to location of the pretrain folder. Example:</li> </ol> <pre><code>OUTDIR=$HOME/pretrain\n</code></pre> <p>Note: the per device batch size (LBS) is set to 1024 here. Also, the number of steps is set to 100, but this can be changed.</p> <ol> <li>SambaNova uses SLURM for job submission and queueing. We will use sbatch to submit our job to the job scheduler. Please refer to Sambanova Documentation for further details. In the following example, 2 RDUs are used:</li> </ol> <p><pre><code>sbatch --output=log_bert_pretrain_LBS1024_np2.out --gres=rdu:2 -c 8 submit-bert-pretrain-job-LBS1024.sh\n</code></pre>    Note: <code>-c</code> represents the number of cores per task</p> <ol> <li> <p>You can follow the status of your job using: <code>squeue</code>. The job should take about 8 min to complete.</p> </li> <li> <p>Once the job is completed, you can see the checkpoint(s) and accuracy metrics in <code>hf_output_lrg_run/</code>. The throughput is outputted in the <code>log_bert_pretrain_LBS1024_np2.out</code> file (search for throughput in the file).</p> <p> Click for sample throughput <pre><code>Measuring peformance with world size:  2\ninitial run starts.\ninitial run completes.\ne2e_latency: 30.75621747970581 seconds, throughput: 665.8816225861821 samples/s, measured over 10 iterations.\nNOTE: This is the combined throughput for 2 workers\ntotal duration: 30.75621747970581 s\n</code></pre> <p> Click for sample train_steps.txt <pre><code>10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n</code></pre> <p> Click for sample step_loss.txt <pre><code>11.16291\n10.76511\n10.44571\n10.16663\n9.98203\n9.85561\n9.76017\n9.66340\n9.57864\n9.50137\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/unused/running-bert-large-on-sn10-8r/#fine-tuning-for-question-answering-using-1-rdu","title":"Fine-tuning for question answering using 1 RDU","text":"<p>Note: for the sake of the tutorial, we have precompiled the model and lowered the number of train steps to reduce the execution time. We will also use a processed dataset.</p> <ol> <li>Create a folder for finetuning in your home repo, and copy the bash script <code>/projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-squad-job.sh</code> to it. Then, go to that folder. Example:</li> </ol> <pre><code>cd $HOME\nmkdir finetune\ncp /projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-squad-job.sh finetune/\ncd finetune/\n</code></pre> <ol> <li> <p>Copy the processed dataset to the finetune repo. This will avoid tokenizing the dataset on the fly.    <pre><code>cp -r /projects/aitestbed_training/SN/precompiled_bert/squad_cache ./\n</code></pre></p> </li> <li> <p>Open the <code>submit-bert-squad-job.sh</code> file, and change <code>OUTDIR</code> to location of the finetune folder. Example:</p> </li> </ol> <p><pre><code>OUTDIR=$HOME/finetune\n</code></pre>    Note: the number of train epochs is set to 0.08, but this can be changed</p> <ol> <li>SambaNova uses SLURM for job submission and queueing. We will use sbatch to submit our job to the job scheduler. Please refer to Sambanova Documentation for further details. In the following example, 1 RDU is used:</li> </ol> <pre><code>sbatch --output=log_bert_squad.out --gres=rdu:1 -c 8 submit-bert-squad-job.sh\n</code></pre> <ol> <li> <p>You can follow the status of your job using: <code>squeue</code>. The job should take about 8 min to complete.</p> </li> <li> <p>Once the job is completed, you can see the checkpoint(s) and accuracy metrics in <code>hf_output_squad_run/</code>.</p> <p> Click for sample log_history.json <pre><code>[\n{\n\"exact\": 54.33301797540208,\n     \"f1\": 66.54507382283774,\n     \"epoch\": 0.07965242577842144,\n     \"total_flos\": 5419063617454080,\n     \"step\": 220\n}\n]\n</code></pre> <p> Click for sample eval_results_squad.txt <pre><code>exact = 54.33301797540208\nf1 = 66.54507382283774\nepoch = 0.07965242577842144\ntotal_flos = 5419063617454080\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/unused/running-bert-large-on-sn10-8r/#other-models-and-use-cases","title":"Other Models and Use-cases","text":"<ul> <li>Full execution scripts (compile, run, measure-perf) for BERT-Large can be found under <code>/projects/aitestbed_training/SN/full_execution_bert/bash_scripts</code>.</li> <li><code>submit-bert-pretrain-job.sh</code>: bash script for pretraining job with 8 RDUs and LBS=256</li> <li> <p><code>submit-bert-squad-job.sh</code>: bash script for fine-tuning job for question answering with 1 RDU</p> </li> <li> <p>See Example Programs for instructions to run other well-known AI applications on SambaNova hardware (e.g., LeNet, FFN-MNIST, logistic regression, UNet)</p> </li> </ul>"},{"location":"ai-testbed/sambanova_gen1/unused/sambanova/","title":"SambaNova","text":""},{"location":"ai-testbed/sambanova_gen1/unused/sambanova/#pytorch-mirrors","title":"PyTorch Mirrors","text":"<p>See https://github.com/pytorch/examples .</p> <p>There are two mirrors (in the python docs) used for downloading the mnist dataset.</p> <p>mirrors = [         'http://yann.lecun.com/exdb/mnist/',         'https://ossci-datasets.s3.amazonaws.com/mnist/']</p> <p>yann.lecun.com appears to be intermittently broken (503 errors).</p>"},{"location":"ai-testbed/sambanova_gen1/unused/sambanova/#resources","title":"Resources","text":"<ul> <li> <p>https://docs.ai.alcf.anl.gov/sambanova/</p> </li> <li> <p>Argonne SambaNova Training   11/20</p> </li> <li> <p>https://docs.sambanova.ai Create a   SambaNova account if you do not have one.</p> </li> <li> <p>Getting Started with   SambaFlow   Skip this one.</p> </li> <li> <p>Tutorial: Creating Models with   SambaFlow</p> </li> <li> <p>Administrators -- @ryade</p> </li> </ul>"},{"location":"ai-testbed/sambanova_gen1/unused/sambanova/#further-information","title":"Further Information","text":""},{"location":"ai-testbed/sambanova_gen1/unused/sambanova/#creating-a-sambanova-portal-account-to-access-the-documentation-portal","title":"Creating a SambaNova Portal Account to access the documentation portal","text":"<ol> <li> <p>Go to  login.sambanova.ai;</p> </li> <li> <p>Select the \"Sign up\" link at the bottom;</p> </li> <li> <p>Enter your information</p> <ol> <li> <p>Your ANL email address;</p> </li> <li> <p>A password that you choose to access the site;</p> </li> <li> <p>First name;</p> </li> <li> <p>Last name;</p> </li> <li> <p>Alternate email address;</p> </li> <li> <p>Use 64693137 for the CLOUD ID;</p> </li> <li> <p>Select \"Register\" button;</p> </li> <li> <p>Note: The new web page may be displaying a QR code.  Do not navigate away from it.  Please edit this page to describe what happenes for you.</p> </li> </ol> </li> <li> <p>Verify your email address</p> <ol> <li> <p>Open your ANL email;</p> </li> <li> <p>Open the email from Okta;</p> </li> <li> <p>Select the \"Activate Account\" button;</p> </li> <li> <p>Select the \"Configure factor\" button on the displayed web page;</p> </li> <li> <p>Select either iPhone or Android for the device time on the new web page;</p> </li> <li> <p>Install Okta Verify from the App Store/Google Play Store onto your mobile device.;</p> </li> <li> <p>Select \"Next\" button on the web page;</p> </li> </ol> </li> <li> <p>On your phone</p> <ol> <li> <p>Open Okta Verify app;</p> </li> <li> <p>Select \"Get Started\" button;</p> </li> <li> <p>Select \"Next\" button;</p> </li> <li> <p>Select \"Add Account\" button;</p> </li> <li> <p>Select \"Organization\" for Account Type;</p> </li> <li> <p>Scan the QR Code shown in the browser;</p> </li> </ol> </li> <li> <p>Sign in to the SambaNova web site</p> <ol> <li>Select the \"SambaNova Documentation\" button.</li> </ol> </li> </ol> <p>Authorization for sections of the SambaNova site uses the tuple (email address, cloud id). For ANL users, these should be an anl email address and the cloud id specified above (64693137). (Note: the cloud id can be changed in the SambaNova user settings.) *If you are not at Argonne, please send us an email (ai@alcf.anl.gov) for access. **</p> <p>If you plan to publish, say to a conference, workshop or journal, we have a review process wherein you share the draft with us (pre-submission) at ai@alcf.anl.gov and we will work with SambaNova for the requisite approvals.</p>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/","title":"SambaTune","text":""},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#notes","title":"Notes","text":"<pre><code>cd /home/rweisner/tmp/uno_test\n</code></pre> <pre><code>#TODOBRW\nssh wilsonb@homes.cels.anl.gov\nssh sm-02\nMobilePass+ password\nOn sm-02\nsource /opt/sambaflow/venv/bin/activate\nexport PATH=/opt/sambaflow/bin:$PATH\nsambatune linear_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\nsambatune_ui --directory /home/wilsonb/tmp/sambatune_gen --port 8580\n#There will be a username and password displayed that you will use in your browser on your laptop.\nCommand used on laptop for port forward\nssh -XL 8580:127.0.0.1:8580 wilsonb@sm-02.cels.anl.gov\nMobilePass+ password\n# You will be logged into sm-02 but, you do not need to do anything.\naddress used in browser on laptop localhost:8580\n#Use username and password from sambatune_ui.\nUsername\nPassword\n\n#TODOBRW\n/home/wilsonb/DL/Sambanova/apps_1.12/private/anl/2022-09-21T19-21-05.html\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#about-sambatune","title":"About SambaTune","text":"<p>SambaTune is a tool for profiling, debugging, and tuning the performance of applications running on SN hardware.</p> <p>The tool automates the collection of hardware performance counters, metrics aggregation, report generation, and visualization. It also automates benchmarking of the application to compute average throughput over a sufficient number of runs. The tool is designed to aid the user with performance bottleneck analysis and tuning.</p> <p>SambaTune is currently used by SN engineers involved in performance tuning efforts. SambaTune is also planned for release to external customers to aid with performance bottleneck analysis and resolution.</p>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#run-sambatune","title":"Run SambaTune","text":"<pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\n# Enter MobilePass+ pass code\nssh sm-01\n</code></pre> <pre><code>#TODOBRW\nssh wilsonb@sambanova.alcf.anl.gov\n# Enter MobilePass+ pass code\nssh sm-01\n</code></pre> <p>First, enter the virtual environment on sm-01 or sm-02:</p> <pre><code>source /opt/sambaflow/venv/bin/activate\n</code></pre> <p>Update path:</p> <pre><code>export PATH=/opt/sambaflow/bin:$PATH\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#usage","title":"Usage","text":"<pre><code>usage: sambatune [-h] [--artifact-root ARTIFACT_ROOT] [--disable-override]\n                 [--compile-only | -m MODES [MODES ...]] [--version]\n                 config\npositional arguments:\n  config                YAML file with model, compile, run configuration.\noptional arguments:\n  -h, --help            show this help message and exit\n  --artifact-root ARTIFACT_ROOT\n                        Custom location to save compile/run artifacts;\n                        defaults to '$DUMP_ROOT/artifact_root' (default: None)\n  --disable-override    Reuse the placement from the baseline compilation\n                        (default: False)\n  --compile-only        Run compilation of PEFs for selected modes only\n                        (default: False)\n  -m MODES [MODES ...], --modes MODES [MODES ...]\n                        Select modes to execute from ['benchmark',\n                        'instrument', 'run'] (default: ['benchmark'])\n  --version             version of sambatune and sambaflow.\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#command-overview","title":"Command Overview","text":"<p>By default, it will run with the benchmarking mode enabled. Use the --modes flag to run modes individually or in any combination. Benchmark-Only:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark\n</code></pre> <p>Instrument-Only:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes instrument\n</code></pre> <p>All modes:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes instrument\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#command-example","title":"Command Example","text":"<pre><code># From Bill\npython /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_500_ws --output-folder=/home/arnoldw//models_dir/1520847 --mac-v1\n\npython /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --pef=/home/arnoldw//models_dir/1520847/uno_16_4_500_ws/uno_16_4_500_ws.pef --in_dir /var/tmp/raw/ --mac-v1\n</code></pre> <pre><code># From Bill --&gt; Bruce\npython /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_500_ws --output-folder='.' --mac-v1\n\nexport OMP_NUM_THREADS=1\npython /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --pef=./uno_16_4_500_ws/uno_16_4_500_ws.pef --in_dir /var/tmp/raw/ --mac-v1\n</code></pre> <pre><code>#TODOBRW  This works.  9/19/22\nsm-01/home/wilsonb/tmp/uno_test/uno_ccle.yaml\napp: /opt/sambaflow/apps/private/anl/uno_full.py\n\nmodel-args: --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial\n\ncompile-args: compile --plot --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nrun-args: --multiprocess-pickle --use-pickle-train  --measure-spatial --train-samba-spatial --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_500 --converted-pickle\n\nenv:\n     OMP_NUM_THREADS: 16,\n     SF_RNT_NUMA_BIND: 2\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune uno_ccle.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <pre><code>#TODOBRW\n# Stand-alone\nexport UNO=.\nexport NS=500\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_${NS}_ws --output-folder='.' --mac-v1\n\nexport OMP_NUM_THREADS=1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS}\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\nRicks run python ${UNO}/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\u201cout/uno_16_4_${NS}/uno_16_4_${NS}.pef\u201d --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n</code></pre> <pre><code>#TODOBRW\nsm-01/home/wilsonb/DL/Sambanova/apps_1.12/private/anl/uno_brw_CCLE_1_12.yaml\nexport OMP_NUM_THREADS=16\napp: /home/wilsonb/DL/Sambanova/apps_1.12/private/anl/uno_full.py\n\nmodel-args: --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial\n\ncompile-args: compile --plot --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nrun-args: --measure-spatial --train-samba-spatial --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_500\n\nenv:\n     OMP_NUM_THREADS: 16,\n     SF_RNT_NUMA_BIND: 2\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune uno_brw_CCLE_1_12.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n\nexport UNO=.\nexport NS=50\nexport OMP_NUM_THREADS=1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nxsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} --epochs 1 &gt; my.log 2&gt;&amp;1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --multiprocess-pickle  --measure-spatial --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./out/uno_full_16_47_${NS}/uno_full_16_47_${NS}.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\ncat my.log # Has pyinstrument run name.\npyinstrument --load-prev 2022-09-21T19-21-05 -r html\n\n1.13\n\nsource /opt/sambaflow/venv/bin/activate\ncd ~/tmp/uno_test/\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nexport PATH=/opt/sambaflow/bin:$PATH\nsntilestat\n\n./uno_pickl.sh compile 500\n./uno_pickl.sh run 500\n</code></pre> <pre><code>sambatune uno_brw_CCLE_1_12.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n\nexport UNO=.\nexport NS=50\nexport OMP_NUM_THREADS=1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nxsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} --epochs 1 &gt; my.log 2&gt;&amp;1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --multiprocess-pickle  --measure-spatial --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./out/uno_full_16_47_${NS}/uno_full_16_47_${NS}.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\ncat my.log # Has pyinstrument run name.\npyinstrument --load-prev 2022-09-21T19-21-05 -r html\n\n1.13\n\nsource /opt/sambaflow/venv/bin/activate\ncd ~/tmp/uno_test/\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nexport PATH=/opt/sambaflow/bin:$PATH\nsntilestat\n</code></pre> <p>uno_pickl.sh</p> <pre><code>#! /bin/bash -x\n#set -e\nsource /opt/sambaflow/venv/bin/activate\nSECONDS=0\nNS=${2}\nUNO=/opt/sambaflow/apps/private/anl/\nDS=\"ALL\"\nDS=\"CCLE\"\nBS=$((NS*16))\nexport OMP_NUM_THREADS=16\necho \"Model: UNO_SPA_TRN\"\necho \"Date: \" $(date +%m/%d/%y)\necho \"Time: \" $(date +%H:%M)\nif [ \"${1}\" == \"convert\" ] ; then\npython3 ${UNO}/uno/uno_data_loaders_converted.py   --in_dir /var/tmp/raw/ --out_dir /software/sambanova/dataset/${DS}_16_${NS}  --batch-size ${BS} --train_sources ${DS} --file-write-frequency 10\nelif [ \"${1}\" == \"compile\" ] ; then\necho \"COMPILE\"\npython ${UNO}/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --mac-human-decision ${UNO}/samba_uno/human_decisions_spatial.json --pef-name=\"uno_16_4_${NS}\" --mac-v1\n\nelif [ \"${1}\" == \"run\" ] ; then\necho \"RUN ${DS}\"\nSF_RNT_NUMA_BIND=2\n#python ${UNO}/uno_full.py run --acc-test --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\npython ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --epochs 1\n#python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\"\nelif [ \"${1}\" == \"pyinstrument\" ] ; then\necho \"RUN ${DS}\"\nSF_RNT_NUMA_BIND=2\n#python ${UNO}/uno_full.py run --acc-test --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\npyinstrument ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --epochs 1\n#python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\"\nelif [ \"${1}\" == \"no_pickle\" ] ; then\necho \"no_pickle ${DS}\"\nSF_RNT_NUMA_BIND=2\npython ${UNO}/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n\nelif [ \"${1}\" == \"mp\" ] ; then\necho \"Duration: \" $SECONDS\nelif [ \"${1}\" == \"mp\" ] ; then\necho \"Duration: \" $SECONDS\necho \"PERF\"\npython uno_full.py measure-performance --measure-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --num-iterations 20 --mac-v1\nfi\necho \"Duration: \" $SECONDS\n</code></pre> <pre><code>./uno_pickl.sh compile 500\n./uno_pickl.sh run 500\n./uno_pickl.sh pyinstrument 500\npyinstrument --load-prev 2022-09-22T18-31-24 -r html\nstdout is a terminal, so saved profile output to /tmp/tmpeo5ehksn.html\ncp /tmp/tmpeo5ehksn.html .\n</code></pre> <p>On dev terminal</p> <pre><code>scp wilsonb@sambanova.alcf.anl.gov:tmp/uno_test/tmpeo5ehksn.html .\n</code></pre> <p>View in local browser.</p>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#running","title":"Running","text":"<p>Create a directory for your work.</p> <pre><code>mkdir ~/sambatune\ncd ~/sambatune\n</code></pre> <p>Create small_vae.yaml with the following content using your favorite editor.</p> <pre><code>app: /opt/sambaflow/apps/private/anl/moleculevae.py\nmodel-args: -b 128 --in-width 512 --in-height 512\ncompile-args: compile --plot --enable-conv-tiling --compiler-configs-file /opt/sambaflow/apps/private/anl/moleculevae/compiler_configs_conv.json --mac-v2 --mac-human-decision /opt/sambaflow/apps/private/anl/moleculevae/symmetric_human_decisions_tiled_v2.json\nrun-args: --input-path /var/tmp/dataset/moleculevae/ras1_prot-pops.h5 --out-path ${HOME}/moleculevae_out --model-id 0 --epochs 10\nenv:\nOMP_NUM_THREADS: 16\nSF_RNT_FSM_POLL_BUSY_WAIT: 1\nSF_RNT_DMA_POLL_BUSY_WAIT: 1\nCONVFUNC_DEBUG_RUN: 0\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune small_vae.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>Create linear_net.yaml with the following content using your favorite editor.</p> <pre><code>app: /opt/sambaflow/apps/micros/linear_net.py\nmodel-args: &gt;\n-b 1024\n-mb 64\n--in-features 8192\n--out-features 4096\n--repeat 128\n--inference\ncompile-args: &gt;\n--n-chips 2\n--plot\nenv:\nSF_RNT_FSM_POLL_BUSY_WAIT: 1\nSF_RNT_DMA_POLL_BUSY_WAIT: 1\nCONVFUNC_DEBUG_RUN\": 0\n</code></pre> <p>NOTE: The following takes 45 minutes to run.</p> <p>Run the following example:</p> <pre><code>sambatune linear_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <pre><code>#TODOBRW\ncd ~/tmp/uno_test\nscreen\nsambatune uno.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>where linear_net.yaml is a user-specified configuration file you created above.</p>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#sambatune-ui","title":"SambaTune UI","text":""},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#port-availability","title":"Port Availability","text":"<p>It is recommended that you check if the port you want to use is available. You may check by:</p> <pre><code>ps -elf | grep desired_port\n</code></pre> <p>Example:</p> <pre><code>ps -elf | grep 8576\n</code></pre> <p>Alternatively, you may check for all ports in use by sambatune_ui:</p> <pre><code>ps -elf | grep sambatune_ui\n</code></pre> <p>If you need to free a port that you are finished with, you may use the kill command.</p>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#start-sambatune-ui","title":"Start SambaTune UI","text":"<p>If you followed the above directions, your artifact_root will be at ~/sambatune/artifact_root.</p> <p>Start the UI:</p> <p>It will tell you the username and password.</p> <p>NOTE: It is recommended to use a port other than 8576 in case someone else is using it.  Select another port close to 8576.</p> <p>Next</p> <pre><code>sambatune_ui --directory ~/sambatune/artifact_root/sambatune_gen/ --port 8576\n</code></pre> <pre><code>#TODOBRW\nsambatune_ui --directory ~/sambatune/artifact_root/sambatune_gen/ --port 8580\nsambatune_ui --directory /home/wilsonb/tmp/uno_test/artifact_root/sambatune_gen --port 8580\nusername: \"admin\", password: \"4f7cac2c-351e-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"aaf1fc88-35c8-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"bf64e4f8-3831-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"8feca89e-384c-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"355222d6-3a88-11ed-93a3-f7ef9c6e5d46\"\n</code></pre> <p>You will see something like:</p> <pre><code>with the,\n    username: \"admin\", password: \"05c63938-2941-11ed-93a3-f7ef9c6e5d46\"\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Starting gunicorn 20.1.0\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Listening at: http://0.0.0.0:8576 (1344959)\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Using worker: sync\n[2022-08-31 15:24:36 +0000] [1345092] [Info] Booting worker with pid: 1345092\n[2022-08-31 15:24:36 +0000] [1345093] [Info] Booting worker with pid: 1345093\n</code></pre> <p>NOTE: Write down the username and password.</p> <p>NOTE: The password only works with this one instance of sambatune_ui.  If you stop this instance of sambatune_ui and start another instance, it will have a new password.</p> <p>NOTE: You will need to &gt; or use the kill command to stop sambatune_ui when you have finished. Not doing so will tie up the port. You can ps -elf | grep the_port_you_used to find the running processes. If you are not comfortable doing this, please ask for help."},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#use-port-forwarding","title":"Use Port-Forwarding","text":"<p>This describes the steps to set up port-forwarding for applications, like SambaTune UI, which runs on the SambaNova system and binds to one or more ports. This example uses 8576 and 18576 as port numbers. Using port numbers other than these may avoid collisions with other users.</p>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#from-your-local-machine","title":"From your local machine","text":"<p>This command sets up a port forward SambaNova login node to your local machine.</p> <p>Run</p> <pre><code>ssh -N -f -L localhost:18576:localhost:18576 ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilPass+ code &gt;\n\nssh ALCFUserID@sambanova.alcf.anl.gov\n</code></pre> <pre><code>#TODOBRW\nssh -v -N -f -L localhost:8580:localhost:8580 wilsonb@sambanova.alcf.anl.gov\nssh -N -f -L localhost:8580:localhost:8580 wilsonb@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilPass+ code &gt;\n\nssh wilsonb@sambanova.alcf.anl.gov\n</code></pre> <p>replacing ALCFUserID with your ALCF User ID.</p>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#from-sambanovaalcfanlgov","title":"From sambanova.alcf.anl.gov","text":"<p>This command sets up a port forward from a SambaNova node to the sambanova login machine.</p> <p>Below are the commands specific to sm-01. You may replace sm-01 with sm-02 when using that system.</p> <p>Run</p> <p>NOTE:  The full name is sm-01.ai.alcf.anl.gov and it may also be used.</p> <pre><code>ssh -N -f -L localhost:18576:localhost:8576 ALCFUserID@sm-01\n</code></pre> <pre><code>#TODOBRW\nssh -N -f -L localhost:8580:localhost:8580 wilsonb@sm-01\n</code></pre>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#browser-on-local-machine","title":"Browser on Local Machine","text":"<p>Then, navigate in your browser to, in this example, http://localhost:18576 on your local machine.</p> <p>Use the username and password from sm-01 to log in.</p>"},{"location":"ai-testbed/sambanova_gen1/unused/sambatune-user-guide/#ssh-notes","title":"SSH Notes","text":"<p>Explanation of ssh command:</p> <pre><code>-N : no remote commands\n\n-f : put ssh in the background\n\n-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt; :\n\nThe full command line will forward &lt;machine1&gt;:&lt;portA&gt; (local scope) to &lt;machine2&gt;:&lt;portB&gt; (remote scope)\n</code></pre> <p>Adapted from:  How can I run Tensorboard on a remote server?</p>"},{"location":"ai-testbed/sambanova_gen2/TODO/","title":"TODO","text":"<ul> <li> docs/ai-testbed/sambanova_gen2/example-multi-node-programs.md</li> <li> docs/ai-testbed/sambanova_gen2/ GPT2 example</li> </ul> <p>Using /data/ANL/results/sn30-r1-h1/wilsonb/032223.18/GPT1.5B.out for output Using /data/ANL/results/sn30-r2-h1/wilsonb/032223.19/GPT1.5B.out for output</p> <p>Using /data/ANL/results/sn30-r2-h1/wilsonb/032223.19/BertLarge.out for output</p>"},{"location":"ai-testbed/sambanova_gen2/documentation/","title":"Documentation","text":"<p>The SambaNova documentation is now available online SambaNova Documentation.</p> <p>The documentation for the SambaTune (a profiling and performance tuning tool for SambaNova systems) is now available at SambaTune Documentation.</p>"},{"location":"ai-testbed/sambanova_gen2/example-multi-node-programs/","title":"Example Multi-Node Programs","text":"<p>In this section we will learn how to extend the UNet2d and Gpt1.5B applications scripts that we introduced in the Example Programs to compile and run multiple instances of the model in a data parallel fashion across multiple tiles or across multiple nodes.</p>"},{"location":"ai-testbed/sambanova_gen2/example-multi-node-programs/#unet2d","title":"UNet2d","text":""},{"location":"ai-testbed/sambanova_gen2/example-multi-node-programs/#set-up","title":"Set Up","text":"<p>Create the following directory and change to it if you have not already done so.</p> <pre><code>mkdir -p ~/apps/image/unet\ncd ~/apps/image/unet\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-multi-node-programs/#create-unet2dsh-and-unet_batchsh","title":"Create Unet2d.sh and unet_batch.sh","text":"<p>Create the file Unet2d.sh and unet_batch.sh in the current directory using your favorite editor. Copy and paste the contents of Unet2d.sh and unet_batch.sh to files with the same name into the current directory using your favorite editor.</p> <pre><code>chmod +x Unet2d.sh\nchmod +x unet_batch.sh\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-multi-node-programs/#compile-and-run","title":"Compile and run","text":"<p>Run these commands for training (compile + train): The compile and run scripts have the following input arguments.</p> <ol> <li> <p>image size:  The images are square.  Valid sizes include 256, 512, and 1024.</p> </li> <li> <p>Batch size: local batch size.  The global batch size is local batch size * Num of instances.</p> </li> <li> <p>num of instances: Total number of instances of Unet2d run in data parallel framework.</p> </li> <li> <p>RunID: A unique Id for the compile or run process.</p> </li> </ol> <p>The script uses the arguments <code>pcompile</code> and <code>prun</code> for the data parallel compile and run.</p> <pre><code>./Unet2d.sh pcompile &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n./Unet2d.sh prun &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n</code></pre> <p>For a image size of 256x256 and local batch size of 256 when running 8 instance, the commands are provided as follows.</p> <pre><code>./Unet2d.sh pcompile 256 256 8 unet2d_8inst_pcompile\n./Unet2d.sh prun 256 256 8 unet2d_8inst_prun\n</code></pre> <p>The above commands displays the file that contains the output for the execution of the above scripts, usually <code>/data/ANL/results/&lt;hostname&gt;/&lt;userId&gt;/&lt;RunID&gt;/Unet2d.out</code></p> <p>You can inspect the compile command that contains <code>--data-parallel -ws 2</code> arguments to ensure that the <code>pef</code> file is compatible for data parallel runs. The pef generated from the compilation process for the above compile command is placed under out/Unet2d/unet_train_256_256_NP_4 inside the current working directory.</p> <pre><code>python /opt/sambaflow/apps/image/segmentation/compile.py compile --mac-v2 --in-channels=3 --in-width=${2} --in-height=${2} --batch-size=${BS} --enable-conv-tiling --num-tiles=4 --pef-name=unet_train_${BS}_${2}_NP_${NUM_TILES}  --data-parallel -ws 2 --output-folder=${OUTDIR}\n</code></pre> <p>Once the model is compiled, sbatch is used to launch the multiple instances. The below example shows that a total of 8 tasks or instances are launched over the host on which the script is launched.</p> <pre><code>sbatch --gres=rdu:1 --tasks-per-node ${NP} --nodes 1 --nodelist $(hostname) --cpus-per-task=${cpus} $(pwd)/unet_batch.sh ${NP} ${NUM_WORKERS} ${BS} ${2} ${5}\n</code></pre> <p>The <code>run</code> command has <code>--data-parallel --reduce-on-rdu</code> arguments that is compatible with data parallel run.</p> <pre><code>srun --mpi=pmi2 python /opt/sambaflow/apps/image/segmentation//hook.py run --data-cache=${CACHE_DIR}  --data-in-memory --num-workers=${NUM_WORKERS} --enable-tiling  --min-throughput 395 --in-channels=3 --in-width=${IM} --in-height=${IM} --init-features 32 --batch-size=${BS} --epochs 10 --data-dir ${DS} --log-dir log_dir_unet_${IM}_${BS}_${NP} --data-parallel --reduce-on-rdu --pef=${OUTDIR}/unet_train_${BS}_${IM}_NP_4/unet_train_${BS}_${IM}_NP_4.pef\n</code></pre> <p>The throughput is calculated by averaging the <code>e2e samples_per_sec</code> over the different instances.</p> <pre><code>inner train loop time : 36.314290046691895 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 563.9653143065\ninner train loop time : 33.36756229400635 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 613.7697389922524\ninner train loop time : 33.94625234603882 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 603.3066563941279\ninner train loop time : 32.309499979019165 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 633.8692958200872\ninner train loop time : 31.418426036834717 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 651.8467849404489\ninner train loop time : 28.164129495620728 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 727.1660927132315\ninner train loop time : 30.29698896408081 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 675.9747651583616\ninner train loop time : 25.332663536071777 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 808.442427336472\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-multi-node-programs/#gpt-15b","title":"Gpt 1.5B","text":""},{"location":"ai-testbed/sambanova_gen2/example-multi-node-programs/#set-up_1","title":"Set up","text":"<pre><code>mkdir ~/nlp-multiNodetest\ncd ~/nlp-multiNodetest\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-multi-node-programs/#create-and-run-gpt15b_compilesh-and-gpt15b_runsh","title":"Create and run Gpt1.5B_compile.sh and Gpt1.5B_run.sh","text":"<p>Create the files Gpt1.5B_compile.sh and Gpt1.5B_run.sh in the current directory. Copy the contents of Gpt1.5B_compile.sh and Gpt1.5B_run.sh. Alternatively, the files can be accessed at <code>/data/ANL/scripts/Gpt1.5B_compile.sh</code> and <code>/data/ANL/scripts/Gpt1.5B_run.sh</code> on any of the compute node and can be copied over to the working directory.</p>"},{"location":"ai-testbed/sambanova_gen2/example-multi-node-programs/#compile-and-run_1","title":"Compile and Run","text":"<p>This script consists of commands to <code>compile</code> and <code>run</code> multiple instances of Gpt1.5B model across multiple nodes. Run the Gpt1.5B_compile.sh to first compile and generate the <code>pef</code> file for the model and it in turn launches the <code>Gpt1.5B_run.sh</code> script to run multiple instances of the model over the different nodes.</p> <pre><code>chmod +x Gpt1.5B_compile.sh\nchmod +x Gpt1.5B_run.sh\n./Gpt1.5B_compile.sh\n</code></pre> <p>You can see the log file path displayed on the screen as seen in the example below. You can use the <code>tail</code> command to check the progress of the run.</p> <pre><code>vsastry@sn30-r1-h1:~/nlp-multiNodetest$ ./Gpt1.5B_compile.sh\nUsing /data/ANL/results/sn30-r1-h1/vsastry/041823.19/GPT1.5B.out for output\n</code></pre> <p>The artifacts of the compile process is produced in the path : <code>/data/scratch/&lt;userId&gt;</code>.</p> <p>Inspect the <code>compile</code> command in the script to see that it includes additional arguments <code>--data-parallel</code> and <code>-ws 2</code> to generate a <code>pef</code> that is compatible for data parallel runs.</p> <pre><code>python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py compile --module_name gpt2_pretrain --task_name clm --max_seq_length 1024 -b 16 --output_dir=${OUTDIR}/hf_output --overwrite_output_dir --do_train  --per_device_train_batch_size 16 --cache ${OUTDIR}/cache/ --tokenizer_name gpt2 --model_name gpt2 --mac-v2 --non_split_head --mac-human-decision /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/mac_v2_overrides/gpt2_48_enc_full_recompute_training_spatialmapping_tiling16_clmerge_gm_nonpardp_lnsd.json --compiler-configs-file /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/compiler_configs/compiler_configs_gpt2_sc_recompute_spatialmapping_tiling16_clsmerge_withcls_nonpardp_norc_e2e.json --skip_broadcast_patch --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --no_index_select_patch --data-parallel -ws 2 --weight_decay 0.1  --max_grad_norm_clip 1.0 --num-tiles 4 --pef-name=gpt15 --output-folder=${OUTDIR}\n</code></pre> <p>Once the model is compiled, <code>sbatch</code> is used to launch the multiple instances across the nodes. The below example shows that a total of <code>32 tasks</code> or instances are launched over <code>2 nodes</code> with each node having a maximum of <code>16 tasks</code>. Slurm allocates any 2 of the available nodes in this example.</p> <pre><code>/usr/local/bin/sbatch --output=${HOME}/slurm-%A.out --ntasks 32 --gres=rdu:1 --ntasks-per-node 16  --nodes 2 --cpus-per-task=8  Gpt1.5B_run.sh ${1} &gt;&gt; ${OUTPUT_PATH} 2&gt;&amp;1\n</code></pre> <p>The <code>run</code> command for each of this instance is present in the <code>Gpt1.5B_run.sh</code> script. You can inspect the command in the script to see that <code>--data-parallel --reduce-on-rdu</code> arguments are present to ensure that the model is run in a data parallel fashion and that the gradient accumulation takes place on the RDU.</p> <pre><code>/usr/local/bin/srun --mpi=pmi2 python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py run  -b 16  --module_name gpt2_pretrain --task_name clm --max_seq_length 1024  --overwrite_output_dir --do_train  --per_device_train_batch_size 16 --cache ${OUTDIR}/cache/  --tokenizer_name gpt2 --model_name gpt2 --non_split_head --skip_broadcast_patch --no_index_select_patch --output_dir=${OUTDIR}/hf_output --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --max_grad_norm_clip 1.0 --skip_checkpoint --data-parallel --reduce-on-rdu --data_dir /data/ANL/ss1024 --data_dir /data/ANL/ss1024  --logging_steps 1 --max_steps 900000 --learning_rate 0.00025 --steps_this_run 800 --min_throughput 299000 --max_throughput 600000 --pef=${OUTDIR}/gpt15/gpt15.pef &gt;&gt; ${OUTPUT_PATH} 2&gt;&amp;1\n</code></pre> <p><code>squeue</code> shows that the model is run on 2 nodes <code>sn30-r1-h1</code> and <code>sn30-r2-h2</code>.</p> <pre><code>JOBID PARTITION                      NAME     USER ST       TIME  NODES NODELIST(REASON)\n10191 sambanova            Gpt1.5B_run.sh  vsastry  R      23:18      2 sn30-r1-h1,sn30-r2-h2\n</code></pre> <p><code>sntilestat</code> can also be used to check the total numbers of tiles used for the runs.</p> <pre><code>TILE                 %idle %exec %pload %aload %chkpt %quiesce    PID     USER COMMAND\n/XRDU_0/RDU_0/TILE_0   8.0  91.6    0.3    0.1    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_1   8.0  91.6    0.3    0.1    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_2   7.9  91.6    0.3    0.3    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_3   7.7  91.8    0.3    0.3    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_4   7.6  91.9    0.4    0.1    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_5   7.5  91.9    0.5    0.1    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_6   7.5  91.8    0.5    0.3    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_7   7.3  92.0    0.6    0.0    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_0   8.9  89.9    1.0    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_1   9.0  89.9    0.9    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_2   8.6  89.8    1.4    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_3   8.5  89.9    1.4    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_4   7.9  90.9    0.9    0.4    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_5   7.7  90.9    0.9    0.5    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_6   7.7  91.0    0.9    0.4    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_7   8.0  91.0    0.6    0.4    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_0   7.6  92.0    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_1   7.6  92.0    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_2   7.5  92.1    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_3   7.5  92.1    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_4   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_5   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_6   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_7   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_0   7.7  91.5    0.4    0.4    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_1   7.9  91.5    0.3    0.4    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_2   7.9  91.5    0.3    0.4    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_3   7.6  91.8    0.4    0.3    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_4   7.7  91.9    0.4    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_5   7.7  91.9    0.4    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_6   7.9  91.9    0.3    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_7   7.9  91.9    0.3    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_0   8.0  91.8    0.1    0.1    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_1   8.0  91.8    0.1    0.1    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_2   8.0  91.8    0.1    0.1    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_3   7.7  91.9    0.1    0.3    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_4   7.5  92.0    0.5    0.0    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_5   7.6  91.9    0.5    0.0    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_6   7.6  91.9    0.4    0.1    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_7   7.5  91.9    0.4    0.3    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_0   7.5  91.8    0.6    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_1   7.5  91.8    0.6    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_2   7.7  91.6    0.5    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_3   7.7  91.6    0.5    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_4   7.9  91.4    0.8    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_5   7.9  91.4    0.8    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_6   8.1  91.4    0.5    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_7   8.2  91.4    0.4    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_0   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_1   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_2   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_3   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_4   7.6  91.8    0.3    0.4    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_5   7.7  91.8    0.1    0.4    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_6   7.7  91.8    0.3    0.3    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_7   7.7  91.9    0.3    0.1    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_0   7.7  92.0    0.1    0.1    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_1   7.7  92.0    0.1    0.1    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_2   7.7  92.1    0.1    0.0    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_3   7.7  92.1    0.1    0.0    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_4   7.3  91.9    0.5    0.3    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_5   7.3  91.9    0.5    0.3    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_6   7.3  91.9    0.5    0.3    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_7   7.3  92.0    0.5    0.1    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n</code></pre> <p>The Slurm log associated with the JOBID (10191 in the above example) is located in the home directory. You can use the <code>tail</code> command to check the progress of the training.</p> <pre><code>vsastry@sn30-r1-h1:~$ tail -f ~/slurm-10191.out\nUsing /data/ANL/results/sn30-r1-h1/vsastry/041823.03/Gpt1.5B.out for output\n</code></pre> <pre><code>vsastry@sn30-r1-h1:~$ tail -f /data/ANL/results/sn30-r1-h1/vsastry/041823.03/Gpt1.5B.out\n</code></pre> <p>Once the run is completed, check the log file for the performance results.</p> <pre><code>{'e2e_train_time': 2179.2292835712433, 'training_sequences_per_second': 192467.31088004305, 'final_loss': 4.781678199768066}\n247/3247 [01:03&lt;00:00, 50.76it/s]\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-programs/","title":"Example Programs","text":"<p>SambaNova provides examples of some well-known simple AI applications under the path: <code>/opt/sambaflow/apps/starters</code>, on all SambaNova compute nodes. Make a copy of this to your home directory:</p> <pre><code>cd ~/\nmkdir apps\ncp -r /opt/sambaflow/apps/starters apps/starters\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-programs/#lenet","title":"LeNet","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/lenet\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-programs/#common-arguments","title":"Common Arguments","text":"<p>Below are some of the common arguments used across most of the models in the example code.</p> Argument Default Help -b 1 Batch size for training -n, 100 Number of iterations to run --num-iterations the pef for -e, 1 Number epochs for training --num-epochs --log-path 'check Log path points' --num-workers 0 Number of workers --measure-train- None Measure training performance performance"},{"location":"ai-testbed/sambanova_gen2/example-programs/#lenet-arguments","title":"LeNet Arguments","text":"Argument Default Help --lr 0.01 Learning rate for training --momentum 0.0 Momentum value for training --weight-decay 0.01 Weight decay for training --data-path './data' Data path --data-folder 'mnist_ Folder containing mnist data data' <p>Establish the Environment</p> <pre><code>source /opt/sambaflow/apps/starters/lenet/venv/bin/activate\n</code></pre> <p>Note:  If you receive an \\\"HTTP error\\\" message on any of the following commands, run the command again. Such errors (e.g 503) are commonly an intermittent failure to download a dataset.</p> <p>Run these commands to compile and train the LeNet model:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\nsrun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Alternatively to use Slurm sbatch, create submit-lenet-job.sh with the following contents:</p> <pre><code>#!/bin/sh\npython lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\npython lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Then</p> <pre><code>sbatch --output=pef/lenet/output.log submit-lenet-job.sh\n</code></pre> <p>Squeue will give you the queue status.</p> <pre><code>squeue\n# One may also...\nwatch squeue\n</code></pre> <p>One may see the run log using:</p> <pre><code>cat pef/lenet/output.log\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-programs/#mnist-feed-forward-network","title":"MNIST - Feed Forward Network","text":"<p>Establish the Environment</p> <pre><code>source /opt/sambaflow/apps/starters/ffn_mnist/venv/bin/activate\n</code></pre> <p>Change directory</p> <pre><code>cd ~/apps/starters/ffn_mnist/\n</code></pre> <p>Commands to run MNIST example:</p> <pre><code>srun python ffn_mnist.py  compile -b 1 --pef-name=\"ffn_mnist\" --mac-v2\nsrun python ffn_mnist.py  run -b 1 -p out/ffn_mnist/ffn_mnist.pef\n</code></pre> <p>To run the same using Slurm sbatch, create and run the submit-ffn_mnist-job.sh with the following contents.</p> <pre><code>#!/bin/sh\npython ffn_mnist.py  compile -b 1 --pef-name=\"ffn_mnist\" --mac-v2\npython ffn_mnist.py  run -b 1 -p out/ffn_mnist/ffn_mnist.pef\n</code></pre> <pre><code>sbatch --output=pef/ffn_mnist/output.log submit-ffn_mnist-job.sh\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-programs/#logistic-regression","title":"Logistic Regression","text":"<p>Establish the Environment</p> <pre><code>source /opt/sambaflow/apps/starters/logreg/venv/bin/activate\n</code></pre> <p>Change directory</p> <pre><code>cd ~/apps/starters/logreg\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-programs/#logistic-regression-arguments","title":"Logistic Regression Arguments","text":"<p>This is not an exhaustive list of arguments.</p> <p>Arguments</p> Argument Default Help Step --lr 0.001 Learning rate for training Compile --momentum 0.0 Momentum value for training Compile --weight-decay 1e-4 Weight decay for training Compile --num-features 784 Number features for training Compile --num-classes 10 Number classes for training Compile --weight-norm na Enable weight normalization Compile <p>Run these commands:</p> <pre><code>srun python logreg.py compile --pef-name=\"logreg\" --output-folder=\"pef\"\nsrun python logreg.py run --pef=\"pef/logreg/logreg.pef\"\n</code></pre> <p>To use Slurm, create submit-logreg-job.sh with the following contents:</p> <pre><code>#!/bin/sh\npython logreg.py compile --pef-name=\"logreg\" --output-folder=\"pef\"\npython logreg.py run --pef=\"pef/logreg/logreg.pef\"\n</code></pre> <p>Then</p> <pre><code>sbatch --output=pef/logreg/output.log submit-logreg-job.sh\n</code></pre> <p>The output, pef/logreg/output.log, will look something like this:</p> <pre><code>2023-03-08 21:18:25.168190: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-08 21:18:25.334389: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-08 21:18:25.334430: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-03-08 21:18:26.422458: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-03-08 21:18:26.422701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-03-08 21:18:26.422709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n[Info][SAMBA]# Placing log files in /home/wilsonb/apps/starters/logreg/pef/logreg/logreg.samba.log\n[Info][MAC]# Placing log files in /home/wilsonb/apps/starters/logreg/pef/logreg/logreg.mac.log\n...\n\nEpoch [1/1], Step [10000/60000], Loss: 0.4642\nEpoch [1/1], Step [20000/60000], Loss: 0.4090\nEpoch [1/1], Step [30000/60000], Loss: 0.3863\nEpoch [1/1], Step [40000/60000], Loss: 0.3703\nEpoch [1/1], Step [50000/60000], Loss: 0.3633\nEpoch [1/1], Step [60000/60000], Loss: 0.3553\nTest Accuracy: 91.40  Loss: 0.3014\n2023-03-08T21:19:08 : [INFO][LIB][2688517]: sn_create_session: PEF File: pef/logreg/logreg.pef\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-programs/#unet2d","title":"UNet2D","text":"<p>The UNet application example is provided in the the path : <code>/opt/sambaflow/apps/image/segmentation/</code>. As any other application, we first compile and then train the model using compile and run arguments respectively. The scripts containing the compile and run commands for UNet2D model can be accessed at Unet2d.sh or at <code>/data/ANL/scripts/Unet2d.sh</code> on any SN30 compute node.</p> <p>Change directory and copy files.</p> <pre><code>mkdir -p ~/apps/image/unet\ncd ~/apps/image/unet\n</code></pre> <p>Copy and paste the contents of Unet2d.sh to a file with the same name into the current directory using your favorite editor.</p> <pre><code>chmod +x Unet2d.sh\n</code></pre> <p>Run these commands for training (compile + train):</p> <pre><code>./Unet2d.sh compile &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n./Unet2d.sh run &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n</code></pre> <p>The <code>compile</code> and <code>run</code> arguments of the script can only be run with number of instances equal to 1, indicating that this is a simple 4 tile run without data parallel framework. For a image size of 256x256 and batch size 256 when running just 1 instance, the commands are provided as follows.</p> <pre><code>./Unet2d.sh compile 256 256 1 unet2d_single_compile\n./Unet2d.sh run 256 256 1 unet2d_single_run\n</code></pre> <p>The above commands displays the file that contains the output for the execution of the above scripts, usually <code>/data/ANL/results/&lt;hostname&gt;/&lt;userid&gt;/&lt;RunID&gt;/Unet2d.out</code></p> <p>If we inspect the compile and run commands for the UNet application provided in the script, we see that the application is compiled with <code>--num-tiles 4</code>, which means that the entire application fits on 4 tiles or half of a RDU. The pef generated from the compilation process of the above command is placed under <code>out/Unet2d/unet_train_256_256_single_4</code> inside the current working directory.</p> <pre><code>python ${UNET}/compile.py compile --mac-v2 --in-channels=3 --in-width=${2} --in-height=${2} --batch-size=${BS} --enable-conv-tiling --num-tiles=4 --pef-name=unet_train_${BS}_${2}_single_${NUM_TILES} --output-folder=${OUTDIR}\n</code></pre> <pre><code>srun --nodelist $(hostname) python /opt/sambaflow/apps/image/segmentation//hook.py run --data-cache=${CACHE_DIR}  --data-in-memory --num-workers=${NUM_WORKERS} --enable-tiling  --min-throughput 395 --in-channels=3 --in-width=${2} --in-height=${2} --init-features 32 --batch-size=${BS} --epochs 10 --data-dir ${DS} --log-dir log_dir_unet_${2}_${BS}_single_${NUM_TILES} --pef=${OUTDIR}/unet_train_${BS}_${2}_single_${NUM_TILES}/unet_train_${BS}_${2}_single_${NUM_TILES}.pef\n</code></pre> <p>The performance data is located at the bottom of log file.</p> <pre><code>inner train loop time : 374.6789753437042 for 10 epochs, number of global steps: 130, e2e samples_per_sec: 88.82270474202953\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/example-programs/#gpt-15b","title":"Gpt 1.5B","text":"<p>The Gpt 1.5B application example is provided in the the path : <code>/opt/sambaflow/apps/nlp/transformers_on_rdu/</code>. The scripts containing the <code>compile</code> and <code>run</code> commands for Gpt1.5B model can be accessed at Gpt1.5B_single.sh or at <code>/data/ANL/scripts/Gpt1.5B_single.sh</code> on any SN30 compute node. This script is compiled and run for only 1 instance and the model fits on 4 tiles or half of a RDU.</p> <p>Change directory and copy files.</p> <pre><code>mkdir -p ~/apps/nlp/Gpt1.5B_single\ncd ~/apps/nlp/Gpt1.5B_single\n</code></pre> <p>Copy and paste the contents of Gpt1.5B_single.sh to a file with the same name into the current directory using your favorite editor.</p> <p>or copy the contents from <code>/data/ANL/scripts/Gpt1.5B_single.sh</code>.</p> <pre><code>cp /data/ANL/scripts/Gpt1.5B_single.sh ~/apps/nlp/Gpt1.5B_single/\n</code></pre> <p>Run the script.</p> <pre><code>chmod +x Gpt1.5B_single.sh\n./Gpt1.5B_single.sh\n</code></pre> <p>You can inspect the <code>compile</code> and <code>run</code> commands in the script to learn that this model trains with a batch size of 16 for 1 instance over 4 tiles. The human decision file and the compiler config file helps to optimize the compute and memory resources specific to this Gpt 1.5B model run.</p> <pre><code>python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py compile --module_name gpt2_pretrain --task_name clm --max_seq_length 1024 -b 16 --output_dir=${OUTDIR}/hf_output --overwrite_output_dir --do_train  --per_device_train_batch_size 16 --cache ${OUTDIR}/cache/ --tokenizer_name gpt2 --model_name gpt2 --mac-v2 --non_split_head --mac-human-decision /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/mac_v2_overrides/gpt2_48_enc_full_recompute_training_spatialmapping_tiling16_clmerge_gm_nonpardp_lnsd.json --compiler-configs-file /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/compiler_configs/compiler_configs_gpt2_sc_recompute_spatialmapping_tiling16_clsmerge_withcls_nonpardp_norc_e2e.json --skip_broadcast_patch --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --no_index_select_patch --weight_decay 0.1  --max_grad_norm_clip 1.0 --num-tiles 4 --pef-name=gpt15_single --output-folder=${OUTDIR}\n</code></pre> <pre><code>python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py run  -b 16  --module_name gpt2_pretrain --task_name clm --max_seq_length 1024  --overwrite_output_dir --do_train  --per_device_train_batch_size 16 --cache ${OUTDIR}/cache/  --tokenizer_name gpt2 --model_name gpt2 --non_split_head --skip_broadcast_patch --no_index_select_patch --output_dir=${OUTDIR}/hf_output --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --max_grad_norm_clip 1.0 --skip_checkpoint --data_dir /data/ANL/ss1024 --logging_steps 1 --max_steps 900000 --learning_rate 0.00025 --steps_this_run 100 --pef=${OUTDIR}/gpt15_single/gpt15_single.pef &gt;&gt; ${OUTPUT_PATH} 2&gt;&amp;1\n</code></pre> <p>The <code>sntilestat</code> command shows that the application runs on 4 tiles as shown below.</p> <pre><code>/XRDU_0/RDU_0/TILE_0   2.1  96.9    0.8    0.1    0.0      0.0 796481  vsastry python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_1   2.1  96.9    0.8    0.1    0.0      0.0 796481  vsastry python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_2   2.5  96.9    0.4    0.1    0.0      0.0 796481  vsastry python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_3   2.5  96.9    0.4    0.1    0.0      0.0 796481  vsastry python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n...\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/getting-started/","title":"Getting Started","text":""},{"location":"ai-testbed/sambanova_gen2/getting-started/#on-boarding","title":"On-Boarding","text":"<p>SambaNova SN30 can be accessed using your ALCF account. See Get Started to request an account and for additional information.</p>"},{"location":"ai-testbed/sambanova_gen2/getting-started/#setup","title":"Setup","text":""},{"location":"ai-testbed/sambanova_gen2/getting-started/#system-view","title":"System View","text":"<p>Connection to a SambaNova node is a two-step process. The first step is to ssh to the login node. This step requires an MFA passcode for authentication - an eight-digit passcode generated by an app on your mobile device, e.g., MobilePASS+. The second step is to log in to a SambaNova node from the login node.</p> <p></p>"},{"location":"ai-testbed/sambanova_gen2/getting-started/#log-in-to-login-node","title":"Log in to Login Node","text":"<p>Log in to the SambaNova login node from your local machine using the below command. This uses the MobilePASS+ token generated every time you log in to the system. This is the same passcode used to authenticate into other ALCF systems, such as Polaris,  Theta and Cooley.</p> <p>In the examples below, replace ALCFUserID with your ALCF user id.</p> <pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\nPassword: &lt; MobilePASS+ code &gt;\n</code></pre> <p>Note: Use the ssh \"-v\" option in order to debug any ssh problems.</p>"},{"location":"ai-testbed/sambanova_gen2/getting-started/#log-in-to-a-sambanova-node","title":"Log in to a SambaNova Node","text":"<p>Once you are on the login node, a SambaNova node can be accessed using an alias, sn30-r[1-4]-h[1-2] where 'r' stands for the rack number, and 'h' stands for host. sn30-r1-h1 is the first host of the first rack.</p> <p>The 8 nodes are aliased as : sn30-r1-h1 , sn30-r1-h2, sn30-r2-h1, sn30-r2-h2, sn30-r3-h1, sn30-r3-h2, sn30-r4-h1, sn30-r4-h2.</p> <p>sn30-r1-h1 can be accessed as below.</p> <pre><code>ssh sn30-r1-h1\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/getting-started/#sdk-setup","title":"SDK setup","text":"<p>The required software environment (SambaFlow software stack and the associated environmental variables) for a SN30 node is set up automatically at login. This is unlike the SN10 where the environment had to be set up by each user.</p>"},{"location":"ai-testbed/sambanova_gen2/job-queuing-and-submission/","title":"Job Queueing and Submission","text":""},{"location":"ai-testbed/sambanova_gen2/job-queuing-and-submission/#introduction","title":"Introduction","text":"<p>SambaNova uses Slurm for job submission and queueing. Below are some of the important commands for using Slurm. For more information refer to Slurm Documentation.</p> <p>Note: Run the Python scripts using 'srun' or 'sbatch', to ensure that concurrent jobs do not interfere with each other.</p> <p>Note: There is just one scheduler for all of the SambaNova nodes.</p>"},{"location":"ai-testbed/sambanova_gen2/job-queuing-and-submission/#srun","title":"SRun","text":"<p>The Slurm command <code>srun</code> can be used to run individual Python scripts in parallel with other scripts on a cluster managed by Slurm. Examples of <code>srun</code> usage are shown below.</p> <p>Slurm will assign a nodelist/host to run a job if a host is not specified.</p> <p>Example:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\nsrun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>You may specify which node/host on which to run a job.</p> <p>Reasons to specify a node list:</p> <ul> <li>One wants to test a specific node to verify the function of the HW and SW  (daily smoke tests do this)</li> <li>The nodes are at different software levels and one wants to use a node that has the needed software level for one's application.</li> </ul> <p>Example:</p> <pre><code>srun --nodelist=sn30-r1-h1 python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/job-queuing-and-submission/#sbatch","title":"SBatch","text":"<p>Alternatively, these jobs can be submitted to the Slurm workload manager through a batch script by using the <code>sbatch</code> command. To do this, create a bash script (submit-lenet-job.sh here as an example) with the commands that you want to execute.</p> <pre><code>#!/bin/sh\npython lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\npython lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Then pass the bash script as an input to the <code>sbatch</code> command as shown below.</p> <pre><code>sbatch --output=pef/lenet/output.log submit-lenet-job.sh\n</code></pre> <p>In case of the need to use multiple RDUs (2 in the example shown below), the <code>sbatch</code> command would be altered as:</p> <pre><code>sbatch --gres=rdu:2 &lt;your_script.sh&gt;\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/job-queuing-and-submission/#squeue","title":"SQueue","text":"<p>The <code>squeue</code> command provides information about jobs located in the Slurm scheduling queue.</p> <pre><code>squeue\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/job-queuing-and-submission/#sinfo","title":"SInfo","text":"<p>SInfo is used to view partition and node information for a system running Slurm.</p> <p>Here is a suggested command:</p> <pre><code>sinfo -O AllocNodes, GresUsed, Gres, NodeList\n</code></pre> <p>For more information, see SInfo.</p>"},{"location":"ai-testbed/sambanova_gen2/job-queuing-and-submission/#scancel","title":"SCancel","text":"<p>SCancel is used to signal or cancel jobs, job arrays, or job steps.</p> <pre><code>scancel job_id\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/miscellaneous/","title":"Miscellaneous","text":""},{"location":"ai-testbed/sambanova_gen2/miscellaneous/#sdk-version","title":"SDK Version","text":"<p>To find the SDK version, run the following commands</p> <pre><code># TODO\n(venv) ALCFUserID@sn30-r1-h1:~$ python\nPython 3.7.6 (default, Feb 18 2020, 21:28:31)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import sambaflow\n&gt;&gt;&gt; sambaflow.__version__\n'1.11.5'\n&gt;&gt;&gt;\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/miscellaneous/#omp_num_threads","title":"OMP_NUM_THREADS","text":"<p>The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel regions.</p> <p>The value of this environment variable must be a list of positive integer values. The values of the list set the number of threads to use for parallel regions at the corresponding nested levels.</p> <p>For the SambaNova system it, is usually set to one.</p> <pre><code>export OMP_NUM_THREADS=16\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/miscellaneous/#where-is-the-model","title":"Where is the Model?","text":"<p>Two copies of the model are maintained.  One in host CPU memory and one in RDU memory. They do not interfere with each other unless you explicitly sync the model/parameter in between using:</p> <pre><code>SambaTensor.rdu() # Moves the CPU model to the RDU\nSambaTensor.cpu() # Moves the RDU model to the CPU\n</code></pre> <p>In order to run the model on the CPU, you can simply use the PyTorch model as if there is no RDU. In order to run the model on RDU, you would need to use session.run().</p>"},{"location":"ai-testbed/sambanova_gen2/miscellaneous/#useful-commands","title":"Useful Commands","text":""},{"location":"ai-testbed/sambanova_gen2/miscellaneous/#sn-configuration","title":"SN Configuration","text":"<pre><code>snconfig show Node static\n</code></pre> <p>The snconfig utility shows the static configuration of the system. The configuration for the first node is as follows:</p> <pre><code>======================================================\n=======                NODE Info               =======\n======================================================\n=======                Static Info             =======\nTimestamp: 2023-03-16 17:00:04\nPlatform Name: DataScale SN30-8\nNode Name: NODE\n    Number of XRDUS: 4\n    XRDU Name: XRDU_0\n        Number of RDUS: 2\n        RDU name: RDU_0\n            Serial Number     : 205057B469B35895\n            Number of TILES: 8\n            TILE Name: TILE_0\n                Serial Number     : N/A\n            TILE Name: TILE_1\n                Serial Number     : N/A\n\n...\n\n                    Size              : 128.0 GB\n                    Serial Number     : 1F5BC22\n            DDR CH Name: DDRCH_6\n                Number of DIMMS: 1\n                DIMM Name: DIMM_L0\n                    Size              : 128.0 GB\n                    Serial Number     : 1F5BC99\n            DDR CH Name: DDRCH_7\n                Number of DIMMS: 1\n                DIMM Name: DIMM_M0\n                    Size              : 128.0 GB\n                    Serial Number     : 1F5BB68\n        Total XRDU_3 memory size (GB): 2048.0\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/miscellaneous/#sambanova-daemon-service","title":"SambaNova Daemon Service","text":"<p>The following command checks if the SambaNova daemon service is running.</p> <pre><code>systemctl status snd\n</code></pre> <p>The output should look something like this:</p> <pre><code>\u25cf snd.service - SN Devices Service\n     Loaded: loaded (/lib/systemd/system/snd.service; enabled; vendor preset: enabled)\n    Drop-In: /etc/systemd/system/snd.service.d\n             \u2514\u2500override.conf\n     Active: active (running) since Fri 2023-01-27 04:03:14 UTC; 1 months 18 days ago\n   Main PID: 5635 (snd)\n      Tasks: 9 (limit: 629145)\n     Memory: 156.8M\n     CGroup: /system.slice/snd.service\n             \u2514\u25005635 /opt/sambaflow/bin/snd\n\nWarning: some journal files were not opened due to insufficient permissions.\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/miscellaneous/#tile-status","title":"Tile status","text":"<pre><code>sntilestat\nwatch sntilestat\n</code></pre> <p>The output shown below is when the system is completely idle.</p> <pre><code>TILE                 %idle %exec %pload %aload %chkpt %quiesce    PID     USER COMMAND\n/XRDU_0/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/miscellaneous/#finding-hung-tiles","title":"Finding Hung Tiles","text":"<pre><code>snconfig show Node dynamic | grep perfect\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/miscellaneous/#how-busy-is-the-system","title":"How busy is the system?","text":"<p>Use one of</p> <pre><code>top\nhtop\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/running-a-model-or-program/","title":"Running a Model/Program","text":"<p>Note:  Please be mindful of how you are using the system. For example, consider running larger jobs in the evening or on weekends</p> <p>Note: Please use only Slurm commands, i.e., srun and sbatch, to run your code. If you run your code directly using the 'python' command, it may cause conflicts on the system.</p>"},{"location":"ai-testbed/sambanova_gen2/running-a-model-or-program/#introduction","title":"Introduction","text":"<p>The SambaNova workflow includes the following main steps to run a model.</p> <ol> <li>Compile</li> <li>Run</li> <li>Test (optional)</li> </ol> <p>The system uses the Slurm job scheduler to schedule the jobs and manage the workload on the system. For more information on Slurm, see Job Queueing and Submission.</p> <p>Example Programs lists the different example applications with corresponding commands for each of the above steps.</p>"},{"location":"ai-testbed/sambanova_gen2/running-a-model-or-program/#compile","title":"Compile","text":"<p>Compiles the model and generates a .pef file. This file contains information on how to reconfigure the hardware, and map the compute and memory resources required to run an application on RDUs. The pef files are by default saved in the 'out' directory; the SambaNova documentation advises saving pef files in separate directories with the '--output-folder' option.</p> <p>It is necessary to re-compile only when the model changes, or parameters specific to the model graph change, including the batch size.</p> <p>Compile times can be significant. Compiling the UNet sample, for example, when using images of size 32x32 pixels, takes 358(s), and 1844(s) for images of size 256x256.</p> <p>The entire compile process is executed on the host and no RDUs are involved in the compile step.</p> <p>Example of compiling the LeNet application:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre> <p>where</p> Argument Default Help -b 1 Batch size for training"},{"location":"ai-testbed/sambanova_gen2/running-a-model-or-program/#run","title":"Run","text":"<p>As part of this step, the model is trained on the RDUs by passing in the PEF file and the training dataset. The location of the pef file generated in the compile step is passed as an argument to the run command. Below is the example of the <code>run</code> command that trains a LeNet model.</p> <pre><code>srun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>The location of the pef file generated in the compile step is passed as an argument to the run command.</p>"},{"location":"ai-testbed/sambanova_gen2/running-a-model-or-program/#test-optional","title":"Test (Optional)","text":"<p>This command is used to run the model on both the host CPU and a SambaNova RDU.  It compares the results from the CPU and RDU and will report if any discrepancies are found. Pass the pef file generated as part of the compile step as the input to this command.</p> <pre><code>srun python lenet.py test --pef=\"pef/lenet/lenet.pef\"\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/system-overview/","title":"System Overview","text":""},{"location":"ai-testbed/sambanova_gen2/system-overview/#introduction","title":"Introduction","text":"<p>The SambaNova DataScale SN30 system is architected around the next-generation Reconfigurable Dataflow Unit (RDU) processor for optimal dataflow processing and acceleration. The AI Testbed's SambaNova SN30 system consists of eight nodes in 4 full racks, each node featuring eight RDUs interconnected to enable model and data parallelism. SambaFlow, Sambanova's software stack, extracts, optimizes, and maps the dataflow graphs to the RDUs from standard machine learning frameworks like PyTorch.</p> <p>Below are some of the links to SambaNova documentation.</p> <p>SambaNova white paper: Accelerated Computing with a Reconfigurable Dataflow Architecture</p> <p>SN30 documentation: SambaNova Documentation</p>"},{"location":"ai-testbed/sambanova_gen2/tunneling-and-forwarding-ports/","title":"Tunneling and Forwarding Ports","text":"<p>Port forwarding is covered here.  This is specifically for TensorBoard.</p>"},{"location":"ai-testbed/sambanova_gen2/tunneling-and-forwarding-ports/#tensorboard-port-forwarding","title":"TensorBoard Port Forwarding","text":"<p>This section describes the steps to be followed to set up port forwarding for applications, like TensorBoard, which runs on the SambaNova system and binds to one or more ports. This example uses 6006 and 16006 as port numbers. Using port numbers other than these may avoid collisions with other users.</p>"},{"location":"ai-testbed/sambanova_gen2/tunneling-and-forwarding-ports/#from-your-local-machine","title":"From Your Local Machine","text":"<p>Replace ALCFUserID with your ALCF User ID.</p> <p>Run</p> <pre><code># Forward a port number from sambanova.alcf.anl.gov to your local machine.\nssh -v -N -f -L localhost:16006:localhost:16006 ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n\n# Connect to sambanova.alcf.anl.gov\nssh ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/tunneling-and-forwarding-ports/#from-sambanovaalcfanlgov","title":"From sambanova.alcf.anl.gov","text":"<p>Below are the commands specific to sn30-r1-h1. You may replace sn30-r1-h1 with any other node when using the appropriate system.</p> <p>Run</p> <p>Note:  The full name is sn30-r1-h1.ai.alcf.anl.gov and it may also be used.</p> <pre><code># Forward the port.\nssh -N -f -L localhost:16006:localhost:6006 ALCFUserID@sn30-r1-h1\n# Connect to the system.\nssh ALCFUserID@sn30-r1-h1\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/tunneling-and-forwarding-ports/#on-sn30-r1-h1","title":"On sn30-r1-h1","text":"<p>Activate the venv appropriate to your project.</p> <p>Navigate to the appropriate directory for your model. Launch your model using srun or sbatch.</p> <pre><code>cd /path/to/your/project\nsbatch --output=pef/my_model/output.log submit-my_model-job.sh\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/tunneling-and-forwarding-ports/#on-another-sn30-r1-h1-terminal-window","title":"On Another sn30-r1-h1 Terminal Window","text":"<p>The SambaNova system has a bash shell script to setup the required software environment. This sets up the SambaFlow software stack, the associated environmental variables and activates a pre-configured virtual environment.</p> <p>Use the command appropriate for your environment.</p> <p>For example, if you are using LogReg:</p> <pre><code>ALCFUserID@sn30-r1-h1:~$ source /opt/sambaflow/apps/starters/logreg/venv/bin/activate\n(venv) ALCFUserID@sn30-r1-h1:~$\n</code></pre> <p>Navigate to the appropriate directory for your model.</p> <pre><code>cd /path/to/your/project\ntensorboard --logdir /logs --port 6006\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/tunneling-and-forwarding-ports/#browser-on-local-machine","title":"Browser on Local Machine","text":"<p>Then, navigate in your browser to, in this example, http://localhost:16006 on your local machine.</p>"},{"location":"ai-testbed/sambanova_gen2/tunneling-and-forwarding-ports/#notes","title":"Notes","text":"<p>Explanation of ssh command:</p> <pre><code>-N : no remote commands\n\n-f : put ssh in the background\n\n-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt; :\n\nThe full command line will forward &lt;machine2&gt;:&lt;portB&gt; (remote scope) to &lt;machine1&gt;:&lt;portA&gt; (local scope)\n</code></pre> <p>Adapted from:  How can I run Tensorboard on a remote server?</p>"},{"location":"ai-testbed/sambanova_gen2/virtual-environment/","title":"Virtual Environments","text":""},{"location":"ai-testbed/sambanova_gen2/virtual-environment/#using-a-venv","title":"Using a Venv","text":"<p>To create a virtual environment, one can use the --system-site-packages flag:</p> <pre><code>python -m venv --system-site-packages my_env\nsource my_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/virtual-environment/#installing-packages","title":"Installing Packages","text":"<p>Install packages in the normal manner such as:</p> <pre><code>python3 -m pip install &lt;package&gt;\n</code></pre> <p>For more details see Use pip for installing.</p> <p>To install a different version of a package that is already installed in one's environment, one can use:</p> <pre><code>pip install --ignore-installed  ... # or -I\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/virtual-environment/#pre-built-sample-venv","title":"Pre-Built Sample Venv","text":"<p>Each of the samples or application examples provided by SambaNova has its own pre-built virtual environment which can be readily used. They are present in the <code>/opt/sambaflow/apps/</code> directory tree within each of the applications.</p> <p>Note: Conda is not supported on the SambaNova system.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/","title":"CosmicTagger Conversion","text":"<p>The intent of this page is to show conceptually how to convert a model to run on the SambaNova system. It is not necessary to convert CosmicTagger because it has already been converted and is located at CosmicTagger on the SambaNova branch. The original is located at CosmicTagger.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#run-model-on-cpu","title":"Run Model on CPU","text":"<p>The first step to converting a model is to verify that it runs on the CPU.  This step has been verified for CosmicTagger.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#configpy","title":"Config.py","text":"<p>CosmicTagger can run on multiple machines.  As such, it is necessary to specify the architecture that one is using.  For example, CPU or GPU.  The architecture is stored in the ComputeMode class.</p> <p>Edit src/config/config.py.  Add RDU to the ComputeMode class.</p> <pre><code>class ComputeMode(Enum):\nCPU   = 0\n#...\nRDU   = 6\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#trainerpy","title":"Trainer.py","text":"<p>Edit src/utils/torch/trainer.py.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#import-sambanova-packages","title":"Import SambaNova Packages","text":"<p>Insert the imports at the top of the file.</p> <p>SambaFlow is a complete software stack designed to take input from standard machine learning frameworks such as PyTorch and TensorFlow. SambaFlow automatically extracts, optimizes, and maps dataflow graphs onto RDUs.</p> <pre><code>try:\nfrom sambaflow import samba\nimport sambaflow.samba.utils as utils\nfrom sambaflow.samba.utils.argparser import parse_app_args\nfrom sambaflow.samba.utils.common import common_app_driver\nexcept:\npass\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#wrap-model","title":"Wrap Model","text":"<p>Wrap the model using poptorch.trainingModel() so that it may be ran on IPUs for training.</p> <p>Wrap the model using poptorch.inferenceModel() when not training.</p> <p>Find the following code around line 90 in the init_network method.</p> <pre><code>        # Foregoing any fusions as to not disturb the existing ingestion pipeline\nif self.is_training() and self.args.mode.quantization_aware:\nself._raw_net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\nself._net = torch.quantization.prepare_qat(self._raw_net)\nelse:\nself._net = self._raw_net\n</code></pre> <p>After the above code, add:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\nif self.is_training():\nopts = poptorch.Options()\nself._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\nelse:\nself._net = poptorch.inferenceModel(self._net)\n</code></pre> <p>See poptorch.trainingModel() and poptorch.inferenceModel() for more information.</p> <p>There is also a Build the Model tutorial.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#update-optimizer","title":"Update Optimizer","text":"<p>Update init_optimizer() to use the poptorch class instead of the torch class as needed.</p> <p>Change:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\n            self._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n        else:\n            self._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre> <p>to:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\nif self.args.run.compute_mode == ComputeMode.IPU:\nself._opt = poptorch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\nelse:\nself._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\nelse:\nif self.args.run.compute_mode == ComputeMode.IPU:\nself._opt = poptorch.optim.Adam(self._net.parameters(), 1.0)\nelse:\nself._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#update-the-forward-pass","title":"Update the Forward Pass","text":"<p>Putting the loss calculation in forward_pass() allows the loss computation to be performed on the IPUs. This will be faster because the data will not need to be transfered round-trip to the CPU.</p> <p>Change forward_pass():</p>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#original","title":"Original","text":"<pre><code>            if net is None:\nlogits_image = self._net(minibatch_data['image'])\nelse:\nlogits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#updated","title":"Updated","text":"<p>The following code changes are to account for the loss function, i.e., self.loss_calculator, and the image labels, i.e., labels_image, to be passed to the model's forward_pass method.  Additionally, the calculated loss is returned from the forward_pass method.</p> <pre><code>            if net is None:\nif self.args.run.compute_mode == ComputeMode.IPU:\nlogits_image, labels_image, loss = self._net(minibatch_data['image'], self.loss_calculator, labels_image)\nreturn logits_image, labels_image, loss\nelse:\nlogits_image = self._net(minibatch_data['image'])\nelse:\nif self.args.run.compute_mode == ComputeMode.IPU and self.args.mode.name != ModeKind.inference:\nlogits_image, labels_image, loss = net(minibatch_data['image'], self.loss_calculator, labels_image)\nreturn logits_image, labels_image, loss\nelse:\nlogits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#update-the-training-step","title":"Update the Training Step","text":"<p>Receive the extra loss variable from the forward_pass method.</p> <p>Update the train_step method.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#original-training-step","title":"Original Training Step","text":"<pre><code>                    with self.timing_context(\"forward\"):\nif self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\nwith torch.cuda.amp.autocast():\nlogits_image, labels_image = self.forward_pass(minibatch_data)\nelse:\nlogits_image, labels_image = self.forward_pass(minibatch_data)\nverbose = False\n# Compute the loss based on the logits\nwith self.timing_context(\"loss\"):\nloss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#updated-training-step","title":"Updated Training Step","text":"<p>The forward_pass() method was changed to return the extra variable loss in the previous section.  It is now received conditionally when using an IPU(s).</p> <p>In the with self.timing_context(\"loss\"): section, only calculate loss if not using an IPU(s).</p> <pre><code>                    with self.timing_context(\"forward\"):\nif self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\nwith torch.cuda.amp.autocast():\nlogits_image, labels_image = self.forward_pass(minibatch_data)\nelse:\nif self.args.run.compute_mode == ComputeMode.IPU:\nlogits_image, labels_image, loss = self.forward_pass(minibatch_data)\nelse:\nlogits_image, labels_image = self.forward_pass(minibatch_data)\nverbose = False\n# Compute the loss based on the logits\nwith self.timing_context(\"loss\"):\nif self.args.run.compute_mode == ComputeMode.IPU:\nloss = loss\nelse:\nloss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#update-validation-step","title":"Update Validation Step","text":"<p>Update the val_step method.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#original-validation-step-code","title":"Original Validation Step Code","text":"<p>Find this code.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\nwith torch.cuda.amp.autocast():\nlogits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\nelse:\nlogits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n# Compute the loss based on the logits\nloss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#updated-validation-step-code","title":"Updated Validation Step Code","text":"<p>Change the code to the following.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\nwith torch.cuda.amp.autocast():\nlogits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n# Compute the loss based on the logits\nloss = self.loss_calculator(labels_image, logits_image)\nelse:\nif self.args.run.compute_mode == ComputeMode.IPU:\nlogits_image, labels_image, loss = self.forward_pass(minibatch_data, net=val_net)\nelse:\nlogits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n# Compute the loss based on the logits\nloss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#uresnet2d-model","title":"UResNet2D Model","text":""},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#update-model","title":"Update Model","text":"<p>The Graphcore system is more computationally efficient if the loss function is on the IPU.  This is accomplished by using the loss function within the model's forward method.</p> <p>Edit src/networks/torch/uresnet2D.py.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#update-the-forward-declaration","title":"Update the Forward Declaration","text":"<p>Find the forward method.</p> <pre><code>def forward(self, input_tensor):\n</code></pre> <p>Update the argument list to include the loss function, i.e., loss_calculator and the image labels, i.e., labels_image.</p> <pre><code>def forward(self, input_tensor, loss_calculator=None, labels_image=None):\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#add-loss-calculation","title":"Add Loss Calculation","text":"<p>Add the loss calculation just before the forward method returns.</p> <pre><code>        if loss_calculator is not None:\nlabels_image = labels_image.long()\nlabels_image = torch.chunk(labels_image, chunks=3, dim=1)\nshape =  labels_image[0].shape\nlabels_image = [ _label.view([shape[0], shape[-2], shape[-1]]) for _label in labels_image ]\nloss = loss_calculator(labels_image, x)\nimport poptorch\nloss = poptorch.identity_loss(loss , reduction=\"mean\")\nreturn x, labels_image, loss\n# This return already exists.\nreturn x\n</code></pre> <p>The poptorch.identity_loss method takes a single PyTorch tensor and will backpropagate a gradient of ones through it.  You may find an example at here</p>"},{"location":"ai-testbed/sambanova_gen2/unused/cosmictagger-conversion/#binexecpy","title":"bin/exec.py","text":"<p>The following is included for completeness.  One will not likely find this in other code.</p> <p>Open bin/exec.py in your favorite editor.  Change:</p> <pre><code>@hydra.main(version_base=None, config_path=\"../src/config\", config_name=\"config\")\n</code></pre> <p>to</p> <pre><code>@hydra.main(config_path=\"../src/config\", config_name=\"config\")\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/performance-tools/","title":"Performance Tools","text":""},{"location":"ai-testbed/sambanova_gen2/unused/performance-tools/#tile-status","title":"Tile Status","text":"<pre><code>sntilestat\nwatch sntilestat\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/performance-tools/#measure-tflops","title":"Measure TFLOPs","text":"<p>This is an example for measuring TFLOPs for Conv2D forward pass.</p> <pre><code>elif args.command == 'run':\nsamba.session.run(inputs, section_types=['fwd'])\n#samba.session.run(inputs, section_types=['bckwd'])\nn_iters = 100\nforward_pass_time = []\nprint(\"run starts\")\nstart_time_forward = time.time()\nfor loop in range(n_iters):\nsamba.session.run(inputs, section_types=['fwd'])\n#samba.session.run(inputs, section_types=['bckwd'])\n#samba.session.run(inputs, section_types=['fwd', 'bckwd'])\nend_time_forward = time.time()\nforward_pass_time.append(end_time_forward - start_time_forward)\nprint(\"run ends\")\nw_0 = (args.w + 2*args.pad_w - args.s)/args.wstride + 1\nh_0 = (args.h + 2*args.pad_h - args.r)/args.hstride + 1\ntflops = 2 * (w_0*h_0) * args.s * args.r * args.c * args.k * args.n\ntflops_forw = tflops/(sum(forward_pass_time)/n_iters/5)/(10**12) #tflops\nprint(tflops)\nprint(sum(forward_pass_time))\nprint(\"tflops: %f\"%tflops_forw)\nprint(\"SN,Training,%s,Conv2d_fwd,%d,100,1,%d,%d,%d,%d,%d,%d,%d,0.0,%f,None,%f,%f,%f\" % (\"dtype\", args.n, args.w, args.h, args.c, args.k, args.s, args.pad_w, args.wstride, (sum(forward_pass_time)/n_iters)/args.n, args.n/(sum(forward_pass_time)/n_iters), tflops_forw, (sum(forward_pass_time)/n_iters)/args.n))\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/running-GPT2-multi-node/","title":"Running GPT-2 on Multiple Nodes","text":"<p>This GPT-2 example is for 1.5B parameters on two (2) nodes. Each node has eight (8) RDUs for a total of sixteen (16) RDUs.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/running-GPT2-multi-node/#create-a-directory","title":"Create a Directory","text":"<pre><code>cd &lt;path to desired directory&gt;\nmkdir GPT1.5B\ncd GPT1.5B\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/running-GPT2-multi-node/#establish-script","title":"Establish Script","text":"<p>Using your favorite editor, create the file 'Gpt1.5B.sh'.</p> <p>Copy the contents of Gpt1.5B.sh.</p> <p>Make the script executable:</p> <pre><code>chmod +x Gpt1.5B.sh\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/running-GPT2-multi-node/#multiple-nodes","title":"Multiple Nodes","text":"<p>Gpt1.5B.sh contains the sbatch command:</p> <pre><code>/usr/local/bin/sbatch --output=${HOME}/slurm-%A.out --ntasks 32 --gres=rdu:1 --ntasks-per-node 16  --nodes 2 --cpus-per-task=8  /data/ANL/scripts/Gpt1.5B_run.sh ${1} &gt;&gt; ${OUTPUT_PATH} 2&gt;&amp;1\n</code></pre> <p>The sbatch nodes argument specifies the number of nodes to use.</p> <p>nodes 2 Nodes to use.</p> <p>Additionally, here are the other sbatch arguments.</p> <p>--ntasks 32: This option specifies the number of tasks to be used in the job.</p> <p>ntasks-per-node 16: This option specifies the number of tasks per node.</p> <p>gres=rdu:1 Indicates the model fits on a single RDU.</p> <p>cpus-per-task=8 CPUs per task.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/running-GPT2-multi-node/#run","title":"Run","text":"<p>The script accepts an optional first parameter to specify the log directory.</p> <p>Run the script:</p> <pre><code>./Gpt1.5B.sh &lt;optional log directory&gt;\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/running-GPT2-multi-node/#output","title":"Output","text":"<p>The output can be found at /data/ANL/results/$(hostname)/${USER}/${LOGDIR}/${MODEL_NAME}.out. The actual path will be displayed on the screen.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/running-GPT2/","title":"Running GPT2","text":"<p>The Pile and OWT data are located in:</p> <pre><code>/data/ANL/pile\n/data/ANL/openwebtext_ss2048\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/running-bert-large-on-sn30/","title":"Running BERT-Large on SambaNova DataScale SN30-8","text":""},{"location":"ai-testbed/sambanova_gen2/unused/running-bert-large-on-sn30/#set-up","title":"Set Up","text":"<p>Establish a test directory from which to work.</p> <pre><code>mkdir $HOME/app-test\ncd $HOME/app-test\n</code></pre> <p>Copy BertLarge.sh into your current directory.</p> <pre><code>cp /data/ANL/scripts/BertLarge.sh .\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/running-bert-large-on-sn30/#running-bert-large-options","title":"Running Bert Large Options","text":"<p>Let's cover several options for executing the script.</p> <ol> <li>Basic</li> </ol> <pre><code>sbatch --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh\n</code></pre> <ol> <li>Specify a Log File</li> </ol> <p>This is helpful if doing multiple runs and one wishes to specify a run ID.    This bash script argument is optional.  Place it at the very end of the command.</p> <p>Example:</p> <pre><code>sbatch --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh my_runID\n</code></pre> <ol> <li>Specify Nodelist</li> </ol> <p>One may optionally specify a nodelist for sbatch. An example is to use hostname.</p> <pre><code>sbatch --nodelist $(hostname) --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/running-bert-large-on-sn30/#running-bert-large","title":"Running Bert Large","text":"<p>Let's specify the log file and the nodelist.</p> <p>Run</p> <pre><code>sbatch --nodelist $(hostname) --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/running-bert-large-on-sn30/#output","title":"Output","text":"<p>Display the slurm output.  For example:</p> <pre><code>cat slurm-9637.out\n</code></pre> <p>The output will look something like:</p> <pre><code>Using /data/ANL/results/sn30-r3-h1/userid/040423.19/BertLarge.out for output\n</code></pre> <p>You may display that file.  You may want to use less to do so because it is quite long.</p> <pre><code>less /data/ANL/results/sn30-r3-h1/userid/040423.19/BertLarge.out\n</code></pre> <p>The organization of the file is:</p> <ol> <li>System Status</li> <li>Compile (very long)</li> <li>Run</li> <li>System Status</li> <li>Run Duration</li> </ol>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/","title":"SambaTune","text":""},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#notes","title":"Notes","text":"<p>Rick  4/16/2023 [10:16 AM] /home/rweisner/sambatune_ui_dir contains  the 1.15.3 version which is the latest released version. It should work on your experimental. You will need browser access to wherever you install it.</p> <pre><code>cd /home/rweisner/tmp/uno_test\n</code></pre> <pre><code>#TODOBRW\nssh wilsonb@homes.cels.anl.gov\nssh sm-02\nMobilePass+ password\nOn sm-02\nsource /opt/sambaflow/venv/bin/activate\nexport PATH=/opt/sambaflow/bin:$PATH\nsambatune linear_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\nsambatune_ui --directory /home/wilsonb/tmp/sambatune_gen --port 8580\n#There will be a username and password displayed that you will use in your browser on your laptop.\nCommand used on laptop for port forward\nssh -XL 8580:127.0.0.1:8580 wilsonb@sm-02.cels.anl.gov\nMobilePass+ password\n# You will be logged into sm-02 but, you do not need to do anything.\naddress used in browser on laptop localhost:8580\n#Use username and password from sambatune_ui.\nUsername\nPassword\n\n#TODOBRW\n/home/wilsonb/DL/Sambanova/apps_1.12/private/anl/2022-09-21T19-21-05.html\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#about-sambatune","title":"About SambaTune","text":"<p>SambaTune is a tool for profiling, debugging, and tuning the performance of applications running on SN hardware.</p> <p>The tool automates the collection of hardware performance counters, metrics aggregation, report generation, and visualization. It also automates benchmarking of the application to compute average throughput over a sufficient number of runs. The tool is designed to aid the user with performance bottleneck analysis and tuning.</p> <p>SambaTune is currently used by SN engineers involved in performance tuning efforts. SambaTune is also planned for release to external customers to aid with performance bottleneck analysis and resolution.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#run-sambatune","title":"Run SambaTune","text":"<pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\n# Enter MobilePass+ pass code\nssh sm-01\n</code></pre> <pre><code>#TODOBRW\nssh wilsonb@sambanova.alcf.anl.gov\n# Enter MobilePass+ pass code\nssh sm-01\n</code></pre> <p>First, enter the virtual environment on sm-01 or sm-02:</p> <pre><code>source /opt/sambaflow/venv/bin/activate\n</code></pre> <p>Update path:</p> <pre><code>export PATH=/opt/sambaflow/bin:$PATH\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#usage","title":"Usage","text":"<pre><code>usage: sambatune [-h] [--artifact-root ARTIFACT_ROOT] [--disable-override]\n                 [--compile-only | -m MODES [MODES ...]] [--version]\n                 config\npositional arguments:\n  config                YAML file with model, compile, run configuration.\noptional arguments:\n  -h, --help            show this help message and exit\n  --artifact-root ARTIFACT_ROOT\n                        Custom location to save compile/run artifacts;\n                        defaults to '$DUMP_ROOT/artifact_root' (default: None)\n  --disable-override    Reuse the placement from the baseline compilation\n                        (default: False)\n  --compile-only        Run compilation of PEFs for selected modes only\n                        (default: False)\n  -m MODES [MODES ...], --modes MODES [MODES ...]\n                        Select modes to execute from ['benchmark',\n                        'instrument', 'run'] (default: ['benchmark'])\n  --version             version of sambatune and sambaflow.\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#command-overview","title":"Command Overview","text":"<p>By default, it will run with the benchmarking mode enabled. Use the --modes flag to run modes individually or in any combination. Benchmark-Only:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark\n</code></pre> <p>Instrument-Only:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes instrument\n</code></pre> <p>All modes:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes instrument\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#command-example","title":"Command Example","text":"<pre><code># From Bill\npython /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_500_ws --output-folder=/home/arnoldw//models_dir/1520847 --mac-v1\n\npython /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --pef=/home/arnoldw//models_dir/1520847/uno_16_4_500_ws/uno_16_4_500_ws.pef --in_dir /var/tmp/raw/ --mac-v1\n</code></pre> <pre><code># From Bill --&gt; Bruce\npython /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_500_ws --output-folder='.' --mac-v1\n\nexport OMP_NUM_THREADS=1\npython /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --pef=./uno_16_4_500_ws/uno_16_4_500_ws.pef --in_dir /var/tmp/raw/ --mac-v1\n</code></pre> <pre><code>#TODOBRW  This works.  9/19/22\nsm-01/home/wilsonb/tmp/uno_test/uno_ccle.yaml\napp: /opt/sambaflow/apps/private/anl/uno_full.py\n\nmodel-args: --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial\n\ncompile-args: compile --plot --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nrun-args: --multiprocess-pickle --use-pickle-train  --measure-spatial --train-samba-spatial --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_500 --converted-pickle\n\nenv:\n     OMP_NUM_THREADS: 16,\n     SF_RNT_NUMA_BIND: 2\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune uno_ccle.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <pre><code>#TODOBRW\n# Stand-alone\nexport UNO=.\nexport NS=500\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_${NS}_ws --output-folder='.' --mac-v1\n\nexport OMP_NUM_THREADS=1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS}\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\nRicks run python ${UNO}/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\u201cout/uno_16_4_${NS}/uno_16_4_${NS}.pef\u201d --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n</code></pre> <pre><code>#TODOBRW\nsm-01/home/wilsonb/DL/Sambanova/apps_1.12/private/anl/uno_brw_CCLE_1_12.yaml\nexport OMP_NUM_THREADS=16\napp: /home/wilsonb/DL/Sambanova/apps_1.12/private/anl/uno_full.py\n\nmodel-args: --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial\n\ncompile-args: compile --plot --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nrun-args: --measure-spatial --train-samba-spatial --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_500\n\nenv:\n     OMP_NUM_THREADS: 16,\n     SF_RNT_NUMA_BIND: 2\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune uno_brw_CCLE_1_12.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n\nexport UNO=.\nexport NS=50\nexport OMP_NUM_THREADS=1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nxsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} --epochs 1 &gt; my.log 2&gt;&amp;1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --multiprocess-pickle  --measure-spatial --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./out/uno_full_16_47_${NS}/uno_full_16_47_${NS}.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\ncat my.log # Has pyinstrument run name.\npyinstrument --load-prev 2022-09-21T19-21-05 -r html\n\n1.13\n\nsource /opt/sambaflow/venv/bin/activate\ncd ~/tmp/uno_test/\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nexport PATH=/opt/sambaflow/bin:$PATH\nsntilestat\n\n./uno_pickl.sh compile 500\n./uno_pickl.sh run 500\n</code></pre> <pre><code>sambatune uno_brw_CCLE_1_12.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n\nexport UNO=.\nexport NS=50\nexport OMP_NUM_THREADS=1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nxsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} --epochs 1 &gt; my.log 2&gt;&amp;1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --multiprocess-pickle  --measure-spatial --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./out/uno_full_16_47_${NS}/uno_full_16_47_${NS}.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\ncat my.log # Has pyinstrument run name.\npyinstrument --load-prev 2022-09-21T19-21-05 -r html\n\n1.13\n\nsource /opt/sambaflow/venv/bin/activate\ncd ~/tmp/uno_test/\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nexport PATH=/opt/sambaflow/bin:$PATH\nsntilestat\n</code></pre> <p>uno_pickl.sh</p> <pre><code>#! /bin/bash -x\n#set -e\nsource /opt/sambaflow/venv/bin/activate\nSECONDS=0\nNS=${2}\nUNO=/opt/sambaflow/apps/private/anl/\nDS=\"ALL\"\nDS=\"CCLE\"\nBS=$((NS*16))\nexport OMP_NUM_THREADS=16\necho \"Model: UNO_SPA_TRN\"\necho \"Date: \" $(date +%m/%d/%y)\necho \"Time: \" $(date +%H:%M)\nif [ \"${1}\" == \"convert\" ] ; then\npython3 ${UNO}/uno/uno_data_loaders_converted.py   --in_dir /var/tmp/raw/ --out_dir /software/sambanova/dataset/${DS}_16_${NS}  --batch-size ${BS} --train_sources ${DS} --file-write-frequency 10\nelif [ \"${1}\" == \"compile\" ] ; then\necho \"COMPILE\"\npython ${UNO}/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --mac-human-decision ${UNO}/samba_uno/human_decisions_spatial.json --pef-name=\"uno_16_4_${NS}\" --mac-v1\n\nelif [ \"${1}\" == \"run\" ] ; then\necho \"RUN ${DS}\"\nSF_RNT_NUMA_BIND=2\n#python ${UNO}/uno_full.py run --acc-test --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\npython ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --epochs 1\n#python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\"\nelif [ \"${1}\" == \"pyinstrument\" ] ; then\necho \"RUN ${DS}\"\nSF_RNT_NUMA_BIND=2\n#python ${UNO}/uno_full.py run --acc-test --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\npyinstrument ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --epochs 1\n#python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\"\nelif [ \"${1}\" == \"no_pickle\" ] ; then\necho \"no_pickle ${DS}\"\nSF_RNT_NUMA_BIND=2\npython ${UNO}/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n\nelif [ \"${1}\" == \"mp\" ] ; then\necho \"Duration: \" $SECONDS\nelif [ \"${1}\" == \"mp\" ] ; then\necho \"Duration: \" $SECONDS\necho \"PERF\"\npython uno_full.py measure-performance --measure-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --num-iterations 20 --mac-v1\nfi\necho \"Duration: \" $SECONDS\n</code></pre> <pre><code>./uno_pickl.sh compile 500\n./uno_pickl.sh run 500\n./uno_pickl.sh pyinstrument 500\npyinstrument --load-prev 2022-09-22T18-31-24 -r html\nstdout is a terminal, so saved profile output to /tmp/tmpeo5ehksn.html\ncp /tmp/tmpeo5ehksn.html .\n</code></pre> <p>On dev terminal</p> <pre><code>scp wilsonb@sambanova.alcf.anl.gov:tmp/uno_test/tmpeo5ehksn.html .\n</code></pre> <p>View in local browser.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#running","title":"Running","text":"<p>Create a directory for your work.</p> <pre><code>mkdir ~/sambatune\ncd ~/sambatune\n</code></pre> <p>Create small_vae.yaml with the following content using your favorite editor.</p> <pre><code>app: /opt/sambaflow/apps/private/anl/moleculevae.py\nmodel-args: -b 128 --in-width 512 --in-height 512\ncompile-args: compile --plot --enable-conv-tiling --compiler-configs-file /opt/sambaflow/apps/private/anl/moleculevae/compiler_configs_conv.json --mac-v2 --mac-human-decision /opt/sambaflow/apps/private/anl/moleculevae/symmetric_human_decisions_tiled_v2.json\nrun-args: --input-path /var/tmp/dataset/moleculevae/ras1_prot-pops.h5 --out-path ${HOME}/moleculevae_out --model-id 0 --epochs 10\nenv:\nOMP_NUM_THREADS: 16\nSF_RNT_FSM_POLL_BUSY_WAIT: 1\nSF_RNT_DMA_POLL_BUSY_WAIT: 1\nCONVFUNC_DEBUG_RUN: 0\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune small_vae.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>Create linear_net.yaml with the following content using your favorite editor.</p> <pre><code>app: /opt/sambaflow/apps/micros/linear_net.py\nmodel-args: &gt;\n-b 1024\n-mb 64\n--in-features 8192\n--out-features 4096\n--repeat 128\n--inference\ncompile-args: &gt;\n--n-chips 2\n--plot\nenv:\nSF_RNT_FSM_POLL_BUSY_WAIT: 1\nSF_RNT_DMA_POLL_BUSY_WAIT: 1\nCONVFUNC_DEBUG_RUN\": 0\n</code></pre> <p>NOTE: The following takes 45 minutes to run.</p> <p>Run the following example:</p> <pre><code>sambatune linear_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <pre><code>#TODOBRW\ncd ~/tmp/uno_test\nscreen\nsambatune uno.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>where linear_net.yaml is a user-specified configuration file you created above.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#sambatune-ui","title":"SambaTune UI","text":""},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#port-availability","title":"Port Availability","text":"<p>It is recommended that you check if the port you want to use is available. You may check by:</p> <pre><code>ps -elf | grep desired_port\n</code></pre> <p>Example:</p> <pre><code>ps -elf | grep 8576\n</code></pre> <p>Alternatively, you may check for all ports in use by sambatune_ui:</p> <pre><code>ps -elf | grep sambatune_ui\n</code></pre> <p>If you need to free a port that you are finished with, you may use the kill command.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#start-sambatune-ui","title":"Start SambaTune UI","text":"<p>If you followed the above directions, your artifact_root will be at ~/sambatune/artifact_root.</p> <p>Start the UI:</p> <p>It will tell you the username and password.</p> <p>NOTE: It is recommended to use a port other than 8576 in case someone else is using it.  Select another port close to 8576.</p> <p>Next</p> <pre><code>sambatune_ui --directory ~/sambatune/artifact_root/sambatune_gen/ --port 8576\n</code></pre> <pre><code>#TODOBRW\nsambatune_ui --directory ~/sambatune/artifact_root/sambatune_gen/ --port 8580\nsambatune_ui --directory /home/wilsonb/tmp/uno_test/artifact_root/sambatune_gen --port 8580\nusername: \"admin\", password: \"4f7cac2c-351e-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"aaf1fc88-35c8-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"bf64e4f8-3831-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"8feca89e-384c-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"355222d6-3a88-11ed-93a3-f7ef9c6e5d46\"\n</code></pre> <p>You will see something like:</p> <pre><code>with the,\n    username: \"admin\", password: \"05c63938-2941-11ed-93a3-f7ef9c6e5d46\"\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Starting gunicorn 20.1.0\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Listening at: http://0.0.0.0:8576 (1344959)\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Using worker: sync\n[2022-08-31 15:24:36 +0000] [1345092] [Info] Booting worker with pid: 1345092\n[2022-08-31 15:24:36 +0000] [1345093] [Info] Booting worker with pid: 1345093\n</code></pre> <p>NOTE: Write down the username and password.</p> <p>NOTE: The password only works with this one instance of sambatune_ui.  If you stop this instance of sambatune_ui and start another instance, it will have a new password.</p> <p>NOTE: You will need to &gt; or use the kill command to stop sambatune_ui when you have finished. Not doing so will tie up the port. You can ps -elf | grep the_port_you_used to find the running processes. If you are not comfortable doing this, please ask for help."},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#use-port-forwarding","title":"Use Port-Forwarding","text":"<p>This describes the steps to set up port-forwarding for applications, like SambaTune UI, which runs on the SambaNova system and binds to one or more ports. This example uses 8576 and 18576 as port numbers. Using port numbers other than these may avoid collisions with other users.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#from-your-local-machine","title":"From your local machine","text":"<p>This command sets up a port forward SambaNova login node to your local machine.</p> <p>Run</p> <pre><code>ssh -N -f -L localhost:18576:localhost:18576 ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n\nssh ALCFUserID@sambanova.alcf.anl.gov\n</code></pre> <pre><code>#TODOBRW\nssh -v -N -f -L localhost:8580:localhost:8580 wilsonb@sambanova.alcf.anl.gov\nssh -N -f -L localhost:8580:localhost:8580 wilsonb@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n\nssh wilsonb@sambanova.alcf.anl.gov\n</code></pre> <p>replacing ALCFUserID with your ALCF User ID.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#from-sambanovaalcfanlgov","title":"From sambanova.alcf.anl.gov","text":"<p>This command sets up a port forward from a SambaNova node to the sambanova login machine.</p> <p>Below are the commands specific to sm-01. You may replace sm-01 with sm-02 when using that system.</p> <p>Run</p> <p>NOTE:  The full name is sm-01.ai.alcf.anl.gov and it may also be used.</p> <pre><code>ssh -N -f -L localhost:18576:localhost:8576 ALCFUserID@sm-01\n</code></pre> <pre><code>#TODOBRW\nssh -N -f -L localhost:8580:localhost:8580 wilsonb@sm-01\n</code></pre>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#browser-on-local-machine","title":"Browser on Local Machine","text":"<p>Then, navigate in your browser to, in this example, http://localhost:18576 on your local machine.</p> <p>Use the username and password from sm-01 to log in.</p>"},{"location":"ai-testbed/sambanova_gen2/unused/sambatune-user-guide/#ssh-notes","title":"SSH Notes","text":"<p>Explanation of ssh command:</p> <pre><code>-N : no remote commands\n\n-f : put ssh in the background\n\n-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt; :\n\nThe full command line will forward &lt;machine1&gt;:&lt;portA&gt; (local scope) to &lt;machine2&gt;:&lt;portB&gt; (remote scope)\n</code></pre> <p>Adapted from:  How can I run Tensorboard on a remote server?</p>"},{"location":"cooley/cooley-overview/","title":"Cooley System Overview","text":"<p>The primary purpose of Cooley is to analyze and visualize data produced on ALCF supercomputers. Equipped with state-of-the-art graphics processing units (GPUs), Cooley converts computational data  into high-resolution visual representations. The resulting images, videos, and animations help users to better analyze and understand the data generated by ALCF supercomputers. Cooley can also be used for statistical analysis, helping to pinpoint trends in the simulation data. Additionally, the system is capable of preprocessing efforts, such as meshing, to assist users preparing for production simulations.</p> <p>Cooley mounts to the Theta file system enabling direct access to Theta-generated results. Theta's project directory can be accessed in /lus/theta-fs0/projects.</p> <p>Cooley has a total of 126 compute nodes; each node has 12 CPU cores and one NVIDIA Tesla K80 dual-GPU card.  Aggregate GPU peak performance is over 293 teraflops double precision (using base GPU clocks), and the entire system has a total of 47 terabytes of system RAM and 3 terabytes of GPU RAM.  Access to Cooley is provided by two login nodes, which provide compilation and job submission capabilities.  Job scheduling is provided by the Cobalt job scheduler. All Theta users are approved for a default allocation of 8000 node-hours on Cooley.</p>"},{"location":"cooley/cooley-overview/#cooley-node-configuration","title":"Cooley Node Configuration","text":"Specifications Architecture Intel Haswell Speed 293 teraflops Processors per node Two 6-core, 2.4-GHz Intel E5\u20132620 GPU per node 1 NVIDIA Tesla K80 w/dual GPUs Nodes 126 Cores 1,512 Memory 47 TB GPU memory 3 TB Interconnect FDR InfiniBand network Racks 6"},{"location":"cooley/compiling-and-linking/compiling-and-linking/","title":"Compiling and Linking on Cooley","text":""},{"location":"cooley/compiling-and-linking/compiling-and-linking/#compilers-and-mpi","title":"Compilers and MPI","text":"<p>GNU compilers, version 4.4.7, are installed and are available in your default environment. To use version 4.8.1 of the GNU compilers instead, add <code>+gcc-4.8.1 to .soft.cooley</code> in your home directory.</p> <p>Intel Composer XE compilers (C/C++ and FORTRAN) are installed in /soft/compilers.  To use the most current installed version, add +intel-composer-xe to .soft.cooley in your home directory.  Specific versioned keys (such as <code>+intel-composer-xe-2013</code> and <code>+intel-composer-xe-2015</code>) are also available if you require a previous version. Our installation of the Intel compilers include the Intel Math Kernel Library (MKL).</p> <p>The Clang compiler is installed in /soft/compilers/llvm. To use the most recent version, add @clang to .soft.cooley in your home directory. For more information on using the Clang compiler, please see: http://clang.llvm.org/docs/UsersManual.html.</p> <p>Multiple MPI versions are available, controlled by the .soft.cooley file in your home directory.  We currently provide both MPICH2 and MVAPICH2 for use with the GNU (+mvapich2), Clang (@mvapich2-clang) and Intel (+mvapich2-intel) compilers.  See the output of the \"softenv\" command for the most current information.  </p> <p>For example, you would put the following in your .soft.cooley file to add the GNU version of MVAPICH2 to your environment: <pre><code>+mvapich2\n@default\n</code></pre> By default, we provide all new users with the +mvapich2 key.  MVAPICH2 is designed to operate efficiently over our Infiniband interconnect, and is therefore preferred over MPICH2.</p> <p>Compiler wrappers (mpicc, mpicxx, mpif90, and so on) are included in your path for any MPI softenv key you use; each one of those softenv keys corresponds to a separate build of <code>mvapich2/mpich2</code>. So, no more than one <code>+mpich2*</code> or <code>+mvapich2*</code> key in your <code>.soft.cooley</code> is necessary. </p> <p>Also avoid explicitly providing MPI paths in link flags, since those could override what the wrappers provide. If you're using the wrappers, it is not necessary to explicitly specify any of the MPICH-related libraries.</p> <p>After you've updated your <code>.soft.cooley</code> file in your home directory run the command \"resoft\" for the changes to take effect.</p>"},{"location":"cooley/job-submission/job-and-queue-scheduling/","title":"Queueing and Job Submission on Cooley","text":""},{"location":"cooley/job-submission/job-and-queue-scheduling/#overview","title":"Overview","text":"<p>Like our other computing resources, Cooley uses the Cobalt job scheduler. </p> <p>Use the \"qstat\" command to see what jobs are in the queue, and the <code>nodelist</code> command to see which of the compute nodes (cc001-cc126) are free.  The <code>showres</code> command will display any special reservations in place.</p> <p>Use the \"qsub\" command to submit jobs; what you submit should be a script which will be executed on the rank0 node allocated to you by the scheduler. This script will have access to an environment variable named <code>COBALT_NODEFILE</code>, which is the name of a file suitable for use with mpirun's <code>-f</code> option.  One important thing to note is that the <code>--proccount</code> option to qsub has no effect on Cooley \u2013 the number of MPI processes run by your job is entirely dependent on the arguments you supply to mpirun in your script.</p> <p>At a minimum, qsub must be supplied with the number of nodes desired (-n), the walltime of the job (-t), and the path to the job script.  If you are associated with more than one project, you will also need to supply the project name using the -A option (or, alternately, set the PROJECT environment variable to that project name).</p> <p>Note: If you modify or replace your .bashrc, you will need to retain the following lines from the default .bashrc in order to ensure that your Cobalt batch jobs receive a complete software environment:</p> <pre><code>#  Source global definitions\nif [ -f /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n</code></pre> <p>Some important notes regarding the job script:   - The job script runs only on one node of your job (designated as the head node); it is up to your script to distribute processes to the other nodes in a multi-node job.  For a normal MPI job, this is accomplished by including <code>-f $COBALT_NODEFILE</code> in the arguments to mpirun/mpiexec. The <code>COBALT_NODEFILE</code> environment variable expands to the location of the nodefile (on the head node), which is a file containing the list of nodes Cobalt has assigned to your job.   - Your job script must have an interpreter at the top (for example, #!/bin/bash) in order for Cobalt to recognize it as a valid script.   - The job script will run using your default environment as set up by .bash_profile, .bashrc, and .soft.cooley at the time the job runs.  It does not inherit the environment the qsub command was run in.  If you need environment changes specific to this job, they must be set explicitly within the job script.</p> <p>For example, you might have a script named test.sh, which runs mpirun with 12 processes per node (one process per core): <pre><code>#!/bin/sh\nNODES=`cat $COBALT_NODEFILE | wc -l`\nPROCS=$((NODES * 12))\nmpirun -f $COBALT_NODEFILE -n $PROCS /home/lueningh/cpi/cpi-x86_64\n</code></pre> Note: The specific mpirun used depends on your software environment; you'll need to use a softenv key to specify which MPICH version you want to use.  By default, we give all new users +mvapich2 to use the most recent version of MVAPICH2, but if desired, a different MPI may be selected from several available.  For more information on softenv, see the softenv-intro man page.</p> <p>To request 5 nodes from cobalt with 10 minutes of walltime, charging to the MyProject project, you would use the command: <pre><code>qsub -n 5 -t 10 -A MyProject ./test.sh\n</code></pre> Cobalt will produce some files in the same directory where you ran qsub (unless of course you tell it to use a different working directory). By default, they are named .error, .output, and .cobaltlog. The .error output is stderr from your script, .output is stdout from your script, and the .cobaltlog file is some cobalt specific information like what your qsub submission contained, and how cobalt tried to invoke your executable. <p>There is also an \"interactive\" mode. \"qsub -I\" will submit an interactive job to cobalt.  </p> <p>Use it like this: <pre><code>qsub -I -n 1 -t 30\n</code></pre></p> <p>It will block until your job runs, print out the list of nodes allocated to you, and then ssh to your rank0 node. When you log out (or your requested walltime expires), your job will be removed from the queue and the nodes will be released.  </p> <p>Note: The old 'qsubi' method of running interactive jobs is deprecated and no longer available on Cooley -- please use 'qsub -I' instead.</p>"},{"location":"cooley/job-submission/job-and-queue-scheduling/#specifying-filesystems","title":"Specifying Filesystems","text":"<p>On systems running Cobalt at the ALCF your job submission should specify which filesystems your will be using.  In the event that a filesystem becomes unavailable, this information is used to preserve jobs that would use that filesystem while allowing other jobs that are not using an affected filesystem to proceed to run normally.  </p> <p>You may specify your filesystem by adding <code>filesystems=&lt;list of filesystems&gt;</code> to the <code>--attrs</code> argument of qsub in Cobalt. Valid filesystems are home, eagle, grand, and theta-fs0. The list is comma-delimited. </p> <p>For example, to request the home and eagle filesystems for your job you would add filesystems=home,eagle to your qsub command. If this is not specified a warning will be printed and then the job will be tagged as requesting all filesystems and may be held unnecessarily if a filesystem is not currently available. The warnings are written to stderr of qsub and qalter commands that change the value of the --attrs flag.  Scripts that are parsing stderr from these utilities may encounter errors from the additional warnings if filesystems are not specified in these commands.</p> <p>If a job is submitted while a filesystem it requested is marked down, the job will automatically be placed into a user_hold and a warning message will be printed, but the job will be otherwise queued. The job is also placed into admin_hold by a sysadmin script. Once the affected filesystem has been returned to normal operation, the admin_hold is released. You are responsible for releasing the user_hold once you receive the message that the affected filesystem has been returned to normal operation. The job cannot run until both the holds are released.</p> <p>If a job requesting a filesystem that is marked down is already in the queue, it will be placed on admin_hold and will be released once the filesystem is operational. <pre><code>qsub -n 128 -t 30 -q default --attrs filesystems=home,grand -A Project ./my_job.sh\n</code></pre> To update the filesystems list for your job, use qalter. Note that qalter --attrs is a replace and not an update operation. This means that you should once again specify all the attributes that you had in the original qsub command. <pre><code>qalter --attr filesystems=home,eagle:mcdram=cache:numa=quad &lt;jobid&gt;\n</code></pre> To release user hold: <pre><code>qrls &lt;jobid&gt;\n</code></pre></p>"},{"location":"cooley/job-submission/job-and-queue-scheduling/#job-scheduling-on-cooley","title":"Job Scheduling on Cooley","text":"<p>There are two primary queues on Cooley, default and debug</p>"},{"location":"cooley/job-submission/job-and-queue-scheduling/#default-queue","title":"Default Queue","text":"<p>The default queue is for production use, and is the default queue for jobs that are submitted without a queue specified. It has the following characteristics:</p> <ul> <li>Max. runtime: 12 hours</li> <li>Max. job size: 110 nodes (the other sixteen nodes are dedicated to debugging)</li> <li>Max. running jobs per user: 10</li> <li>Max. running and queued jobs per user: 30</li> <li>Max. node-hours (queued and running): 1320</li> <li>Priority: FIFO -- (jobs are run in order, with small, short jobs run on any otherwise-free nodes)</li> </ul>"},{"location":"cooley/job-submission/job-and-queue-scheduling/#debug-queue","title":"Debug Queue","text":"<p>In addition, there are sixteen nodes set aside for dedicated debugging in the debug queue. This is intended for short debugging and interactive visualization runs only. It has the following scheduling policy:</p> <ul> <li>Max. runtime: 2 hour</li> <li>Max. job size: 16 nodes</li> <li>Max. running jobs per user: 1</li> <li>Priority: FIFO -- (jobs are run in order, with small, short jobs run on any otherwise-free nodes)</li> </ul>"},{"location":"cooley/job-submission/job-and-queue-scheduling/#public-network-connectivity","title":"Public Network Connectivity","text":"<p>For jobs that require public network connectivity on the compute nodes (i.e. connectivity to non-ALCF resources), you may include the argument --<code>attrs=pubnet</code> in your qsub command . </p>"},{"location":"cooley/job-submission/job-and-queue-scheduling/#gpus-directly-for-computation","title":"GPUs Directly for Computation","text":"<p>For jobs that use the GPUs directly for computation (e.g. CUDA) and don't require an X sever, you may wish to include the argument <code>--attrs=nox11</code> in your qsub command.  This will stop the X server that normally runs on the nodes in order to prevent any performance impact on your GPU jobs.</p> <p>If required, the above job attributes may be combined as a colon-separated list, e.g. <code>--attrs=pubnet:nox11</code></p> <p>While we currently continue to maintain special-purpose queues for the above functions (the queues named pubnet, pubnet-debug, nox11, and pubnet-nox11) in order to maintain compatibility for submission scripts that use them, these queues have been deprecated in favor of using the above job attributes, which provide more flexbility and can be used within reservations.</p> <p>If you have needs not addressed by the standard queues, please send mail to support@alcf.anl.gov requesting a reservation.</p> <p>We will monitor Cooley's queues and evaluate the above policies as needed. Your feedback is appreciated.</p>"},{"location":"cooley/performance-tools/darshan/","title":"Darshan on Cooley","text":""},{"location":"cooley/performance-tools/darshan/#overview","title":"Overview","text":"<p>Darshan is a lightweight I/O instrumentation library that can be used to investigate the I/O behavior of production applications. It records statistics, such as the number of files opened, time spent performing I/O, and the amount of data accessed by an application.</p>"},{"location":"cooley/performance-tools/darshan/#enabling-darshan-on-cooley","title":"Enabling Darshan on Cooley","text":"<p>Darshan is not automatically enabled for all jobs on Cooley. Unlike Theta, all applications on Cooley are dynamically linked by default, which means that Darshan must be loaded at runtime using the LD_PRELOAD environment variable. In order to instrument a job on Cooley, you must first add the SoftEnv key +darshan to your ~/.soft.cooley file and run the \u201cresoft\u201d command. </p> <p>Then add the following to the mpirun command line in your job script: <pre><code> --env LD_PRELOAD=$DARSHAN_PRELOAD\n ```\n\nExample:\n</code></pre> within Cooley job script mpirun --env LD_PRELOAD=$DARSHAN_PRELOAD -np  -f $COBALT_NODEFILE ./app.exe  <pre><code>After your job completes, you can find the Darshan output file in the following directory:\n</code></pre> /lus/theta-fs0/logs/darshan/cooley/// ``` <p>The same tools described in the Theta documentation can be used to interpret Darshan output files generated on Cooley.</p>"},{"location":"cooley/performance-tools/remote-visualization-using-vnc/","title":"Remote Visualization on Cooley Using VNC","text":"<p>When running graphics applications on Cooley, it is best to use the client/server mode when available. </p> <p>A lightweight client can run on your local resource and connect to a server application running on the Cooley visualization nodes.  For applications that do not support a client/server mode, VNC can be used for remotely accessing such applications running on Cooley, and leveraging its GPUs.</p>"},{"location":"cooley/performance-tools/remote-visualization-using-vnc/#setup-on-cooley","title":"Setup on Cooley","text":"<p>On cooley.alcf.anl.gov, if you do not have a ~/.vnc/xstartup file, create one like the following: <pre><code>#!/bin/sh\nxterm &amp;\ntwm\n</code></pre></p> <p>Be sure to make it executable: <pre><code>&gt; chmod u+x ~/.vnc/xstartup\n</code></pre></p> <p>Also, create a VNC password, which you will need to provide each time you connect a remote VNC client to a VNC server running on Cooley: <pre><code>&gt; vncpasswd\n</code></pre></p> <p>This will store an obfuscated version in ~/.vnc/passwd</p>"},{"location":"cooley/performance-tools/remote-visualization-using-vnc/#start-a-vnc-server-on-cooley","title":"Start a VNC server on Cooley","text":"<p>Since we want the VNC server to run on a backend node, in order to leverage the GPU, we need to submit a job: <pre><code>&gt; qsub -I -n 1 -t &lt;time&gt; -A &lt;projectID&gt;\n</code></pre></p> <p>Once your job starts, you will be logged into a visualization node, where you can launch a VNC server: <pre><code>&gt; x0vncserver --display=:0.0 --NeverShared=1 --geometry=2400x1500+0+0 --PasswordFile=/home/&lt;username&gt;/.vnc/passwd --MaxProcessorUsage=100\n</code></pre></p> <p>Note: Take note of the hostname where your job is running (in the form cc###). You will need this in the next steps.</p> <ul> <li>We use x0vncserver so that we can leverage the existing X server running on the node, which uses the GPU.  </li> <li>We specify --display=:0.0 to tell it which display to use.</li> <li>Because the existing display has a resolution of 4096x4096, we use the --geometry flag to specify a region of that display to use.  This should be set this to a size appropriate for displaying on your local display.  You may also wish to adjust the +0+0 to adjust the portion of the display that is visible.</li> <li>Replace  with your login name in the path to your VNC PasswordFile. <li>Since we will have exclusive use of the node, we set the --MaxProcessorUsage=100 (otherwise the default is 35).</li>"},{"location":"cooley/performance-tools/remote-visualization-using-vnc/#on-your-local-resource","title":"On Your Local Resource","text":"<p>From a shell on your local resource, establish an ssh tunnel through the Cooley login node to the backend node where you started the VNC server (the cc### noted above.) This will require the use of your OTP token.</p> <pre><code>&gt; ssh -L 5900:cc###:5900 &lt;username&gt;@cooley.alcf.anl.gov\n</code></pre> <p>Once the ssh connection is established, from this shell launch the xstartup script on your visualization node.  </p> <p>If your default shell is bash, use the following command (this will block, and not return you to a command prompt): <pre><code>ssh cc### \u201cexport DISPLAY=:0.0; ~/.vnc/xstartup\u201d\n</code></pre></p> <p>If your default shell is csh/tcsh, use the following command (this will block, and not return you to a command prompt): <pre><code>ssh cc### \u201csetenv DISPLAY :0.0; ~/.vnc/xstartup\u201d\n</code></pre></p> <p>Now in a start a vnc viewer on your local resource, for example: <pre><code>&gt; xvncviewer localhost::5900\n</code></pre></p> <p>Notes: - Since we are tunneling, set the host to localhost. - Syntax for VNC clients may vary.  Check the documentation for your specific client to determine appropriate syntax for specifying the host and port.</p> <p>This should open a VNC viewer with an xterm running in it, where you can launch graphics applications running on the Cooley backend node, and taking advantage of the GPU.</p> <p>Additional note: Because you are likely not using the full 4096x4096 area of the display, it is possible that some applications that automatically place their windows may place them outside of the region that you are viewing.  Some application may provide a mechanism for placing the window at a specific location.  Otherwise, you may need to adjust the +0+0 portion of the --geometry flag when running the x0vncserver executable to adjust the portion of the display that is visible.</p>"},{"location":"cooley/performance-tools/remote-visualization-using-vnc/#cleaning-up","title":"Cleaning Up","text":"<p>When you are all done, be sure to clean up:</p> <ul> <li>Exit the VNC viewer</li> <li>Kill the VNC server (cntrl C), and exit the cc### shell</li> <li>You may need to cntrl C to exit the ssh command in the shell used to create the tunnel </li> <li>Then exit that shell to close the tunnel</li> </ul>"},{"location":"cooley/programming-models/kokkos/","title":"Kokkos on Cooley","text":""},{"location":"cooley/programming-models/kokkos/#overview","title":"Overview","text":"<p>Kokkos implements a programming model in C++ for writing performance portable applications targeting all major HPC platforms. It provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. It can use OpenMP, etc as backend programming model. For more information please visit https://github.com/kokkos/kokkos</p> <p>The Kokkos shared memory programming model is a C++ library, that provides the necessary architecture specific backends (e.g. OpenMP, CUDA, \u2026). To begin with, though, it is important to note that the Kokkos programming model is usable only in C/C++ codes. Hence, for those with Fortran codes, Kokkos must first be encapsulated within C/C++ functions and called from the main Fortran code.</p> <p>The purpose of this document is to provide guidance on using Kokkos on Cooley. Please see the following pages for tutorial and more information on Kokkos: Kokkos GitHub and Kokkos Tutorials.</p>"},{"location":"cooley/programming-models/kokkos/#using-kokkos-at-alcf","title":"Using Kokkos at ALCF","text":"<p>ALCFprovides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"cooley/programming-models/kokkos/#building-kokkos-on-cooley","title":"Building Kokkos on Cooley","text":"<p>Users should look at the <code>/soft/ProgrammingModels/KOKKOS-JULY-29-2019/</code> directory. The project directory consists of the following directories that users should feel free to copy over to their own directories. <pre><code>[bramesh@cooleylogin2 KOKKOS-JULY-29-2019]$ ls\nkokkos/  kokkos-tools/  kokkos-tutorials/\n[bramesh@cooleylogin2 KOKKOS-JULY-29-2019]$\n</code></pre></p> <p>In order to compile and use Kokkos on Cooley, users will need the following settings in the .soft.cooley file: <pre><code>+mvapich2-2.2\n+cuda-10.0\n+gcc-7.1.0\n@default\n</code></pre></p> <p>The idea behind these settings is to use the CUDA-10 wrapper, \u201cnvcc,\u201d together with the gcc-7.1.0 compiler, and the mvapich 2-2.2 build with the gcc-7.1.0 compiler. In order to compile/run Kokkos, users are encouraged to look at the tutorial example(s) in the following directories: <pre><code>/soft/ProgrammingModels/KOKKOS-JULY-29-2019/kokkos-tutorials/Intro-Full/Exercises/&lt;01,02, \u2026&gt;/Begin\n</code></pre></p> <p>The Makefile(s), and the settings therein, should indicate the specifications for the KOKKOS_DEVICE variable that one would need for either the OpenMP, or CUDA, or OpenMP+CUDA backends in Kokkos.</p>"},{"location":"cooley/programming-models/opencl/","title":"OpenCL on Cooley","text":""},{"location":"cooley/programming-models/opencl/#overview","title":"Overview","text":"<p>OpenCL (Open Computing Language) is an open standard from the Khronos group that enables heterogeneous device programming (e.g. CPUS, GPUs, and FPGAs). Complete descriptions of the API, memory hierarchy, and usage can be found in OpenCL documentation and technical specifications found on the Khronos website.</p>"},{"location":"cooley/programming-models/opencl/#using-opencl-at-alcf","title":"Using OpenCL at ALCF","text":"<p>OpenCL is provided on Cooley via the NVIDIA GPU drivers for the Kepler K80 GPUs. OpenCL 1.2 is supported on Cooley. There is not an OpenCL CPU driver available on Cooley to launch OpenCL kernels on the CPUs.</p>"},{"location":"cooley/programming-models/opencl/#building-on-cooley","title":"Building on Cooley","text":"<p>To build an OpenCL application for the GPUs on Cooley, one can modify their software environment to load CUDA. <pre><code>soft add +cuda-10.0\nsoft add +gcc-7.1.0\nexport CPATH=/soft/visualization/cuda-10.0/include:$CPATH\nexport LIBRARY_PATH=/soft/visualization/cuda-10.0/include:$LIBRARY_PATH\n</code></pre></p> <p>The following is an example to compile a simple OpenCL application. <pre><code>mpicxx main.cpp -lOpenCL \n</code></pre></p>"},{"location":"cooley/programming-models/opencl/#running-jobs-on-cooley","title":"Running jobs on Cooley","text":"<p>An example \u2018test.sh\u2019 job submission script follows. <pre><code>#!/bin/sh\nNODES=`cat $COBALT_NODEFILE | wc -l`\nPROCS=$((NODES * 1))\nmpirun -f $COBALT_NODEFILE -n $PROCS ./a.out\n</code></pre></p> <p>To request a single node with 10 minutes of walltime, charging to the MyProject project, one can use the following command. <pre><code>qsub -n 1 -t 10 -A MyProject ./test.sh\n</code></pre></p>"},{"location":"cooley/programming-models/opencl/#alcf-tutorials","title":"ALCF Tutorials","text":"<p>There is an OpenCL tutorial available from the ALCF on GitHub that provides a walk through on several concepts of the programming model: introduction to API, querying device info, compiling kernels, using buffers, profiling, etc\u2026 The repo can be cloned to your working directory with one of the following commands. <pre><code>git clone https://github.com/alcf-perfengr/alcl.git --branch cooley\n\nor\n\ngit clone git@github.com:alcf-perfengr/alcl.git --branch cooley\n</code></pre></p> <p>The tutorial provides examples for C, C++, Python, and Ruby. Individual C and C++ examples can be built and run, from their respective directories using \u2018make\u2019 and the name of the example (e.g. platform). <pre><code>cd alcl/C++\nmake platform\nmake run_platform\n</code></pre> Note: OpenCL examples will only run correctly on the Cooley compute nodes as there is no CPU driver installed. For the Python examples, users will need to first install pyopencl before running similar \u2018make\u2019 commands.</p> <pre><code>pip install --user --upgrade numpy pyopencl\n\ncd alcl/Python\nmake my_first_kernel\nmake run_my_first_kernel\n</code></pre>"},{"location":"cooley/programming-models/openmp/","title":"OpenMp on Cooley","text":""},{"location":"cooley/programming-models/openmp/#overview","title":"Overview","text":"<p>The OpenMP API is an open standard for parallel programming. The specification document can be found here: https://www.openmp.org. The specification describes directives, runtime routines, and environment variables that allow an application developer to express parallelism (e.g. shared memory multiprocessing and device offloading). Many compiler vendors provide implementations of the OpenMP specification.</p>"},{"location":"cooley/programming-models/openmp/#using-openmp-at-alcf","title":"Using OpenMP at ALCF","text":"<p>OpenMP support for CPUs on Cooley is provided through the GNU, Intel, and LLVM Clang compilers available on Cooley. Guidance on updating your environment to use one of these compilers is available here.</p> <p>OpenMP offload support for GPUs on Cooley is provided via community compilers. The LLVM Clang compiler is installed on Cooley to support OpenMP 4.5+ offload features. The compiler is in rapid development and ALCF staff build it frequently from the master branch. </p> <p>The status of offload features in this compiler is available on the LLVM Clang website. Considering that the compiler is under active development, the compiler may contain bugs and those should be reported directly to the compiler team here.</p>"},{"location":"cooley/programming-models/openmp/#building-on-cooley","title":"Building on Cooley","text":"<p>OpenMP parallelism for CPUs can be enabled for each supported compiler using the appropriate compiler flag: -fopenmp for GNU/Clang and -qopenmp for Intel compilers. <pre><code>soft add +intel-composer-xe-2018\n\nicpc -qopenmp main.cpp\n</code></pre></p> <p>OpenMP settings, such as number of threads and affinity, can be controlled via OpenMP environment variables.</p> <p>The offload compiler is installed on Cooley at /soft/compilers/clang-ykt. To use this compiler, first update your software environment with the following CUDA and gcc softkeys and paths. <pre><code>soft add +cuda-10.0\nsoft add +gcc-6.4.0\nexport PATH=/soft/compilers/clang-ykt/latest/bin:$PATH\nexport LD_LIBRARY_PATH=/soft/compilers/clang-ykt/latest/lib:$LD_LIBRARY_PATH\n</code></pre></p> <p>The following compiler flags are needed to enable offload compilation: <code>-fopenmp -fopenmp-targets=nvptx64-nvidia-cuda</code>. <pre><code>clang++ -fopenmp -fopenmp-targets=nvptx64-nvidia-cuda main.cpp\n</code></pre></p>"},{"location":"cooley/programming-models/openmp/#running-jobs-on-cooley","title":"Running jobs on Cooley","text":"<p>An example \u2018test.sh\u2019 job submission script follows. <pre><code>#!/bin/sh\nNODES=`cat $COBALT_NODEFILE | wc -l`\nPROCS=$((NODES * 1))\nmpirun -f $COBALT_NODEFILE -n $PROCS ./a.out\n</code></pre></p> <p>To request a single node with 10 minutes of walltime, charging to the MyProject project, one can use the following command. <pre><code>qsub -n 1 -t 10 -A MyProject ./test.sh\n</code></pre></p>"},{"location":"cooley/programming-models/openmp/#examples","title":"Examples","text":"<p>There are a handful of simple examples available in the /soft/compilers/clang-ykt/example directory. To run an example, copy the source file to current working directory, compile, and submit to a compute node in an interactive job or as batch job using example script above. <pre><code>cp /soft/compilers/clang-ykt/example/test_simple2.cpp ./\nclang++ -fopenmp -fopenmp-targets=nvptx64-nvidia-cuda test_simple2.cpp\n\n./a.out\nenter constructor 0x7ffe3f6bb648\nhost pointer 0x2125590\ndevice pointer 0x620b840200\nRunning target region on device!\nmaptest constructor\ncheck_size = 6\ncheck_value = 1\n</code></pre></p> <p>The NVIDIA tools can be used to debug and profile offloaded kernels compiled with the OpenMP offload clang-ykt compiler. For example, nvprof can be used to profile and verify that your application offloaded kernels to the GPUs on Cooley. <pre><code>nvprof ./a.out \n==2755== NVPROF is profiling process 2755, command: ./a.out\nenter constructor 0x7ffea1bb91c8\nhost pointer 0x3fcc7a0\ndevice pointer 0x620c240200\nRunning target region on device!\nmaptest constructor\ncheck_size = 6\ncheck_value = 1\n==2755== Profiling application: ./a.out\n==2755== Profiling result:\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n GPU activities:   48.51%  156.96us         1  156.96us  156.96us  156.96us  __omp_offloading_2d_e6fe0d__ZN7maptestIdEC1Em_l18\n                   46.44%  150.27us         1  150.27us  150.27us  150.27us  __omp_offloading_2d_e6fe0d__ZN7maptestIdE3runEv_l51\n                    3.56%  11.520us         5  2.3040us  1.9840us  2.7520us  [CUDA memcpy DtoH]\n                    1.49%  4.8310us         3  1.6100us  1.3110us  2.2080us  [CUDA memcpy HtoD]\n      API calls:   75.94%  292.38ms         1  292.38ms  292.38ms  292.38ms  cuCtxCreate\n                   21.48%  82.678ms         1  82.678ms  82.678ms  82.678ms  cuCtxDestroy\n                    0.98%  3.7905ms       256  14.806us  1.3690us  458.24us  cuStreamCreate\n                    0.69%  2.6478ms         1  2.6478ms  2.6478ms  2.6478ms  cuModuleLoadDataEx\n                    0.39%  1.4933ms         1  1.4933ms  1.4933ms  1.4933ms  cuModuleUnload\n                    0.17%  638.23us       256  2.4930us  2.0450us  23.244us  cuStreamDestroy\n\u2026\n</code></pre></p>"},{"location":"cooley/programming-models/raja/","title":"RAJA on Cooley","text":""},{"location":"cooley/programming-models/raja/#overview","title":"Overview","text":"<p>RAJA is a collection of C++ software abstractions, being developed at Lawrence Livermore National Laboratory (LLNL), that enable architecture portability for HPC applications. The overarching goals of RAJA are to:</p> <p>Make existing (production) applications portable with minimal disruption Provide a model for new applications so that they are portable from inception. RAJA targets portable, parallel loop execution by providing building blocks that extend the generally-accepted parallel for idiom.</p> <p>Additional information can be found at RAJA User Guide.</p>"},{"location":"cooley/programming-models/raja/#using-raja","title":"Using RAJA","text":"<p>RAJA provides a project template for how to use RAJA in an application project that uses CMake or Make. This is located at RAJA Project Template.</p>"},{"location":"cooley/programming-models/raja/#how-to-get-the-source-code","title":"How to get the source code","text":"<p>The RAJA source code lives at RAJA github.</p> <p>It can be cloned with git clone --recursive https://github.com/llnl/raja.git. The recursive clone will also clone RAJA's dependencies in the proper locations.</p>"},{"location":"cooley/programming-models/raja/#building-on-cooley","title":"Building on Cooley","text":"<p>RAJA requires a compiler with C++11 support and CMake version 3.9 or greater. RAJA includes an example build script for Cooley that you can use. A quick example is below. <pre><code>cd raja\ncp scripts/alcf-builds/cooley_nvcc9.1_clang4.0.sh .\n./cooley_nvcc9.1_clang4.0.sh\ncd build_alcf-cooley-nvcc9.1_clang4.0/\nmake -j 4\n</code></pre></p> <p>The build script makes use of a CMake configuration file <code>raja/host-configs/alcf-builds/cooley_nvcc_clang4.0.cmake</code>. The RAJA and compiler options can be adjusted in this configuration file.</p>"},{"location":"cooley/software-and-libraries/jupyter-notebooks/","title":"Jupyter Notebooks on Cooley","text":""},{"location":"cooley/software-and-libraries/jupyter-notebooks/#jupyter-notebooks-on-cooley_1","title":"Jupyter Notebooks on Cooley","text":"<p>Frequently, it's very useful for prototyping, analysis, or debugging to have an interactive session on Cooley using jupyter notebooks to debug your python scripts. The single node containers supported by the ALCF datascience group have jupyter and common python packages installed, and so it is possible to use jupyter notebooks with the production ML/DL software in an interactive way.</p> <p>For more information about the supported software in the containers, or alternative ways to get pytorch/tensorflow, please see the page on Machine Learning Tools.</p>"},{"location":"cooley/software-and-libraries/jupyter-notebooks/#setting-up-jupyter-on-an-interactive-node","title":"Setting up Jupyter on an Interactive node","text":"<p>To use jupyter notebooks on the GPUs, you will need an interactive node. Please refer to the cobalt and job submission documentation for details, but a simple interactive node request could look like this: <pre><code>qsub -I -n 1 -t 60 -A [project] -q debug --attrs nox11.  \n</code></pre></p> <p>If you need network access from your interactive node, be sure to use the pubnet queues.</p> <p>Once your interactive job has started, take note of which node you are on.  On Cooley, it's possible to use ssh from the login node directly to the worker nodes. There are several ways to see which node your job is running on. The easiest is to see in your terminal the <code>user@ccXXX</code> where XXX is the node number, ranging from 001 to 0016 for the debug queue, and cc017 to cc126 for the other queues. You can also find the node information from <code>qstat -u [username]</code>.</p> <p>From the interactive node, you can launch one of the datascience containers (\"singularity exec --nv /soft/datascience/singularity/pytorch/centos7-cuda9.0-torch1.0.img  bash\"). This will give you a shell inside the container, and from there you can start a jupyter notebook with: <pre><code>juypter notebook.\n</code></pre></p> <p>You will see output from jupyter, including a link to access the notebook in a browser. The link will look something like <code>http: //localhost:8888/?token=[long string of numbers and letters]</code>.  This should indicate that your notebook is up and running on the interactive node.  The port in the above address (8888) can be configured with the <code>--port</code> syntax to <code>jupyter notebook</code>, if you want to use a different port.</p>"},{"location":"cooley/software-and-libraries/jupyter-notebooks/#connecting-your-laptop-browser-to-your-juypter-notebook","title":"Connecting your laptop browser to your juypter notebook","text":"<p>To use your browser on your laptop with jupyter hub, connect to cooley with port forwarding to the login node. The easiest way to do this is to open a new terminal and create an ssh connection to Cooley and connect the port number from your jupyter notebook. </p> <p>If it's port 8888 like in the example above, do: <pre><code>ssh -L 8888:localhost:8888 user@cooley.alcf.anl.gov.  \n</code></pre></p> <p>From the login node, make a second port-forwarding ssh connection to the interactive node running your jupyter notebook: <pre><code>`ssh -L 8888:localhost:8888 ccXXX`, \n</code></pre> where ccXXX is the interactive node with the correct numbers.  This will forward the port running jupyter notebook on the interactive node to the login node, and from the login node to your local computer.  You can copy/paste the link from jupyter into your browser, and a jupyter hub window should appear as normal in your browser.  Since it is running the datascience container on the interactive node, you will have access to tensorflow or pytorch (whichever you picked) and the other available software from your jupyter notebook.</p> <p>Running your software on the GPUs is then exactly the same as in a normal python script, by calling <code>x.cuda()</code> for a pytorch tensor x for example.</p> <p>For questions or bug reports on jupyter on Cooley, please email support@alcf.anl.gov</p>"},{"location":"cooley/software-and-libraries/machine-learning-tools/","title":"Machine Learning Tools on Cooley","text":"<p>Cooley is designed as a visualization cluster, because of the support for Nvidia GPUs. However,  it is also possible to run machine learning workflows on Cooley. </p> <p>Horovod will not have any significant impact on performance on single GPUs, however since the K80 nodes have 2 GPUs per node it is recommended you use horovod with data parallel learning to take advantage of both GPUs.</p>"},{"location":"cooley/software-and-libraries/machine-learning-tools/#running-a-machine-learning-workflow-on-cooley-with-containers","title":"Running a machine learning workflow on Cooley with containers","text":"<p>For more information about building containers for Cooley, please see the Singularity on Cooley page.  This section will focus on using containers for machine learning and deep learning workflows.</p> <p>Because singularity is setting up a containerized system, there are several important steps to take note of:</p> <ol> <li> <p>Use the <code>--nv</code> option to singularity exec to enable Nvidia GPU drivers within the container.  Without this, you will not be able to take advantage of Nvidia gpu acceleration.</p> </li> <li> <p>Make sure you bind necessary directories correctly.  By default, not all areas mounted on the host system (outside the container) are available inside the container.  To access an area, you can bind it with the -B outside_loc:inside_loc syntax.  For example, to access the theta projects area from inside a container on Cooley, use <code>-B /lus:/lus</code> as part of your singularity command.</p> </li> <li> <p>Run the container inside of mpirun calls.  For example, do <code>mpirun -n 2 singularity exec --nv -B /lus:/lus $IMAGE /path/to/python/script.py</code> and NOT <code>singularity exec $IMAGE mpirun -n 2 /path/to/python/script.py</code> (where $IMAGE is the path to the container you want to run)</p> </li> </ol> <p>Running the mpi containers with both GPUs per node has been demonstrated to scale to many nodes on Cooley, so distributed learning is feasible on Cooley.</p>"},{"location":"cooley/software-and-libraries/machine-learning-tools/#extending-available-software-in-containers","title":"Extending Available Software in Containers","text":"<p>If you start with an existing Singularity container, it is possible to add additional software.  The most straightforward path is to install it via pip while in the container, using the <code>--user</code> flag if you can.  In this way, you can add extensions to tensorflow/pytorch, or IO frameworks, etc. Email support@alcf.anl.gov for questions concerning these techniques.</p>"},{"location":"cooley/software-and-libraries/machine-learning-tools/#non-container-software-solutions","title":"Non-container software solutions","text":"<p>It is perfectly possible to run tensorflow, pytorch, etc outside of a container on Cooley.  We don't support official builds or distributions of this, but because Nvidia GPUs are very common for ML and DL software, there are many excellent tools available for getting GPU optimized tensorflow, pytorch, etc.  Solutions that can work on Cooley are pip, conda, and virtualenv, and possibly others.  Note that you will need to add Cuda libraries from softenv to use these tools.</p>"},{"location":"cooley/software-and-libraries/paraview-tutorial/","title":"ParaView Tutorial on Cooley","text":""},{"location":"cooley/software-and-libraries/paraview-tutorial/#overview","title":"Overview","text":"<p>This tutorial is intended to be a hands-on resource for users interested in learning the basic concepts of ParaView. The examples can easily be run on a laptop, using the example data set provided.</p> <ul> <li>Tour of ParaView</li> <li>Show range of visualization methods</li> <li>Walk through various visualization techniques, hopefully illustrate how these can apply to your own data</li> <li>Feel for ParaView \"way\"</li> <li>Terminology and step-by-step process peculiar to ParaView, which may differ from other packages, e.g. VisIt</li> </ul> <p> </p> Bloodflow Visualization by Joe Insley, ALCF"},{"location":"cooley/software-and-libraries/paraview-tutorial/#data","title":"Data","text":"<p>The data used for this tutorial is: - Blood flow simulation data - Multiple data types   - Continuum data field (unstructured mesh, tetrahedral): fluid field, plasma   - Particle data (unstructured points): individual particles moving in the flow   - Red Blood Cells (RBC, unstructured mesh, triangle): mesh of the surface of an RBC     - Healthy     - Diseased - Generated using an integrated Nektar/LAMMPS simulation code - Courtesy of George Karniadakis and Leopold Grinberg of Brown University</p> <p>The data is available for download here (~27MB compressed, ~39MB uncompressed): Data set for ParaView Red Blood Cell Tutorial</p>"},{"location":"cooley/software-and-libraries/paraview-tutorial/#1-load-multi-component-dataset","title":"1. Load Multi-component Dataset","text":"<ul> <li>From the Filemenu, (you can also click the file folder icon, shown above) open each of the following data sets (select then click \"OK\")</li> <li>The files will then appear in the Pipeline Browser</li> <li>Click Apply in the Object Inspector</li> <li>You will need to do this one at a time:</li> <li>continuum...vtu</li> <li>particles...vtu</li> <li>rbc_...vtu</li> <li>bad_rbc...vtu Note: The \"...\" in the name, and the arrow in the file browser, indicates that there are multiple time steps for each of these files</li> </ul> With all of the default settings, you should see something like this"},{"location":"cooley/software-and-libraries/paraview-tutorial/#2-select-which-data-to-view","title":"2. Select which data to view","text":"<p>Let's start by looking at the continuum.000*data. This is an unstructured mesh that has velocity and count (density) values. - Hide other data sets using the Eyeball icon next their names in the Pipeline Browser.   - Black = visible, Grey = hidden - Select continuum.000*(name is highlighted) in the Pipeline Browser   - Click on the name to highlight it - When manipulating appearance or applying filters, these always affect the selected data set - Switch to the Display tab in the Object Inspector - Under Color by, select Velocity from the dropdown   - There is also a shortcut to Color by in the menu bar near the top of the GUI   - </p> <p> </p> Select which data to view"},{"location":"cooley/software-and-libraries/paraview-tutorial/#3-manipulating-the-color-map","title":"3. Manipulating the Color Map","text":"<p>To change the colors used to represent the Velocity: - Under Color byclick the Edit Color Map... button - On the Color Scale Editor window click the Choose Preset button - On the Preset Color Scales window, select: Blue to Red Rainbow, and click OK. Then click Close on the Color Scale Editor window - You can also create and save your own color maps</p> <p> </p> Manipulating the Color Map"},{"location":"cooley/software-and-libraries/paraview-tutorial/#4-data-representation","title":"4. Data Representation","text":"<p>In order to be able to see the particles and red blood cells inside the cylinder, we need to be able to see through it. If we scroll down a bit in the Object Inspector view: - Group of controls labeled Style - In the Representation dropdown, select Wireframe</p> <p> </p> Data Representation"},{"location":"cooley/software-and-libraries/paraview-tutorial/#5-generate-streamlines","title":"5. Generate Streamlines","text":"<ul> <li>ParaView enables the generation of different types of data from existing data sets in the Pipeline</li> <li>Streamlines: Generated from vectors of the flow field. These curves show the direction a fluid element will travel in at any point in time</li> <li>Make sure that the continuum.000*data is selected in the Pipeline Browser</li> <li>From the main menu select: Filters-&gt;Alphabetical-&gt;Stream Tracer, or click on the Stream Tracer icon from the menu bar</li> <li>In the Object Inspector make sure the Properties tab is selected.</li> <li>Scroll down to seeds, and change Seed Type to Line Source</li> <li>Click the Y Axis button to set the seed line to run along the Y axis.</li> <li>The default Resolution is set to 100. This will make things a bit cluttered, especially when we start adding in the other data, so let's reduce this to 25</li> <li>Click the Apply button</li> </ul> Generate Streamlines"},{"location":"cooley/software-and-libraries/paraview-tutorial/#6-streamlines-as-tubes","title":"6. Streamlines as Tubes","text":"<p>The streamlines are just that, lines. We can use the Tubes filter to represent them as 3D objects, rather than just lines. - With StreamTracer1selected in the Pipeline Browser, from the main menu select: Filters-&gt;Alphabetical-&gt;Tube - In the Object Inspector make sure the Properties tab is selected - The default value for the Radius is a bit too large for this data, let's set that value to 0 - Click the Apply button - Notice that the StreamLine1 object has automatically been hidden - There are many different ways to color these tubes - With Tubes1 selected, switch to the Display tab in the Object Inspector - The Color bydropdown lets you choose from a handful of different variables</p> <p> </p> Streamlines as Tubes"},{"location":"cooley/software-and-libraries/paraview-tutorial/#7-cutting-planes-slices","title":"7. Cutting Planes (Slices)","text":"<p>Now let's add some cutting plans, or slices, to see what the cross-section of the continuum data looks like. - Again, be sure that the <code>continuum.000*data</code> is selected in the Pipeline Browser - Filters-&gt;Alphabetical-&gt;Slice or Click on the Slice icon from the menu bar - In the Object Inspector make sure the Propertiestab is selected - At the bottom on the Object Inspector is a section titled Slice Offset Values. Here we can generate values for multiple slices to be made - First click the Delete All button to remove initial values - Next, click the New Range button. This will bring up an Add Range dialog box. - Set the number of Steps to 7. Click OK - Click the Apply button - With Slice1 selected in the Object Inspector, switch to the Display tab - Set Color by value to Velocity</p> <p> </p> Cutting Planes (Slices)"},{"location":"cooley/software-and-libraries/paraview-tutorial/#8-data-representation-opacity","title":"8. Data Representation: Opacity","text":"<p>Even with the continuum data represented as wireframe, there is still considerable occlusion of the interior structures. In order to further reduce this occlusion by the wireframe, we can make it more transparent.</p> <ul> <li>Again, be sure that the <code>continuum.000*data</code> is selected in the Pipeline Browser</li> <li>In the Object Inspector make sure the Display tab is selected</li> <li>In the Object Inspector there is a section titled Style</li> <li>Set Opacity to 0.2</li> </ul> <p> </p> Data Representation: Opacity"},{"location":"cooley/software-and-libraries/paraview-tutorial/#9-animating-simulation-data","title":"9. Animating Simulation Data","text":"<p>Since our data has multiple time steps, we can easily animate through them to see how the data changes over time.</p> <ul> <li>Simply click the Play button on the animation bar at the top of the GUI</li> <li>Pause to make it stop</li> <li>Loop: With this button toggled on, animation will repeat until stopped</li> </ul> <p> </p> Animating Simulation Data"},{"location":"cooley/software-and-libraries/paraview-tutorial/#10-animations","title":"10. Animations","text":"<p>Animations can be saved to disk as a movie file, to be played back later.</p> <ul> <li>From the main menu: File-&gt;Save Animation</li> <li>Animation Settings Dialog: Save Animation</li> <li>Files of type: AVI files (*.avi)</li> <li>Enter a name in File name:</li> <li>Click OK</li> <li>Movie can be played back with standard media players (Windows Media Player, QuickTime, VLC, etc.)</li> </ul> <p> </p> Animations"},{"location":"cooley/software-and-libraries/paraview-tutorial/#11-particles-as-glyphs","title":"11. Particles as Glyphs","text":"<p>Glyphs are another way of visually representing data where the attributes of a graphical element are dictated by attributes of the data.</p> <p>All of the particles are displayed as red points in the graphics window. There are ~39K particles in this particular data set, which makes the display a bit cluttered. In order to both filter some of these out, and create 3D representations for them, let's apply a glyph filter to this data.</p> <p>Now let's add some of our other data back into the scene. Let's start with the particle data.</p> <p>All of the particles are displayed as red points in the graphics window. There are ~39K particles in this particular data set, which makes the display rather cluttered. In order to both filter some of these out, and create 3D representations for them, we will apply the glyph filter to this data.</p> <p>Note: that the particles.000* is still visible.</p> <ul> <li>Unhide the <code>particles.000*data</code>: click Eye icon</li> <li>Select <code>particles.000*data</code>: click on name</li> <li>Filters-&gt;Alphabetical-&gt;Glyph or click on the Glyph icon from the menu bar</li> <li>Glyph Type: Sphere</li> <li>Radius:. 0.15</li> <li>Orient: Unchecked</li> <li>Scale Mode: off</li> <li>Set Scale Factor: 1 - Edit: Checked</li> <li>Maximum Number of Points: 3000</li> <li>Mask Points: Checked</li> <li>Random Mode: Unchecked</li> <li>Click the Apply button</li> <li>Since our goal was to unclutter the display, let's hide the particles.000*by toggling them off, by clicking on the Eye icon next to it in the Pipeline Browser</li> <li>Let's also switch to the Display tab in the Object Inspector, with Glyph1 selected, and change the Color by value to GlyphVector. Since the GlyphVector value is based on the velocity. We can Edit Color Map...and choose the same Blue to Red Rainbow preset that we previously chose for velocity</li> </ul> <p> </p> Particles as Glyphs"},{"location":"cooley/software-and-libraries/paraview-tutorial/#12-enter-red-blood-cells","title":"12. Enter: Red Blood Cells","text":"<p>Now let's add in both of the other data sets, which are polygonal meshes which make up Red Blood Cells (RBCs).</p> <p>These two data sets are essentially the same kind of data, so we can apply the same filters and make the same types of representation changes to each of them. However, some of the RBCs are marked by the simulation that generated them as healthy (rbc.000) and some of them are marked as diseased (bad_rbc.000).</p> <ul> <li>Unhide the rbc.000 and bad_rbc.000 data sets by clicking the Eye icon next to each of them to make them visible</li> </ul> <p> </p> Enter: Red Blood Cells"},{"location":"cooley/software-and-libraries/paraview-tutorial/#13-using-color-to-differentiate-data","title":"13. Using Color to Differentiate Data","text":"<p>To enable us to distinguish these two types of data from one other, we can vary their representations.</p> <p>One way to do this is by setting the color of the two data sets to different colors. Repeat this process for each of rbc.000 and bad_rbc.000, picking different colors.</p> <ul> <li>Select one of the rbc data sets in the Pipeline Browser</li> <li>Go to the Displaytab in the Object Inspector</li> <li>In the Color by:dropdown select Solid Color</li> <li>Click on the Set Solid Color... button</li> <li>Select a color from the Select Colordialog that appears</li> <li>Repeat for the other RBC data set, choosing a different color</li> </ul> <p> </p> Using Color to Differentiate Data"},{"location":"cooley/software-and-libraries/paraview-tutorial/#14-further-exploration-highlight-the-mesh","title":"14. Further Exploration: Highlight the Mesh","text":"<p>Change the representation of one of the RBC data sets.</p> <p>In this example, the continuum.000* data is also hidden to reduce confusion with showing multiple overlapping meshes.</p> <ul> <li>Select on of the RBC data sets</li> <li>Go to the Displaytab in the Object Inspector</li> <li>For the Representationselect Surface With Edges</li> <li>In the Edge Style section click on the Set Edge Color...button to select a different color from the Select Color dialog</li> </ul> <p> </p> Further Exploration: Highlight the Mesh"},{"location":"cooley/software-and-libraries/paraview-tutorial/#15-further-exploration-highlight-the-vertices","title":"15. Further Exploration: Highlight the Vertices","text":"<p>Add glyphs to illustrate the position of the vertices of one of the RBC data sets.</p> <ul> <li>Select one of the RBC data sets</li> <li>Select the Glyphfilter</li> <li>Since this filter was used recently, can also be found under: Filters-&gt;Recent-&gt;Glyph</li> <li>As in the earlier example, set the various configuration options for the glyph attributes </li> <li>Note: that this time, we want to show all of the vertices of the RBC, so we should uncheckthe Mask Points option</li> </ul> <p> </p> Further Exploration: Highlight the Vertices"},{"location":"cooley/software-and-libraries/paraview-tutorial/#16-further-exploration-color-by-variable","title":"16. Further Exploration: Color by Variable","text":"<p>Try playing around with the viewing options and representations of the other data objects.</p> <p>Change the: - Color by values - Opacity - Representation - Etc.</p> <p> </p> Further Exploration: Color by Variable"},{"location":"cooley/software-and-libraries/paraview-tutorial/#17-background-color","title":"17. Background Color","text":"<ul> <li>Background color is an important part of final visualization</li> <li>From the main menu choose: Edit-&gt;View Settings...</li> <li>Under General in the View Settings dialog box, select Choose Color</li> <li>Select Color: OK</li> <li>Apply, then OK</li> </ul> Background Color <p>This tutorial was developed with support from National Science Foundation Grant OCI-0904190, and from the Argonne Leadership Computing Facility at Argonne National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under contract DE-AC02-06CH11357.</p>"},{"location":"cooley/software-and-libraries/paraview/","title":"Paraview on Cooley","text":"<p>The recommended way of running ParaView on Cooley is in client/server mode. This consists of running the ParaView client on your local resource, and the ParaView server (pvserver) on the Cooley visualization nodes. There are two ways to accomplish this, detailed below. In both cases, the ParaView client needs to first be installed on your local resource, and needs to match the version that you run on Cooley.</p> <p>The most recent production version currently installed on Cooley is ParaView  5.10.0. Binary and source packages for Linux, MacOS, and Windows are available from the ParaView Download Page. (Run the 'softenv' command on a Cooley login node to see earlier versions of ParaView that are available.)</p> <p>As mentioned, there are two ways to run ParaView in client/server mode. For more details, including advantages and disadvantages of each, see the section below on: Trade Offs</p> <p>The first, and arguably easier, way is to run the ParaView client locally, and have it launch the pvserver on Cooley, and connect back to your local client. For details see the section below on: Automated / Reverse Connection</p> <p>The other way is to first manually launch the pvserver on Cooley, and then launch the ParaView client locally and connect to your running pvserver. For details see the section below on: Manual / Forward Connection</p>"},{"location":"cooley/software-and-libraries/paraview/#automated-reverse-connection","title":"Automated / Reverse Connection","text":"<p>This section describes how to launch the pvserver on Cooley from a local ParaView client.</p>"},{"location":"cooley/software-and-libraries/paraview/#start-paraview-client","title":"Start ParaView Client","text":"<p>First, launch the ParaView client on your local resource. In order to launch the pvserver on Cooley and have it connect back to our local client, we will need to configure some server settings in the client. This initial set up should only need to be done once, and can be reused each time you want to run ParaView on Cooley.</p>"},{"location":"cooley/software-and-libraries/paraview/#server-configuration","title":"Server Configuration","text":""},{"location":"cooley/software-and-libraries/paraview/#1-select-connect","title":"1. Select Connect","text":"<p>From the ParaView client choose to connect to a server by either clicking on the \"Connect\" icon in the menu bar, or selecting:</p> <p>File-&gt;Connect</p> <p>From the main menu:</p> <p> </p> Select connect"},{"location":"cooley/software-and-libraries/paraview/#2-fetch-servers-first-time-only","title":"2. Fetch Servers (first time only)","text":"<p>The first time we want to run a pvserver on Cooley and have it connect to our local ParaView client, we need to set up a Server. Once we set up this server, we can reuse it each time we run the ParaView client with the pvserver on Cooley.</p> <p>Kitware, the maintainers of ParaView, maintain a database of server configurations, which we can retrieve through the ParaView client.</p> <p>Click \"Fetch Servers\"</p> <p> </p> Fetch servers"},{"location":"cooley/software-and-libraries/paraview/#3-fetch-server-configuration-cooley","title":"3. Fetch Server Configuration, Cooley","text":"<p>From the list of server configurations, if your local resource is Linux or Mac, select COOLEY@ANL</p> <p>If your local resource is Windows, select  windows to COOLEY@ANL</p> <p>Click \"Import Selected\"</p> <p> </p> Fetch server configuration Cooley"},{"location":"cooley/software-and-libraries/paraview/#4-connect","title":"4. Connect","text":"<p>Now that we have a server defined and configured, highlight it in the list.</p> <p>Click \"Connect\"</p> <p> </p> Connect to Cooley"},{"location":"cooley/software-and-libraries/paraview/#5-configure-server-settings","title":"5. Configure Server Settings","text":"<p>In order to connect to Cooley and submit a job to launch the pvserver, you'll need to edit a few configuration settings.</p> <p>First, ParaView needs a way to securely connect to Cooley, in order to run a qsub command to launch the pvserver. On Linux/Mac it does this through Xterm and SSH.  Be sure that the path to the Xterm executable is set correctly. On Windows be sure that the path to the SSH executable (such as PuTTY) is set correctly.</p> <p>Username: Should be set to your login name on Cooley.</p> <p>ParaView version: Be sure this matches the version of the ParaView client that you are running.</p> <p>Client port and Server port: Can use the default values</p> <p>Set the Number of nodes to reserve and Number of minutes to reserve accordingly. </p> <p>NOTE: You will be submitting a job to the queue on Cooley, and the number of nodes you request are not guaranteed to be available.  If they are not, you may have to wait in the queue until the requested number of nodes become available.</p> <p>Account: set to your project id / allocation on Cooley</p> <p>Click \"OK\"</p> <p> </p> Configure server settings"},{"location":"cooley/software-and-libraries/paraview/#6-connecting-enter-password","title":"6. Connecting: Enter Password","text":"<p>A window will pop up, indicating that ParaView is connecting to Cooley, and is waiting for the server to connect back to your client.  </p> <p>A second window will pop up (Xterm on Linux/Mac.  PuTTY, or other local SSH client, on Windows).  Enter your PIN and secure token one time password, just as you would when logging into Cooley.</p> <p>This will enable ParaView to submit your job to run the pvserver on Cooley.  Once your job starts, and the pvserver connects back to your ParaView client, the Waiting for Server Connection window will go away. </p> <p> </p> Establishing connection to Cooley <p> </p> Select connect"},{"location":"cooley/software-and-libraries/paraview/#2-add-or-edit-server","title":"2. Add (or Edit) Server","text":"<p>The first time we connect our local ParaView client to a pvserver on Cooley, we need to Add Server. Once we set up this server, we can reuse it each time we connect the ParaView client to the pvserver on Cooley. We may need to Edit Server in the future if our pvserver ends up on a different host.</p> <p>Click \"Add Server\" (first time) or \"Edit Server\" (subsequent times)</p> <p> </p> Add (or edit) server"},{"location":"cooley/software-and-libraries/paraview/#3-configure-server-part-1","title":"3. Configure Server, Part 1","text":"<p>Configure the server by first giving it a Name, such as Cooley</p> <p>Select Server Type: Client/Server</p> <p>The Host value should be set to the full name of the head node of our job where the pvserver is listening: cc018.cooley.pub.alcf.anl.gov</p> <p>And the port should be set to the value we used when we started the pvserver: 8000.</p> <p>Click \"Configure\"</p> <p> </p> Configure server, part 1"},{"location":"cooley/software-and-libraries/paraview/#4-configure-server-part-2","title":"4. Configure Server, Part 2","text":"<p>Because we are going to connect to a ParaView server that we have already started, we don't need the ParaView client to start a server for us.</p> <p>Select Startup Type: Manual</p> <p>Click \"Save\"</p> <p> </p> Configure server, part 2"},{"location":"cooley/software-and-libraries/paraview/#5-connect","title":"5. Connect","text":"<p>Now that we have a server defined and configured, highlight it in the list.</p> <p>Click \"Connect\"</p> <p> </p> Connect to Cooley"},{"location":"cooley/software-and-libraries/paraview/#6-open-file","title":"6. Open File","text":"<p>Now when you select File-&gt;Open from the main menu, you will be browsing the filesystem on Cooley. You're ready to go.</p>"},{"location":"cooley/software-and-libraries/singularity-cooley/","title":"Singularity on Cooley","text":""},{"location":"cooley/software-and-libraries/singularity-cooley/#singularity-on-alcf-resources","title":"Singularity on ALCF Resources","text":"<p>Singularity is a software container solution to enable fine grained control over application environments on a diverse set of hardware systems.  For details on singularity, please see the Singularity documentation.</p>"},{"location":"cooley/software-and-libraries/singularity-cooley/#singularity-on-cooley_1","title":"Singularity on Cooley","text":"<p>This page is specific to setting up and running singularity on Cooley.  If you are interested in running singularity on theta, we have detailed Theta specific documentation for singularity.</p>"},{"location":"cooley/software-and-libraries/singularity-cooley/#why-use-singularity-on-cooley","title":"Why use Singularity on Cooley?","text":"<p>Singularity provides several advantages on Cooley.  First, singularity provides more control over the software environment you run your code in, meaning you can have fine grained control over packages like tensorflow, pytorch, or other code designed to run on GPUs.  While Cooley is primarily a visualization cluster, it can still be useful to be able to apply it's GPUs to datascience and machine learning challenges.</p> <p>Second, singularity allows you to \"bootstrap\" images off of other singularity or docker images.  In particular, this means you can leverage an image built by nvidia with cuda, cudnn, without having to worry about the installation of that software yourself.  Singularity can save you time spent managing your runtime environment and let you focus on your application development.</p>"},{"location":"cooley/software-and-libraries/singularity-cooley/#how-to-use-singularity-on-cooley","title":"How to use Singularity on Cooley","text":"<p>In general, using a singularity container is a simple process.  You can execute a script inside of a container with a command such as: <pre><code>singularity exec /path/to/singularity_container.img command_to_execute\n</code></pre> If you want to run an interactive container, you can use bash as your executable: <pre><code>singularity exec /path/to/singularity_container.img bash\n</code></pre></p>"},{"location":"cooley/software-and-libraries/singularity-cooley/#using-nvidia-drivers-inside-of-singularity","title":"Using nvidia drivers inside of Singularity","text":"<p>Singularity allows you to use the nvidia device drivers on the host system within the container by passing the --nv option.  On the cooley compute nodes, this allows you to run tensorflow/pytorch on the GPUs inside of the container transparently, with no special set up or modification to your scrips.</p>"},{"location":"cooley/software-and-libraries/singularity-cooley/#other-important-runtime-flags","title":"Other Important Runtime Flags","text":"<p>For a full list of available runtime flags, you can look at the output of \"singularity run help\".  An important flag to know is to bind mount points into the container.  By default, most mount points will not be available in the container, but if you want to use the /lus system from within your container you can have singularity make it available at runtime like this: <pre><code>singularity exec -B /lus:/lus /path/to/singularity_container.img command_to_execute\n</code></pre></p>"},{"location":"cooley/software-and-libraries/singularity-cooley/#using-singularity-with-mpi","title":"Using Singularity with MPI","text":"<p>Singularity has builtin support to use MPI and run MPI applications, as long as MPI is built into your container (more on how to do that below).  You can see details of the design on the singularity docs for HPC.</p> <p>In general, singularity must have MPI installed within the container.  When you execute an MPI program with singularity, each rank of your program runs singularity: <pre><code>mpirun -n ${NPROC} singulari\u200bty exec /path/to/singularity_container.img command_to_execute [executable args]\n</code></pre> If you want to run 2 ranks per node on Cooley (one for each of the two GPUs on each node) you can pass the list of hosts to mpirun with the -H command: <pre><code>mpirun -n ${NPROC} -H $COBALT_NODEFILE singularity exec /path/to/singularity_container.img command_to_execute [executable args]\n</code></pre></p>"},{"location":"cooley/software-and-libraries/singularity-cooley/#special-considerations-for-running-mpi-and-singularity-with-gpus-on-cooley","title":"Special Considerations for running MPI and Singularity with GPUs on Cooley","text":"<p>Executing MPI code on Cooley nodes can sometimes have extra details due to the communication that happens across GPUs if you use NCCL2.  If your code is hanging unexpectedly when using both GPUs per node, you can try disabling peer to peer communication in NCCL with: <pre><code>export NCCL_P2P_DISABLE=1\n</code></pre> within the container execution (or pass it into the container with a singularity environment variable).</p> <p>Additionally, running with both GPUs on a node can cause a crash with CUDA code if there are X processes running.  You can use the nox11 queue, or pass the attribute nox11 to any cobalt job to turn off X processes for your job.</p>"},{"location":"cooley/software-and-libraries/singularity-cooley/#building-a-singularity-container-with-tensorflow-or-pytorch-for-gpu","title":"Building a Singularity Container with TensorFlow or PyTorch for GPU","text":"<p>Building a container for singularity requires either root priveleges on a machine with singularity installed, or the use of singularity hub to build your images.  This guide won't go in to the details of how to build your singularity image, but rather contain recipes and snippets that are useful for builds targeted towards Cooley using common packages.</p>"},{"location":"cooley/software-and-libraries/singularity-cooley/#bootstrapping-a-basic-image","title":"Bootstrapping a Basic Image","text":"<p>The basic details of your singularity recipe could follow a recipe that looks like this: <pre><code>Bootstrap: docker\nFrom: nvidia/cuda:9.0-cudnn7-devel-centos7\n\n%help\nCentos7 with cuda9.0 cudnn7\n\nTo start your container simply try\nsingularity exec THIS_CONTAINER.simg bash\n\nTo use GPUs, try\nsingularity exec --nv THIS_CONTAINER.simg bash\n\n%environment\n\n    # for system\n    export CUDA_DEVICE_ORDER=PCI_BUS_ID\n\n    # Add cupti to the path for profiling:\n    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\n\n    source scl_source enable devtoolset-4\n\n%post\n\n    # yum basics\n    yum update -y\n    yum groupinstall -y \"Development Tools\"\n    yum install -y epel-release\n    yum install -y centos-release-scl\n    yum install -y devtoolset-4\n    yum install -y wget emacs vim\n    yum install -y emacs vim openssh-clients zip\n    yum install -y python-devel python-pip python-setuptools\n    yum install -y hdf5\n\n    # pip basics\n    pip --no-cache-dir --disable-pip-version-check install --upgrade setuptools\n    pip --no-cache-dir --disable-pip-version-check install future\n    pip --no-cache-dir --disable-pip-version-check install 'matplotlib&lt;3.0' # for python2.7\n    pip --no-cache-dir --disable-pip-version-check install 'ipython&lt;6.0'    # for python2.7\n    pip --no-cache-dir --disable-pip-version-check install 'ipykernel&lt;5.0'  # for python2.7\n    pip --no-cache-dir --disable-pip-version-check install numpy wheel zmq six pygments pyyaml cython gputil psutil humanize h5py tqdm scipy seaborn tables\n    pip --no-cache-dir --disable-pip-version-check install  pandas scikit-image scikit-learn Pillow opencv-python\n    pip --no-cache-dir --disable-pip-version-check install jupyter notebook\n</code></pre></p> <p>To add a package like tensorflow, or pytorch, you can install them with pip at the end of the post section as normal.  While building from source is not discouraged, it is much easier to install tensorflow/pytorch using pip:</p> <pre><code># tensorflow\npip --no-cache-dir --disable-pip-version-check install --upgrade tensorflow-gpu==1.12.0\npip --no-cache-dir --disable-pip-version-check install tensorboard\n\n# keras\npip --no-cache-dir --disable-pip-version-check install keras\n</code></pre> <p>The recipe above has a couple of details worth pointing out.  First, note the \"Bootstrap\" lines at the very beginning.  This indicates to singularity that this recipe is going to take the centos7 cuda enable docker image from nvidia, and build a singularity container on top of that.  Those two lines get cuda, cudnn, and save a lot of work.</p> <p>The %help section is for informational purposes only, you can put whatever you like here to help keep organized.</p> <p>The %environment section runs every time this container activates.  It functions somewhat like a login script in that commands there will be executed prior to whatever command has been passed to the container.  In this recipe, it adds the nvidia profiling tools to the path, and enables centos libraries installed to get gcc 5+.</p> <p>The %post section runs only during the image build, after the basic set up completes.  It is where you can control and install software (with sudo permission for inside the container).  In this recipe, there are a lot of general purpose tools installed for development purposes, many of which may be unnecessary for your application and you can (and should) prune to what you want if you use this recipe.  Additionally, there are python tools installed with pip that may be useful and again you should prune as needed.</p>"},{"location":"cooley/software-and-libraries/singularity-cooley/#building-mpi-and-common-mpi-tools-for-your-container","title":"Building MPI and common MPI tools for your container","text":"<p>MPI needs to be installed inside the container for MPI applications to run within singularity.   To do this, add the following snippet to the %post section (this is nearly identical to the Theta singularity instructions): <pre><code># install MPICH\nwget -q http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gz\ntar xf mpich-3.2.1.tar.gz\nrm mpich-3.2.1.tar.gz\ncd mpich-3.2.1\n# disable the addition of the RPATH to compiled executables\n# this allows us to override the MPI libraries to use those\n# found via LD_LIBRARY_PATH\n./configure --prefix=/usr/local/mpich/install --disable-wrapper-rpath\nmake -j 4 install\n# add to local environment to build pi.c\nexport PATH=$PATH:/usr/local/mpich//install/bin\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/mpich//install/lib\nenv | sort\ncd ..\nrm -rf mpich-3.2.1\n</code></pre></p> <p>Additionally, the following lines should be added to the %environment section:    # for MPICH:</p> <pre><code>export PATH=/usr/local/mpich/install/bin/:${PATH}\nexport LD_LIBRARY_PATH=/usr/local/mpich/install/lib/:${LD_LIBRARY_PATH}\n</code></pre> <p>If you want to use Nvidia's Collective Communications Library (NCCL2), you can install it:</p> <pre><code># nccl2\ngit clone https://github.com/NVIDIA/nccl.git\ncd nccl;\nmake -j src.build\nmake pkg.redhat.build\nrpm -i build/pkg/rpm/x86_64/libnccl* \ncd -\n</code></pre> <p>mpi4py has worked in containers on Cooley from the pip installation: </p> <pre><code>pip --no-cache-dir --disable-pip-version-check install mpi4py\n</code></pre> <p>Lastly, if you want to use horovod for distributed training, the following installation technique has been shown to work with tensorflow (but not pytorch): <pre><code>ldconfig /usr/local/cuda/lib64/stubs\n# install Horovod, add other HOROVOD_* environment variables as necessary\nHOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_WITH_TENSORFLOW=1 HOROVOD_NCCL_HOME=/nccl/build/ pip install --no-cache-dir horovod\n\n# revert to standard libraries\nldconfig\n</code></pre></p> <p>For pytorch, this technique causes segmentation faults at runtime.  Instead, build your container without horovod.  Then, on an interactive node with --nv, and with torch installed into your container, build horovod with the same pip command from inside an interactive shell in your container.  Install it to a location in your area (--user if you like, or --prefix for more control) and add that area to your python path in your run scripts.  While this is a workaround, it works to enable horovod inside of your container for distributed learning.</p>"},{"location":"cooley/software-and-libraries/visit-on-cooley/","title":"Visit on Cooley","text":""},{"location":"cooley/software-and-libraries/visit-on-cooley/#getting-started","title":"Getting Started","text":"<p>On your local machine:</p> <ul> <li>Download (https://wci.llnl.gov/simulation/computer-codes/visit/downloads) and install VisIt (The most recent version installed on Cooley is 2.12.3. Versions 2.10.2 and  2.9.1 are also available.)</li> <li>Download the Cooley host profile for VisIt (you may need to right-click and choose \"Save link as...\" or \"Save target as...\")</li> <li>Copy this file to a file called ~/.visit/hosts/host_anl_cooley.xml </li> </ul>"},{"location":"cooley/software-and-libraries/visit-on-cooley/#running-visit-in-interactive-mode","title":"Running VisIt in Interactive Mode","text":"<ul> <li>Start up VisIt on your local machine (Version 2.12.3, 2.10.2, or 2.9.1)</li> <li>Click File -&gt; Open File and choose \"ANL Cooley\" from the \"Host\" dropdown</li> <li>You'll be prompted for your password; enter your Cryptocard response (with PIN)</li> <li> <p>When you open a selected file, it will launch a job on Cooley</p> <ul> <li>You will need to specify the \"Bank\" (Project) to use when VisIt submits jobs to the queue on Cooley. Specify a project in the Options box.</li> <li>If your environment doesn't get sourced correctly with non-interactive SSH, you can set the default project to use under Options -&gt; Host profiles</li> <li>Note: Don't change the contents of the \"Machine file\" field (it should be $COBALT_NODEFILE)</li> <li>Note: The default Launch Profile is set to serial.  Do not change this default setting.</li> <li>If you'd like to change other job parameters (like the number of processes, nodes, and walltime), you can do so.</li> <li>If you'd like these changes to be used as your default, be sure to save them using Save Settings under the Options menu.</li> </ul> </li> </ul>"},{"location":"cooley/software-and-libraries/visit-on-cooley/#running-visit-in-batch-mode","title":"Running VisIt in Batch Mode","text":"<ul> <li>Edit your .soft.cooley file to include the \"@visit\" key before the \"@default\" line </li> <li>When running VisIt in batch mode on Cooley, the default version is 2.12.3   If you need a different version, you can specify this as a command line option using \"-v \" (versions 2.10.2 and 2.9.1 are also supported)"},{"location":"cooley/software-and-libraries/visit-on-cooley/#additional-information","title":"Additional Information","text":"<ul> <li>Visit user manual: https://wci.llnl.gov/codes/visit/manuals.html</li> <li>Visit wiki: http://www.visitusers.org</li> </ul>"},{"location":"data-management/acdc/acdc-overview/","title":"ALCF Community Data Co-Op (ACDC)","text":""},{"location":"data-management/acdc/acdc-overview/#overview-of-the-alcf-community-data-co-op-acdc","title":"Overview of the ALCF Community Data Co-Op (ACDC)","text":"<p>The ALCF Community Data Co-Op (ACDC) powers data-driven research by providing a platform for data access and sharing, and value-added services for data discovery and analysis.</p> <p>A fundamental aspect of ACDC is a data fabric that allows programmatic data access, and straightforward large-scale data sharing with collaborators via Globus services.This provides a platform to build out different modalities for data access and use, such as indexing of data for discovery, data portals for interactive search and access, and accessible analysis services. ACDC will continue to be expanded to deliver ALCF users the platform to build customizable and accessible services towards the goal of supporting data-driven discoveries.</p>"},{"location":"data-management/acdc/acdc-overview/#data-access-and-sharing","title":"Data access and sharing","text":"<p>ALCF project PIs can share data on Eagle with their collaborators, making facility accounts unnecessary. With this service, the friction of data sharing amongst collaborators is eliminated \u2013 there is no need to create copies of data for sharing, or allocation and accounts just to access data. ALCF PIs can grant access to data, at read-only or read/write access levels. Non-ALCF users throughout the scientific community, who have been granted permissions, can access the data on Eagle filesystem using Globus.</p> <p>Access to the data for ALCF users and collaborators is supported via bulk transfer (Globus transfer) or direct browser-based access (HTTP/S). Direct connections to high-speed external networks permit data access at many gigabytes per second. Management of permissions and access is via a web application or command line clients, or directly via an Application Programming Interface (APIs). The interactivity permitted by the APIs distinguishes ACDC from the ALCF\u2019s previous storage systems and presents users with many possibilities for data control and distribution.</p>"},{"location":"data-management/acdc/acdc-overview/#data-portal-for-discovery-and-access","title":"Data portal for discovery and access","text":"<p>ACDC\u2019s fully supported production environment is the next step in the expansion of edge services that blur the boundaries between experimental laboratories and computing facilities. The use and prominence of such services at the ALCF are only expected to increase as they become more integral to the facility\u2019s ability to deliver data-driven scientific discoveries.</p> <p>ACDC includes several project-specific data portals that enable search and discovery of the data hosted on Eagle. The portals allow users to craft queries and filters to find specific sets of data that match their criteria and use faceted search for the discovery of data. Portals also provide the framework for other interfaces including data processing capabilities, all secured with authentication and configured authorization policy.</p> <p>The ACDC portal is a deployment of Django Globus Portal Framework customized for a variety of different projects For most of these projects, the search metadata links directly to data on Eagle, with browser-based download, preview, and rendering of files, and bulk data access.</p>"},{"location":"data-management/acdc/acdc-overview/#getting-started","title":"Getting Started","text":"<ol> <li>Request an allocation: Researchers or PIs request an allocation on Eagle, and a project allocation is created upon request acceptance.</li> <li>Manage Access: PIs can manage the space independently or assign other users to manage the space, as well as provide other users with read or read/write access for folders in the space. Globus groups and identities are used to manage such access.</li> <li>Authentication: Globus is used for authentication and identity needed to access the system. As Globus has built-in support for federated logins, users can access ACDC using their campus or institution federated username and passcode</li> </ol> <p>If you are new to the ALCF, follow these instructions on how to transfer your data to ACDC: Transferring Data to Eagle</p> <p>If you already have an ALCF account, follow these instructions on how to share your data: Sharing Data to Eagle</p>"},{"location":"data-management/acdc/eagle-data-sharing/","title":"Sharing on Eagle Using Globus","text":""},{"location":"data-management/acdc/eagle-data-sharing/#overview","title":"Overview","text":"<p>Collaborators throughout the scientific community have the ability to write data to and read scientific data from the Eagle filesystem using Globus sharing capability. This capability provides PIs with a natural and convenient storage space for collaborative work.</p> <p>Note: The project PI needs to have an active ALCF account to set up Globus guest collections on Eagle, and set permissions for collaborators to access data. </p> <p>Globus is a service that  provides research data management, including managed transfer and sharing. It makes it easy to move, sync, and share large amounts of data. Globus will manage file transfers, monitor performance, retry failures, recover from faults automatically when possible, and report the status of your data transfer. Globus supports GridFTP for bulk and high-performance file transfer, and direct HTTPS for download. The service allows the user to submit a data transfer request, and performs the transfer asynchronously in the background. For more information, see Globus data transfer and Globus data sharing.</p> <p> </p> Logging into Globus"},{"location":"data-management/acdc/eagle-data-sharing/#logging-into-globus-with-your-alcf-login","title":"Logging into Globus with your ALCF Login","text":"<p>ALCF researchers can use their ALCF Login username and password to access Globus. Go to the Globus website and click on Log In in the upper right corner of the page.</p> <p>Type or scroll down to \"Argonne LCF\" in the \"Use your existing organizational login\" box, and then click \"Continue\".</p> <p> </p> Select Organization Argonne LCF <p>You will be taken to a familiar-looking page for ALCF login. Enter your ALCF login username and password.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#accessing-your-eagle-project-directory","title":"Accessing your Eagle Project Directory","text":"<p>There are two ways for a PI to access their project directory on Eagle. </p> <ol> <li>Web Interface: By logging in to Globus interface directly and navigating to the ALCF Eagle endpoint.</li> </ol> <p>Note: Specifically for PIs with Eagle 'Data-only' projects and no other compute allocations, logging in from the Globus-side to get to Eagle is the only way for them to access their Eagle project directory. </p> <p> </p> File Manager <ol> <li>POSIX: By logging in to the ALCF systems from the terminal window.</li> </ol> <p>Note: For Eagle Data and Allocation projects, the PI will have access to the required ALCF systems (besides the Globus Web Interface) to login and access their Eagle project directory. </p> <p> </p> Terminal Window"},{"location":"data-management/acdc/eagle-data-sharing/#creating-a-guest-collection","title":"Creating a Guest Collection","text":"<p>A project PI needs to have an 'active' ALCF account in place to create and share guest collections with collaborators. Please note that ONLY a PI has the ability to create guest collections. </p> <ul> <li> <p>If you have an \"Inactive/Deleted\" ALCF account, please click on the account re-activation link to begin the re-activation process: Re-activation Link</p> </li> <li> <p>If you DO NOT have an ALCF account, click on the account request link to begin the application process: Account Request Link </p> </li> </ul> <p>In the Globus application in your browser:</p> <ol> <li>There are multiple ways to Navigate to the Collections tab in \"Endpoints\":<ol> <li>Click the link to get started. It will take you to the Collections tab for Eagle. OR</li> <li>Click on 'Endpoints' located in the left panel of the Globus web app. Type \"alcf#dtn_eagle\" in the search box located at the top of the page and click the magnifying glass to search. Click on the Managed Public Endpoint \"alcf#dtn_eagle\" from the search results. Click on the  Collections tab. OR</li> <li>Click on 'File Manager' located in the left panel of the Globus web app. Search for 'alcf#dtn_Eagle' and select it in the Collection field. Select your project directory or a sub directory that you would like to share with collaborators as a Globus guest collection. Click on 'Share' on the right side of the panel, which will take you to the Collections tab.</li> </ol> </li> </ol> <p>Note: Shared endpoints always remain active. When you select an endpoint to transfer data to/from, you may be asked to authenticate  with  that endpoint. Follow the instructions on screen to activate the endpoint and to authenticate. You may also have to provide Authentication/Consent for the Globus web app to manage collections on this endpoint</p> <ol> <li> <p>In the Collections tab, click 'Add a Guest Collection' located at the top right hand corner</p> </li> <li> <p>Fill out the form:</p> <ol> <li> <p>If the path to the directory is not pre-populated, click the browse button, navigate and select the directory.  Note that you can create a single guest collection and set permissions for folders within a guest collection. There is no reason to create multiple guest collections to share for a single project.</p> </li> <li> <p>Give the collection a Display Name (choose a descriptive name)</p> </li> </ol> </li> <li> <p>Click \"Create Collection\"</p> </li> </ol> <p> </p> Create New Guest Collection"},{"location":"data-management/acdc/eagle-data-sharing/#sharing-data-with-collaborators-using-guest-collections","title":"Sharing Data with Collaborators Using Guest Collections","text":"<p>If your data is on the ALCF systems, you can easily share it with collaborators who are at ALCF or elsewhere. You have full control over which files your collaborator can access, and whether they have read-only or read-write permissions. </p> <p>You can share with their institutional email. The collaborator can use the Globus web interface to download the data, or use Globus transfer to move the data to their machine.</p> <p>To share data with collaborators (that either have a Globus account or an ALCF account), click on 'Endpoints', select your newly created Guest Collection (as described in the section above), and go to the 'Permissions' tab. Click on 'Add Permissions - Share With':</p> <p> </p> Add Permissions <p>You can share with other Globus users or Globus Groups (for more information on Groups, scroll down to Groups). You can give the collaborators read, write or read+write permissions. Once the options have been selected, click 'Add Permission'.</p> <p> </p> Add Permissions - Share With <p>PI can also choose to share their data with 'Public' with anonymous read access (and anonymous write disabled).  This allows anyone that has access to the data read and/or download it without authorizing the request.</p> <p> </p> Add Permissions - Share With <p>You should then see the share and the people you have shared it with. You can repeat this process for any number of collaborators. At any time, you can terminate access to the directory by clicking the trash can next to the user.</p> <p> </p> List of people that you have shared with"},{"location":"data-management/acdc/eagle-data-sharing/#additional-information-on-globus-guest-collections","title":"Additional information on Globus Guest Collections","text":"<ul> <li> <p>ONLY you (a project PI) can create guest collections and make them accessible to collaborators. Project Proxy (on the POSIX side) cannot create guest collections. </p> </li> <li> <p>You can only share directories, not individual files.</p> </li> <li> <p>Globus allows directory trees to be shared as either read or read/write. This means that any subdirectories within that tree also have the same permissions. Globus supports setting permissions at a folder level, so there is no need to create multiple guest collections for a project. You can create a guest collection at the top level and share sub-directories with the collaborators by assigning the appropriate permissions.</p> </li> <li> <p>When you create a guest collection endpoint and give access to one or more Globus users, you can select whether each person has read or read/write access. If they have write access, they can also delete files within that directory tree, so you should be careful about providing write access.</p> </li> <li> <p>Globus guest collections are created and managed by project PIs. If the PI of a project changes, the new PI will have to create a new guest collection and share them with the users. Globus guest collections' ownership cannot be transferred.</p> </li> <li> <p>Guest collections are active as long as the project directory is available and the PI's ALCF account is active. If the account goes inactive, the collections become inaccessible to all the users. Access is restored once the PI's account is reactivated.</p> </li> <li> <p>All RW actions are performed as the PI, when using Guest Collections. If a PI does not have permissions to read or write a file or a directory, then the Globus guest collection users won't either.</p> </li> </ul>"},{"location":"data-management/acdc/eagle-data-sharing/#creating-a-group","title":"Creating a group","text":"<ol> <li>Go to Groups on the left panel</li> <li>Click on \u2018Create a new group\u2019 at the top</li> <li>Give the group a descriptive name and add Description for more information</li> <li>Make sure you select \u2018group members only\u2019 radio button</li> <li>Click on \u2018Create Group\u2019</li> </ol> Create new group"},{"location":"data-management/acdc/eagle-data-sharing/#transferring-data-from-eagle","title":"Transferring data from Eagle","text":"<p>Log in to Globus using your ALCF credentials. After authenticating, you will be taken to the Globus File Manager tab. In the 'Collection' box, type the name of Eagle managed endpoint (alcf#dtn_eagle). Navigate to the folder/file you want to transfer. HTTPS access (read-only) is enabled so you can download files by clicking the \"Download\" button.</p> <p>Click on 'Download' to download the required file. </p> <p> </p> Download the required file <p>To transfer files to another Globus endpoint, in the \"collection\" search box in the RHS panel, enter the destination endpoint (which could also be your Globus Connect Personal endpoint). </p> <p> </p> Transferring files to another Globus endpoint <p>To transfer files, select a file or directory on one endpoint, and click the blue 'Start' button.</p> <p> </p> Transferring files <p>If the transfer is successful, you should see the following message:</p> <p> </p> A Successful Transfer <p>Click on 'View details' to display task detail information.</p> <p> </p> Transfer completed <p>You will also receive an email when the transfer is complete.</p> <p> </p> Email confirmation"},{"location":"data-management/acdc/eagle-data-sharing/#deleting-a-guest-collection","title":"Deleting a guest collection","text":"<p>To see all guest collections you have shared, go to 'Endpoints' in the left hand navigation bar, then 'Administered by You'. Select the guest collection endpoint you wish to delete, and click on 'Delete endpoint'.</p> <p> </p> Deleting a guest collection"},{"location":"data-management/acdc/eagle-data-sharing/#what-to-tell-your-collaborators","title":"What to tell your Collaborators","text":"<p>If you set up a shared endpoint and want your collaborator to download the data, this is what you need to tell them.</p> <p>First, the collaborator needs to get a Globus account. The instructions for setting up a Globus account are as described above. This account is free. They may already have Globus access via their institution.</p> <p>If the collaborator is downloading the data to his/her personal workstation, they need to install the Globus Connect client. Globus connect clients are available for Mac, Windows or Linux systems and are free.</p> <p>If you clicked on the 'notify users via email' button when you added access for this user, they should have received a message that looks like this:</p> <p> </p> Click on the 'notify users via email' button for collaborators to receive an email <p>You can, of course, also send email to your collaborators yourself, telling them you've shared a folder with them. The collaborator should click on the link, which will require logging in with their institutional or Globus login username and password. They should then be able to see the files you shared with them. External collaborator's view of the shared collection is shown below: </p> <p> </p> Collaborator transfer or sync to <p>They should click on the files they want to transfer, then 'Transfer or Sync to', enter their own endpoint name and desired path and click the 'Start' button near the bottom to start the transfer.</p> <p> </p> Chossing transfer path"},{"location":"data-management/acdc/eagle-data-sharing/#encryption-and-security","title":"Encryption and Security","text":"<p>Data can be encrypted during Globus file transfers. In some cases encryption cannot be supported by an endpoint, and Globus Online will signal an error.</p> <p>For more information, see How does Globus Online ensure my data is secure?</p> <p>In the Transfer Files window, click on 'More options' at the bottom of the 2 panes. Check the 'encrypt transfer' checkbox in the options.</p> <p> </p> Encrypting the transfer <p>Alternatively, you can encrypt the files before transfer using any method on your local system, then transfer them using Globus, then unencrypt on the other end.</p> <p>Note: Encryption and verification will slow down the data transfer.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#faqs","title":"FAQs","text":""},{"location":"data-management/acdc/eagle-data-sharing/#general-faqs","title":"General FAQs:","text":"<p>1. What is the Eagle File system?</p> <p>It is a Lustre file system residing on an HPE ClusterStor E1000 platform equipped with 100 Petabytes of usable capacity across 8480 disk drives. This ClusterStor platform also provides 160 Object Storage Targets and 40 Metadata Targets with an aggregate data transfer rate of 650GB/s. Primary use of Eagle is data sharing with the research community using Globus.The file system is available on all ALCF compute systems. It allows sharing of data between users (ALCF and external collaborators).</p> <p>2. What is the difference between Guest, Shared and a Mapped collection? </p> <ul> <li>Guest collections: A Guest collection is a logical construct that a PI sets up on their project directory in Globus that makes it accessible to collaborators. The PI creates a guest collection at or below their project and shares it with the Globus account holders.</li> <li>Shared collection: A guest collection becomes a shared collection when it is shared with a user/group.</li> <li>Mapped Collections: Mapped Collections are created by the endpoint administrators. In the case of Eagle, these are created by ALCF.</li> </ul> <p>3. Who can create Guest collections?</p> <p>ONLY a project PI (or project owner) can create guest collections and make them accessible to collaborators. </p> <p>Project Proxy (on the POSIX side) or Access Manager (on the Globus side) do not have the ability to create guest collections. </p> <p>4. Who is an Access Manager? </p> <p>Access Manager is someone who can act as a Proxy on behalf of the PI to manage the collection. The Access Manager has the ability to add users, remove users, grant or revoke read/write access privileges for those users on that particular guest collection. However, Access Managers DO NOT have permissions to create guest collections. </p> <p>5. What are Groups? </p> <p>Groups are constructs that enable multi-user data collaboration. A PI (and an Access Manager) can create new groups, add members to them and share a guest collection with a group of collaborators. </p> <p>Note Members of groups do not need to have an ALCF account.</p> <p>6. What are some of the Common Errors you see and what do they mean?</p> <pre><code>- EndpointNotFound   -  Wrong endpoint name \n- PermissionDenied    -  If you do not have permissions to view or modify the collection on &lt;endpoint&gt; (refer to the appropriate section for what this error could mean)\n- ServiceUnavailable  -  If the service is down for maintenance\n</code></pre>"},{"location":"data-management/acdc/eagle-data-sharing/#pi-faqs","title":"PI FAQs:","text":"<p>1. How can a PI request for a data-only, Eagle storage allocation? </p> <p>A project PI can request an allocation by filling out the Director\u2019s Discretionary Allocation Request form: Request an allocation. The allocations committee reviews the proposals and provides its decision in 1-2 weeks. To request a storage allocation on Eagle for an existing project, please email support@alcf.anl.gov with your proposal.</p> <p>2. Does a PI need to have an ALCF account to create a Globus guest collection?</p> <p>Yes. The PI needs to have an 'active' ALCF account in place to create and share guest collections with collaborators. </p> <ul> <li>If the PI has an 'Inactive/Deleted' ALCF account, they should click on the link here to start the account re-activation process: Account re-activation link</li> <li>If they don't have an ALCF account, they request for one: Account request link </li> </ul> <p>3. What endpoint should the PI use?</p> <p><code>alcf#dtn_eagle</code></p> <p>4. What are the actions an Eagle PI can perform?</p> <ul> <li>Create and delete guest collections, groups</li> <li>Create, delete and share the data with ALCF users and external collaborators</li> <li>Specify someone as a Proxy (Access Manager) for the guest collections</li> <li>Transfer data between the guest collection on Eagle and other Globus endpoints/collections</li> </ul> <p>5. How can a PI specify someone as a Proxy on the Globus side?</p> <p>Go to alcf#dtn_eagle -&gt; collections -&gt; shared collection -&gt; roles -&gt; select 'Access Manager'</p> <p> </p> To specify someone as a Proxy, click on \"Roles\" <p> </p> Choose Access Manager and \"Add Role\" <p>6. What is the high-level workflow for setting up a guest collection? </p> <ul> <li>PI requests an Eagle allocation project</li> <li>The ALCF Allocations Committee reviews and approves requests</li> <li>ALCF staff sets up a project, unixgroup, and project directory (on Eagle) </li> <li>A Globus sharing policy is created for the project with appropriate access controls</li> <li> <p>PI creates a guest collection for the project, using the Globus mapped collection for Eagle.</p> <ul> <li>Note: PI needs to have an active ALCF Account and will need to log in to Globus using their ALCF credentials.</li> <li>If PI has a Globus account, it needs to be linked to their ALCF account</li> </ul> </li> <li> <p>PI adds collaborators to the guest collection. Collaborators can be ALCF users and external collaborators</p> </li> <li>Added with Read only or Read-Write permissions</li> </ul> <p>7. Should PI add their ALCF project members to Eagle separately to access guest collections?</p> <p>ALCF project members already have access to the project directory that they can access by browsing the endpoint <code>alcf#dtn_eagle</code>. Globus guest collections allows sharing of data with collaborators that don't have ALCF accounts.  </p> <p>8. Who has the permissions to create a guest collection?</p> <p>Only the PI has the ability to create a guest collection. The Access Manager, along with the PI, has permissions to share it with collaborators (R-only or R-W permissions as needed). </p> <p>9. I am the project PI. Why do I see a \"Permission Denied\" error when I try to CREATE a shared collection?</p> <p>If you are a PI and you see this error, it could mean that a sharing policy might not have been created by ALCF. Please contact support@alcf.anl.gov for assistance.</p> <p>10. If a PI added someone as a project proxy on the POSIX-side, is it safe to assume that the Proxy can create guest collections?</p> <p>No, project proxies cannot create guest collections, only the PI can.</p> <p>11. Who can create groups? A PI (and an Access Manager) can create new groups, add members to them and share a guest collection with a group of collaborators. For more information, refer to: Creating a Group</p> <p>12. What happens when the PI of a project changes? What happens to the shared collection endpoint?</p> <p>The new PI will need to create new shared collections and share it with collaborators again.</p> <p>13. I notice that I am the owner of all the files that were transferred by external collaborators using the guest collection. Why is that?</p> <p>When collaborators read files from or write files to the guest collection, they do so on behalf of the PI. All writes show up as having been carried by the PI. Also, if the PI does not have permission to read or write to a file or folder in the directory, then the collaborators will not have those permissions either. </p> <p>14. What happens to the guest collections when the PI's account goes inactive?</p> <p>The collections will also become inactive until the PI's account is re-activated. </p> <p>15. How long does it take for the endpoint to become accessible to collaborators after PI's account is activated?</p> <p>Right away. The page needs to be refreshed and sometimes you may have to log out and log back in.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#access-manager-faqs","title":"Access Manager FAQs:","text":"<p>1. What are the actions an Access Manager can perform?</p> <pre><code>1. Access Manager should be able to see the collection under \"Shared with you\" and \"Shareable by you\" tabs.\n2. Has permissions to add and/or delete collaborators on the shared collection and restrict their R-W access as needed.\n</code></pre> <p>2. Does an Access Manager need to have an ALCF account?</p> <p>Not necessary. However, if they need to manage the membership on the POSIX side, they will need an ALCF account and be a Proxy on the project.</p> <p>3. What is the difference between an ALCF project Proxy and a guest collection Access Manager?</p> <p>ALCF Project Proxy has permissions to manage project membership on the POSIX side whereas guest collection Access Manager has permissions to manage the project membership specific to that guest collection shared by the PI on the Globus side.</p> <p>4. I am an 'Access Manager' on the collection. Why do I see a 'Permission Denied' error when I try to SHARE a guest collection created by the PI? </p> <p>If you are a non-PI who is able to access the guest collection but unable to share it, it means that your role on this guest collection is limited to a \"Member\". If you want the ability to share folders and sub-folders from the collections that are shared with you, please talk to the PI. They will need to set your role to an \"Access Manager\" for the collection within Globus</p> <p>5. Can an Access Manager give external collaborators access to the collections that are shared with them on Eagle?</p> <p>Yes, an Access Manager will see \"Permissions\" tab at the top of the shared collection page and can share it with collaborators and/or a group.</p> <p>6. Can an Access Manager create collections using the shared endpoint?</p> <p>No. An access manager cannot create a collection, only a PI can do that. The access manager can however share folders and sub-folders from the collections that are shared with them.</p> <p>7. Can an Access Manager leave a globus group or withdraw membership request for collaborators?</p> <p>Yes.[Go to alcf#dtn_eagle-&gt; Groups &gt; group_name -&gt; Members -&gt; click on specific user -&gt; Role &amp; Status -&gt; Set the appropriate status]</p> <p> </p> If you get thie error, you do not have read permissions. <p>8. Can an Access Manager delete guest collections created by PI? No. Access managers cannot delete guest collections.</p> <p>Guest Collection Collaborators: </p> <p>1. What actions can collaborators perform?     1. Collaborators can read files from a collection *     2. Collaborators can write to a collection **     3. Collaborators can delete files in a collection **</p> <p>** If the PI has read permissions for those files on the POSIX side and the collaborator is given read permissions in Globus for the guest collection.</p> <p>** If the PI has write permissions for those files on the POSIX side and the collaborator is given write permissions in Globus  for the guest collection.</p> <p>2. I am a collaborator. Why do I see a 'Permission Denied' error when I try to ACCESS a guest collection created by the PI? If you are a non-PI and you see this error while trying to access the collection, it means that you do not have read permissions to access the quest collection. Please contact the PI for required access.</p> <p> </p> If you get thie error, you do not have read permissions."},{"location":"data-management/acdc/transferring-data-to-eagle/","title":"Transferring Data to Eagle","text":""},{"location":"data-management/acdc/transferring-data-to-eagle/#evolution-of-the-petrel-data-service-to-the-alcf-community-data-co-op","title":"Evolution of the Petrel Data Service to the ALCF Community Data Co-Op","text":"<p>The Petrel data service is evolving into a more mature service called the ALCF Community Data Co-Op (ACDC) which will be launched later this year. </p> <p>In preparation for this shift, all current Petrel project PIs will need to move their project data to ALCF's Eagle filesystem by December 2021.</p> <p>For detailed instructions on how to move your data, please follow the steps outlined below. You will need to follow the order of the steps as listed.</p> <p>If you have any questions, please email: support@alcf.anl.gov.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#transferring-data-to-eagle_1","title":"Transferring data to Eagle","text":""},{"location":"data-management/acdc/transferring-data-to-eagle/#1-request-a-dd-project-on-eagle-filesystem","title":"1. Request a DD project on Eagle Filesystem","text":"<p>All Petrel project owners/PIs should request for a Director's Discretionary project on the Eagle filesystem by filling out the form at https://accounts.alcf.anl.gov/allocationRequests. Select \"New Project\" and then \"Eagle\" as the resource and fill out the rest of the form. In the \"Project and Justification Summary\" section, along with the requested details you should also state that you are migrating your data from Petrel.</p> <p>Once the submission is reviewed and approved by the allocations committee, your project will be created on the Eagle filesystem and you will be notified via email. The approval process may take 1-2 weeks. Once the project is approved, proceed to the next step.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#2-apply-for-an-alcf-account","title":"2. Apply for an ALCF account","text":"<p>A project PI will need an active ALCF account to: - Transfer their data from Petrel to the Eagle filesystem - Enable data sharing on their Eagle project (See section \"4 Share your data on Eagle using Globus Guest Collections\" for more details)</p> <p>NOTE: A collaborator does not need an ALCF account to access data that is shared on Eagle (as a Globus Guest Collection). They can sign into Globus with their institutional identity to access the data. The first time they log in, they will need to accept terms and conditions.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#to-apply-for-an-alcf-account","title":"To apply for an ALCF account:","text":"<ul> <li>Visit https://accounts.alcf.anl.gov and click on \"Request An Account\".</li> <li>When prompted for project name, please select the project on Eagle that was created for your Petrel data as a result of Step 1: Request a DD project on Eagle (you have to wait for your project to be created before you can apply for an account)</li> <li>If you don't have one, please follow the directions under \"Step 1: Request a DD project on Eagle\" (above)</li> <li>For more details on the ALCF account request process, visit the webpage Request an account</li> <li>Once your account is created and you have the cryptocard/mobile token to login to Eagle, proceed to the next step to transfer the data from Petrel to Eagle</li> </ul>"},{"location":"data-management/acdc/transferring-data-to-eagle/#3-transfer-data-from-your-source-endpoint-to-eagle-using-globus","title":"3. Transfer data from your source endpoint to Eagle using Globus","text":"<p>You can use the Globus web app to transfer data or the CLI. See Using CLI for instructions on how to use the CLI to transfer data. The following set of instructions use the Globus web app, using alcf#dtn_eagle (path /projectname) as the destination to transfer data from your source endpoint.</p> <p>NOTE: Anonymous HTTPS read access is enabled on Eagle.</p> <p>Step 1: Log into https://app.globus.org/file-manager?destination_id=05d2c76a-e867-4f67-aa57-76edeb0beda0 which opens two panes in the Globus File Manager, with ALCF Eagle on the right-hand side. - Enter the name of your source endpoint in the pane on the left-hand side.</p> <p> </p> File Manager <p> </p> Enter the name of your source endpoint <p>Step 2: You may have to log in and link your ALCF identity to your Globus account.</p> <p> </p> Log in and link your ALCF identity to your Globus account <p>Step 3: Log in using your ALCF credentials.</p> <p> </p> Use ALCF credentials <p>Step 4: If the login is successful, the folders and files on the Eagle file system will be displayed in the project/file viewer.</p> <p> </p> Eagle file system in the project/file viewer <p>Step 5: Navigate to the correct destination (project folder) on the Eagle file system. Choose the files/folders to transfer in the left-hand side panel (Petrel endpoint).</p> <p>NOTE: Before clicking the \"Start\" button, click on the Transfer and Sync Options and check the \"sync\" checkbox and then click start.</p> <p> </p> Choose the files/folders <p>Step 6: Click on the \"Activity\" tab on the left-hand side navigation panel to view the status and details of your transfers.</p> <p> </p> Activity tab <p>Step 7: Once the transfer is successful, you should see the files and folders on the Eagle file system. You will also receive an email notification from Globus letting you know that your transfer was successful.</p> <p> </p> Files and folders on the Eagle file system"},{"location":"data-management/acdc/transferring-data-to-eagle/#migrating-permissions-from-petrel-to-eagle","title":"Migrating permissions from Petrel to Eagle:","text":"<p>For PIs who had previously stored data on Petrel, and are migrating to Eagle, the following tool automates the step of copying the permissions set on Petrel to Eagle. The tool, migrate_permissions.py at https://github.com/globus/globus-tool-examples takes the source endpoint (your shared endpoint on Petrel in this case), and destination endpoint (the guest collection on Eagle that has the data), and copies over all the permissions. The tool assumes the data was coped over as is from source to destination.</p> <p>If you have any questions on the tool, or need further support, please contact support@globus.org.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#4-share-your-data-on-eagle-using-globus-guest-collections","title":"4. Share your data on Eagle using Globus Guest Collections","text":"<p>Your data on the Eagle file system can easily be shared with collaborators who are at ALCF or elsewhere. You have full control over which files your collaborator can access, and whether they have read-only or read-write permissions.</p> <p>See below for step-by-step instructions on how to share data from Eagle using Globus Guest Collections:</p> <p>https://docs.alcf.anl.gov/data-management/acdc/eagle-data-sharing/</p> <p>NOTE: Guest Collections are tied to the project PI's account so if the PI's account becomes inactive, the Guest Collections will also become inactive. Once the PI's account is reactivated, access to the Guest Collections is restored.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#using-globus-cli-tool","title":"Using Globus CLI tool:","text":"<p>To copy data and permissions from a source collection, PIs can use a Globus CLI tool that automates the step of copying the permissions set on the source collection and applies them to the collection on Eagle. This is especially useful for PIs who had previously stored data on Petrel. See https://github.com/globus/globus-tool-examples for more information.</p> <p>The tool, migrate_permissions.py in the github repo takes the source endpoint (the shared endpoint on Petrel for example), and destination endpoint (the guest collection on Eagle that has the data), and copies over all the permissions. The tool assumes the data was coped over as is from source to destination. Note that you need to have a guest collection set up for your project on Eagle to use the CLI command and tool. See this page for instructions on how to set up guest collections.</p> <p>If you have any questions on the tool, or need further support, please contact support@globus.org.</p> <p>Existing data portals: To reconfigure and update your existing data portals to point to your guest collections on Eagle, please work directly with developer/maintainer of the portal.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#faqs-for-migrating-petrel-data-to-eagle","title":"FAQs for migrating Petrel data to Eagle:","text":""},{"location":"data-management/acdc/transferring-data-to-eagle/#1-is-it-important-for-a-petrel-project-ownerpi-to-obtain-an-alcf-account","title":"1. Is it important for a Petrel project owner/PI to obtain an ALCF account?","text":"<p>Yes, the data from Petrel needs to be moved to an ALCF project directory on the Eagle filesystem. The PI will need an ALCF account to log into Globus and move the data to their Eagle project directory.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#2-what-is-the-workflow-for-migrating-data-from-petrel-and-giving-access-to-collaborators-on-eagle","title":"2. What is the workflow for migrating data from Petrel and giving access to collaborators on Eagle?","text":"<ol> <li>PI requests an Eagle allocation project</li> <li>Allocations Committee reviews and approves requests</li> <li>Once the allocation request is approved, the project is created and associated with a UNIX group and project directory on Eagle</li> <li>PI requests an ALCF account (if they don't have one)</li> <li>Once the ALCF account is created and tied to the project on Eagle, the PI moves the data from Petrel to Eagle using Globus</li> <li>PI creates guest collections for the project on Eagle, using the Globus web app using the mapped collection/endpoint for Eagle (alcf#dtn_eagle). Note that:</li> <li>The PI needs to have an active ALCF Account and will need to log in to Globus using their ALCF credentials</li> <li>Only the PI (and not a proxy) can create guest collections</li> <li>If the PI already has a Globus account, it needs to be linked to their ALCF account</li> <li>PI adds collaborators to the guest collection. </li> <li>Added with read-only or read-write permissions.</li> <li>Note: Anonymous HTTPS write is disabled and only anonymous HTTPS read is allowed.</li> <li>Existing data portals on Petrel should be updated to point to the new guest collection on Eagle. Please work directly with developer/maintainer of the portal.</li> </ol>"},{"location":"data-management/acdc/transferring-data-to-eagle/#3-what-endpoints-should-the-pi-use-to-move-data-from-petrel","title":"3. What endpoints should the PI use to move data from Petrel?**","text":"<ul> <li>Source: Globus endpoint on Petrel for the Petrel allocation</li> <li>Destination: Globus endpoint on the Eagle filesystem and the path to the directory  (alcf#dtn_eagle, path /) OR the name of the    guest collection on Eagle"},{"location":"data-management/data-transfer/sftp-scp/","title":"SFTP and SCP","text":"<p>These standard utilities are available for local area transfers of small files; they are not recommended for use with large data transfers due to poor performance and excess resource utilization on the login nodes.</p> <p>See Globus for performing large data transfers.</p>"},{"location":"data-management/data-transfer/using-globus/","title":"Using Globus","text":"<p>Globus addresses the challenges faced by researchers in moving, sharing, and archiving large volumes of data among distributed sites. With Globus, you hand off data movement tasks to a hosted service that manages the entire operation. It monitors performance and errors, retries failed transfers, corrects problems automatically whenever possible, and reports status to keep you informed and keep you focused on your research. </p> <p>Command line and Web-based interfaces are available. The command line interface, which requires only ssh to be installed on the client, is the method of choice for script-based workflows. Globus also provides a REST-style transfer API for advanced-use cases that require scripting and automation.</p>"},{"location":"data-management/data-transfer/using-globus/#getting-started","title":"Getting Started","text":"<p>Basic documentation for getting started with Globus can be found at the following URL: https://docs.globus.org/how-to/</p>"},{"location":"data-management/data-transfer/using-globus/#data-transfer-node","title":"Data Transfer Node","text":"<p>A total of 13 data transfer nodes (DTNs) for /home, theta-fs0, and Grand (6 of these DTNs are also used for HPSS) and 4 DTNs for Eagle are available to ALCF users, allowing users to perform wide and local area data transfers. Access to the DTNs is provided via the following Globus endpoints:</p>"},{"location":"data-management/data-transfer/using-globus/#alcf-globus-endpoints","title":"ALCF Globus Endpoints","text":"<p>The Globus endpoint and the path to use depends on where your data resides. If your data is on:</p> <ul> <li>/home which is where your home directory resides : alcf#dtn_home for accessing /home (i.e. home directories on swift-home filesystem). Use the path /&lt;username&gt;</li> <li>theta-fs0 filesystem: alcf#dtn_theta-fs0 for accessing /lus/theta-fs0 (i.e. project directories on Theta-fs0 filesystem). Use the path /&lt;project name&gt;</li> <li>HPSS: alcf#dtn_hpss</li> <li>Grand filesystem: alcf#dtn_grand for accessing /lus/grand/projects or /grand (.i.e. project directories on Grand filesystem). Use the path /grand/&lt;project name&gt;</li> <li>Eagle filesystem: alcf#dtn_eagle for accessing /lus/eagle/projects or /eagle (.i.e project directories on Eagle filesystem). Use the path /eagle/&lt;project name&gt;</li> </ul> <p>After registering, simply use the appropriate ALCF endpoint, as well as other sources or destinations. Use your ALCF credentials (your OTP generated by the CryptoCARD token with PIN or Mobilepass app) to activate the ALCF endpoint.</p> <p>Globus Connect Personal allows users to add laptops or desktops as an endpoint to Globus, in just a few steps. After you set up Globus Connect Personal, Globus can be used to transfer files to and from your computer.</p>"},{"location":"data-management/data-transfer/using-globus/#references","title":"References","text":"<p>Research Data Management with Globus</p>"},{"location":"data-management/filesystem-and-storage/data-storage/","title":"ALCF Data Storage","text":""},{"location":"data-management/filesystem-and-storage/data-storage/#disk-storage","title":"Disk Storage","text":"<p>The ALCF operates a number of file systems that are mounted globally across all of our production systems.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#home","title":"Home","text":"<p>A Lustre file system residing on a DDN AI-400X NVMe Flash platform. It has 24 NVMe drives with 7 TB each with 123 TB of usable space. It provides 8 Object Storage Targets and 4 Metadata Targets.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#grand","title":"Grand","text":"<p>A Lustre file system residing on an HPE ClusterStor E1000 platform equipped with 100 Petabytes of usable capacity across 8480 disk drives. This ClusterStor platform provides 160 Object Storage Targets and 40 Metadata Targets with an aggregate data transfer rate of 650GB/s.  The primary use of grand is compute campaign storage.</p> <p>Also see ALCF Data Policies and Data Transfer</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#eagle","title":"Eagle","text":"<p>A Lustre file system residing on an HPE ClusterStor E1000 platform equipped with 100 Petabytes of usable capacity across 8480 disk drives. This ClusterStor platform provides 160 Object Storage Targets and 40 Metadata Targets with an aggregate data transfer rate of 650GB/s.  The primary use of eagle is data sharing with the research community.  Eagle has community sharing community capabilities which allow PIs to share their project data with external collabortors using Globus.  Eagle can also be used for compute campaign storage.</p> <p>Also see ALCF Data Policies and Data Transfer</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#theta-fs0","title":"theta-fs0","text":"<p>A Lustre file system residing on an HPE Sonexion 3000 storage array with a usable capacity of 9.2PB and an aggregate data transfer rate of 240GB/s.  This is a legacy file system.  No new allocations are granted on theta-fs0.</p> <p>Also see ALCF Data Policies and Data Transfer</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#theta-fs1","title":"theta-fs1","text":"<p>A GPFS file system that resides on an IBM Elastic Storage System (ESS) cluster with a usable capacity of 7.9PB and an aggregate data transfer rate of 400GB/s.  This is a legacy file system.  No new allocations are granted on theta-fs1.</p> <p>Also see ALCF Data Policies and Data Transfer</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#tape-storage","title":"Tape Storage","text":"<p>ALCF operates three 10,000 slot Spectralogic tape libraries.  We are currently running a combination of LTO6 and LTO8 tape technology.  The LTO tape drives have built-in hardware compression which typically achieve compression ratios between 1.25:1 and 2:1 depending on the data yielding an effective capacity of approximately 65PB.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#hpss","title":"HPSS","text":"<p>HPSS is a data archive and retrieval system that manages large amounts of data on disk and robotic tape libraries. It provides hierarchical storage management services that allow it to migrate data between those storage platforms.</p> <p>HPSS is currently configured with a disk and tape tier. The disk tier has a capacity of 1.2PB on a DataDirect Networks SFA12K-40 storage array. By default, all archived data is initially written to the disk tier. The tape tier consists of 3 SpectraLogic T950 robotic tape libraries containing a total of 72 LTO6 tape drives with total uncompressed capacity 64 PB. Archived data is migrated to the tape tier at regular intervals, then deleted from the disk tier to create space for future archives.</p> <p>Access to HPSS is provided by various client components. Currently, ALCF supports access through two command-line clients, HSI and HTAR.  These are installed on the login nodes of Theta and Cooley. In order for the client to authenticate with HPSS, the user must have a keytab file that should be located in their home directory under subdirectory .hpss. The file name will be in the format .ktb_."},{"location":"data-management/filesystem-and-storage/data-storage/#hsi-general-usage","title":"HSI General Usage","text":"<p>Before you can use HSI on XC40 systems such as Theta, you must load a module:</p> <p><code>module load hsi</code></p> <p>HSI can be invoked by simply entering hsi at your normal shell prompt. Once authenticated, you will enter the hsi command shell environment:</p> <pre><code>&gt; hsi\n[HSI]/home/username-&gt;\n</code></pre> <p>You may enter \"help\" to display a brief description of available commands.</p> <p>If archiving from or retrieving to grand or eagle you must disable the Transfer Agent. -T off</p> <p>Example archive <pre><code>[HSI]/home/username-&gt; put mydatafile                # same name on HPSS\n[HSI]/home/username-&gt; put local.file : hpss.file    # different name on HPSS\n[HSI]/home/username-&gt; put -T off mydatafile\n</code></pre></p> <p>Example retrieval <pre><code>[HSI]/home/username-&gt; get mydatafile\n[HSI]/home/username-&gt; get local.file : hpss.file\n[HSI]/home/username-&gt; get -T off mydatafile\n</code></pre></p> <p>Most of the usual shell commands will work as expected in the HSI command environment. For example, checking what files are archived:</p> <p><code>[HSI]/home/username-&gt; ls -l</code></p> <p>And organizing your archived files:</p> <pre><code>[HSI]/home/username-&gt; mkdir dataset1\n[HSI]/home/username-&gt; mv hpss.file dataset1\n[HSI]/home/username-&gt; ls dataset1\n[HSI]/home/username-&gt; rm dataset1/hpss.file\n</code></pre> <p>It may be necessary to use single or double quotes around metacharacters to avoid having the shell prematurely expand them.  For example:</p> <pre><code>[HSI]/home/username-&gt; get *.c\n</code></pre> <p>will not work, but</p> <pre><code>[HSI]/home/username-&gt; get \"*.c\"\n</code></pre> <p>will retrieve all files ending in .c.</p> <p>Following normal shell conventions, other special characters in filenames such as whitespace and semicolon also need to be escaped with \"\\\" (backslash).   For example:</p> <pre><code>       [HSI]/home/username-&gt; get \"data\\ file\\ \\;\\ version\\ 1\"\n</code></pre> <p>retrieves the file named \"data file ; version 1\".</p> <p>HSI can also be run as a command line or embedded in a script as follows:</p> <pre><code>hsi -O log.file \"put local.file\"\n</code></pre>"},{"location":"data-management/filesystem-and-storage/data-storage/#htar-general-usage","title":"HTAR General Usage","text":"<p>HTAR is a tar-like utility that creates tar-format archive files directly in HPSS. It can be run as a command line or embedded in a script.</p> <p>Example archive <pre><code>htar -cf hpssfile.tar localfile1 localfile2 localfile3\n</code></pre></p> <p>Example retrieval</p> <pre><code>htar -xf hpssfile.tar localfile2\n</code></pre> <p>NOTE: On Theta you must first load the HSI module to make HSI and HTAR available. \"module load hsi\" NOTE:  The current version of HTAR has a 64GB file size limit as well as a path length limit.  The recommended client is HSI.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#globus","title":"Globus","text":"<p>In addition, HPSS is accessible through the Globus endpoint <code>alcf#dtn_hpss</code>.  As with HSI and HTAR, you must have a keytab file before using this endpoint.  For more information on using Globus, please see [Using Globus].</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#keytab-file-missing","title":"Keytab File Missing","text":"<p>If you see an error like this:</p> <pre><code>*** HSI: (KEYTAB auth method) - keytab file missing or inaccessible: /\n home/username/.hpss/.ktb_username\n Error - authentication/initialization failed\n</code></pre> <p>it means that your account is not enabled to use the HPSS yet. Please contact support to have it set up.</p>"},{"location":"data-management/filesystem-and-storage/disk-quota/","title":"Disk Quota","text":""},{"location":"data-management/filesystem-and-storage/disk-quota/#overview","title":"Overview","text":"<p>Disk quotas are enabled on project directories. ALCF's HPC systems use the swift-home file system located at /lus/swift/home where quotas are also enforced. Theta has three project file systems available to user. Details on the home file system are listed in file systems. Following are descriptions and examples for the home file system, as well as the theta-fs0, grand and eagle project filesystems.</p>"},{"location":"data-management/filesystem-and-storage/disk-quota/#home-directory-quotas","title":"Home Directory Quotas","text":"<p>By default, each home directory is assigned a default of 50GB. File ownership determines disk space usage.</p> <p>To check the home directory usage, enter this command: <pre><code>&gt; myquota\nName                           Type     Filesystem        Used               Quota          Grace\n=========================================================================================================\nuserX                         User     /lus/swift         44.13G          50.00G             none\n</code></pre></p>"},{"location":"data-management/filesystem-and-storage/disk-quota/#project-directory-quotas","title":"Project Directory Quotas","text":"<p>The Grand, Eagle, and Lustre project file system (/lus/theta-fs0) support project quotas. The amount of data stored under /lus//projects/PROJECT_NAME cannot exceed the approved project quota limit approved during the allocation period. The total data usage under the project directory is used to calculate the disk quota. <p>To check project quota usage on the file systems, enter this command: <pre><code>&gt; myprojectquotas\n\nLustre : Current Project Quota information for projects you're a member of:\n\nName                       Type        Filesystem          Used             Quota           Grace\n==============================================================================================================\nprojectX                  Project      theta-fs0            354.4T             700T            -\nprojectY                  Project      theta-fs0            916k                 1T            -\nprojectZ                  Project      grand                  8k              1000T            -\nprojectX                  Project      eagle                1.87T             1000T            -\n</code></pre></p>"},{"location":"data-management/filesystem-and-storage/disk-quota/#requesting-a-new-eagle-allocation","title":"Requesting a New Eagle Allocation","text":"<p>For requesting a new project having an allocation on Eagle (with or without a compute allocation), please make a request by filling out the Director's Discretionary allocation form. Note that all new compute projects will have Grand as the default file system.</p>"},{"location":"data-management/filesystem-and-storage/disk-quota/#quota-increases","title":"Quota Increases","text":"<p>If you need a quota increase for Director's Discretionary allocations, please make a request by filling out the Director's Discretionary allocation form.</p> <p>If you need a quota increase for your INCITE/ALCC/ALCC/ESP project directory, please send an email to support@alcf.anl.gov with the machine, project name, new quota amount and reason for the increase.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/","title":"ALCF File Systems","text":"<p>Our HPC systems have three discrete file systems for project data: theta-fs0, Grand, and Eagle.  Theta-fs0 is an Intel Enterprise Edition Lustre parallel file system mounted as /lus-projects or /projects.  Grand and Eagle are 100 PB Lustre file systems mounted as /grand and /eagle respectively.  For more information on the Lustre file system, here is a document on Lustre File Striping Basics.</p> <ul> <li>Lustre File Striping Basics</li> </ul> <p>For information on the AI Testbed storage systems, refer to the AI Testbed storage page: https://argonne-lcf.github.io/ai-testbed-userdocs/common/storage/</p> <p>Our HPC systems also share a Lustre home file system, called swift-home. The home file system is mounted as /home, and should generally be used for small files and any binaries to be run on Theta. The performance of this file system is reasonable, but using it for intensive I/O from the compute nodes is discouraged because I/O from the compute nodes uses the project data file systems, which are fast parallel systems and have far more storage space and greater I/O performance than the home directory space.</p> <p>The swift-home file system is regularly backed up to tape. The data file system is not backed up. It is the user\u2019s responsibility to ensure that copies of any critical data on the data file system have either been archived to tape or stored elsewhere.</p> Name Accessible From Type Path Production Backed-up Usage swift-home Theta  ThetaGPU  Cooley  Polaris Lustre /home or /lus/swift/home Yes Yes General use lus-projects  (theta-fs0) Theta  ThetaGPU  Cooley Lustre /projects or /lus-projects or /lus/theta-fs0/projects Yes No Intensive job output, large files Grand Theta  ThetaGPU  Cooley  Polaris Lustre /grand or /lus/grand/projects Yes No Intensive job output, large files Eagle Theta  ThetaGPU  Cooley  Polaris Lustre /eagle or /lus/eagle/projects Yes No Community sharing via Globus;  Intensive job output, large files Node SSD  (Compute node only) Theta  ThetaGPU  Polaris xfs /local/scratch (Theta)  /local/scratch (Polaris) /raid/scratch (ThetaGPU) Yes   Theta &amp; ThetaGPU by request only No Local node scratch during run"},{"location":"data-management/filesystem-and-storage/file-systems/#available-directories","title":"Available Directories","text":""},{"location":"data-management/filesystem-and-storage/file-systems/#home-directories","title":"Home Directories","text":"<ul> <li>Created when an account is created</li> <li>Located under /home</li> <li>Each home directory is subject to a quota based on user file ownership. The default quota is 50 GB</li> </ul>"},{"location":"data-management/filesystem-and-storage/file-systems/#sharing-home-directory-files-or-subdirectories-with-others","title":"Sharing Home Directory Files or Subdirectories with Others","text":"<p>If you need to share files or subdirectories (folders) under your home directory with collaborators (other ALCF users), you need to change file permissions from their defaults. You must change permissions of your top-level /home/username directory, even if you only want to share certain files/directories within it. Using normal linux file permissions control is good enough to give access to all other users, and is simple. For more fine-grained control over specific users, you need to use linux access control list (ACL) commands.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/#simple-method-permission-to-all-users","title":"Simple Method: Permission to All Users","text":"<p>First, a one-time-only change to your top-level /home/username directory.</p> <pre><code>chmod o+x /home/username\n</code></pre> <p>Then you may permission individual files and/or subdirectories with read access. For example, to recursively change permissions on /home/username/subdirectoryname so that all files in that subdirectory and any subdirectory trees within it are world-readable, you would use</p> <pre><code>chmod -R o+Xr /home/username/subdirectoryname\n</code></pre>"},{"location":"data-management/filesystem-and-storage/file-systems/#refined-method-use-acl-to-give-permission-to-specific-users","title":"Refined Method: Use ACL to Give Permission to Specific Users","text":"<p>First, a one-time-only change to your top-level /home/username directory. To share files/directories with user gilgamesh, for example:</p> <pre><code>setfacl u:gilgamesh:--x /home/username\n</code></pre> <p>Then you may permission individual files and/or subdirectories with read access. For example, to recursively change permissions on /home/username/subdirectoryname so that all files in that subdirectory and any subdirectory trees within it are readable to user gilgamesh, you would use</p> <pre><code>setfacl -R -u gilgamesh:m:r-X,d:u:gilgamesh:r-X /home/username/subdirectoryname\n</code></pre>"},{"location":"data-management/filesystem-and-storage/file-systems/#project-directories","title":"Project Directories","text":"<ul> <li>Directories on Grand or Eagle are created when an allocation (INCITE, ALCC, Discretionary, etc.) is awarded. Eagle directories can be created as stand-alone allocations. Use the allocation request form to submit requests for an allocation on Eagle. Note that project directories are no longer created on theta-fs0.</li> <li>Directory paths:<ul> <li>theta-fs0: /projects or /lus-projects or /lus/theta-fs0/projects</li> <li>Grand: /grand or /lus/grand/projects</li> <li>Eagle: /eagle or /lus/eagle/projects</li> </ul> </li> </ul> <p>These project spaces do not have user quotas but a directory quota, meaning that ALL files contained within a project directory, regardless of the username, cannot exceed the disk space allocation granted to the project. For more information on quotas, see the Disk Quota page.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/#local-node-ssd","title":"Local Node SSD","text":"<p>Access to SSDs is disabled by default for Theta and ThetaGPU. Project PIs may request access by emailing support@alcf.anl.gov. A use case will need to be provided.</p> <p>Access to SSDs is enabled by default on Polaris.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/#ssd-information","title":"SSD Information","text":"<ul> <li>Local scratch SSD storage on compute nodes for running jobs</li> <li>Completely local non-parallel filesystem</li> <li>Located at /local/scratch on Theta and Polaris computes and /raid/scratch on ThetaGPU computes</li> <li>Wiped between Cobalt/PBS Pro jobs</li> <li>No automatic backups provided</li> <li>Information on the current SSD drives in use is below:</li> </ul> <p>Polaris SSD Specs</p> <p>Model PM1725a drives specifications</p> Model PM1725a drives ------- Capacity 1.6 TB Sequential Read 3300 MB/s Sequential Write    3300 MB/s <p>Theta and ThetaGPU SSD Specs</p> <p>Model SM961 drives</p> Model SM961 drives ------- Capacity 128 GB Sequential Read 3100 MB/s Sequential Write    700 MB/s <p>Model SM951 drives specifications</p> Model SM951 drives ------ Capacity 128 GB Sequential Read 2150 MB/s Sequential Write    1550 MB/s"},{"location":"data-management/filesystem-and-storage/hpss/","title":"Using HPSS","text":""},{"location":"data-management/filesystem-and-storage/hpss/#overview","title":"Overview","text":"<p>HPSS is a data archive and retrieval system that manages large amounts of data on disk and robotic tape libraries. It provides hierarchical storage management services that allow it to migrate data between those storage platforms.</p> <p>HPSS is currently configured with a disk and tape tier. The disk tier has a capacity of 1.2PB on a DataDirect Networks SFA12K-40 storage array. By default, all archived data is initially written to the disk tier. The tape tier consists of 3 SpectraLogic T950 robotic tape libraries containing a total of 72 LTO6 tape drives with total uncompressed capacity 64 PB. Archived data is migrated to the tape tier at regular intervals, then deleted from the disk tier to create space for future archives.</p> <p>Access to HPSS is provided by various client components. Currently, ALCF supports access through two command-line clients, HSI and HTAR.  These are installed on the login nodes of Theta and Cooley (NOTE: The HPSS clients, HSI and HTAR are not yet installed on Polaris). In order for the client to authenticate with HPSS, the user must have a keytab file that should be located in their home directory under subdirectory .hpss. The file name will be in the format .ktb_."},{"location":"data-management/filesystem-and-storage/hpss/#hsi-general-usage","title":"HSI General Usage","text":"<p>HSI can be invoked by simply entering hsi at your normal shell prompt. Once authenticated, you will enter the hsi command shell environment: <pre><code>&gt; hsi\n[HSI]/home/username-&gt;\n</code></pre></p> <p>You may enter \"help\" to display a brief description of available commands.</p> <p>Example archive: <pre><code>[HSI]/home/username-&gt; put mydatafile                # same name on HPSS\n[HSI]/home/username-&gt; put local.file : hpss.file    # different name on HPSS\n</code></pre></p> <p>Example retrieval: <pre><code>[HSI]/home/username-&gt; get mydatafile\n[HSI]/home/username-&gt; get local.file : hpss.file\n</code></pre></p> <p>Most of the usual shell commands will work as expected in the HSI command environment. </p> <p>For example, checking what files are archived: <pre><code>[HSI]/home/username-&gt; ls -l\n</code></pre></p> <p>And organizing your archived files: <pre><code>[HSI]/home/username-&gt; mkdir dataset1\n[HSI]/home/username-&gt; mv hpss.file dataset1\n[HSI]/home/username-&gt; ls dataset1\n[HSI]/home/username-&gt; rm dataset1/hpss.file\n</code></pre></p> <p>It may be necessary to use single or double quotes around metacharacters to avoid having the shell prematurely expand them.  </p> <p>For example: <pre><code>[HSI]/home/username-&gt; get *.c\n\nwill not work, but\n\n[HSI]/home/username-&gt; get \"*.c\"\n\nwill retrieve all files ending in .c.  \n</code></pre></p> <p>Following normal shell conventions, other special characters in filenames such as whitespace and semicolon also need to be escaped with \"\\\" (backslash).   For example:</p> <pre><code>   [HSI]/home/username-&gt; get \"data\\ file\\ \\;\\ version\\ 1\"\n</code></pre> <p>retrieves the file named \"data file ; version 1\".</p> <p>HSI can also be run as a command line or embedded in a script as follows: <pre><code>hsi -O log.file \"put local.file\"\n</code></pre></p>"},{"location":"data-management/filesystem-and-storage/hpss/#htar-general-usage","title":"HTAR General Usage","text":"<p>HTAR is a tar-like utility that creates tar-format archive files directly in HPSS. It can be run as a command line or embedded in a script.</p> <p>Example archive: <pre><code>htar -cf hpssfile.tar localfile1 localfile2 localfile3\n</code></pre></p> <p>Example retrieval: <pre><code>htar -xf hpssfile.tar localfile2\n</code></pre></p> <p>Note: - On Theta you must first load the HSI module to make HSI and HTAR available. \"module load hsi\" - The current version of HTAR has a 64GB file size limit as well as a path length limit.  The recommended client is HSI</p>"},{"location":"data-management/filesystem-and-storage/hpss/#globus","title":"Globus","text":"<p>In addition, HPSS is accessible through the Globus endpoint alcf#dtn_hpss.  As with HSI and HTAR, you must have a keytab file before using this endpoint.  For more information on using Globus, please see Using Globus.</p>"},{"location":"data-management/filesystem-and-storage/hpss/#common-problems","title":"Common Problems","text":""},{"location":"data-management/filesystem-and-storage/hpss/#keytab-file-missing","title":"Keytab File Missing","text":"<p>If you see an error like this: <code>*** HSI: (KEYTAB auth method) - keytab file missing or inaccessible: /  home/username/.hpss/.ktb_username  Error - authentication/initialization failed</code>  it means that your account is not enabled to use the HPSS yet. Please contact support to have it set up.</p>"},{"location":"polaris/getting-started/","title":"Getting Started on Polaris","text":""},{"location":"polaris/getting-started/#logging-into-polaris","title":"Logging Into Polaris","text":"<p>To log into Polaris: <pre><code>ssh &lt;username&gt;@polaris.alcf.anl.gov\n</code></pre> Then, type in the password from your CRYPTOCard/MobilePASS+ token.</p>"},{"location":"polaris/getting-started/#hardware-overview","title":"Hardware Overview","text":"<p>An overview of the Polaris system including details on the compute node architecture is available on the Machine Overview page.</p>"},{"location":"polaris/getting-started/#compiling-applications","title":"Compiling Applications","text":"<p>Users are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.</p>"},{"location":"polaris/getting-started/#submitting-and-running-jobs","title":"Submitting and Running Jobs","text":"<p>Users are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. Some example job submission scripts are available on the Example Job Scripts page as well.</p>"},{"location":"polaris/getting-started/#lustre-file-striping","title":"Lustre File Striping","text":"<p>In addition to the content above, here is a document on Lustre File Striping Basics. </p> <ul> <li>Lustre File Striping Basics</li> </ul>"},{"location":"polaris/getting-started/#proxy","title":"Proxy","text":"<p>If the node you are on doesn\u2019t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host</p> <pre><code># proxy settings\nexport HTTP_PROXY=\"http://proxy-01.pub.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy-01.pub.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy-01.pub.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy-01.pub.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy-01.pub.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov\"\n</code></pre>"},{"location":"polaris/getting-started/#getting-assistance","title":"Getting Assistance","text":"<p>Please direct all questions, requests, and feedback to support@alcf.anl.gov.</p>"},{"location":"polaris/known-issues/","title":"Known Issues","text":"<p>This is a collection of known issues that have been encountered during Polaris's early user phase. Documentation will be updated as issues are resolved.</p> <ol> <li> <p>The <code>nsys</code> profiler packaged with <code>nvhpc/21.9</code> in some cases appears to be presenting broken timelines with start times not lined up. The issue does not appear to be present when <code>nsys</code> from <code>cudatoolkit-standalone/11.2.2</code> is used. We expect this to no longer be an issue once <code>nvhpc/22.5</code> is made available as the default version.</p> </li> <li> <p>With <code>PrgEnv-nvhpc/8.3.3</code>, if you are using <code>nvcc</code> to indirectly invoke <code>nvc++</code> and compiling C++17 code (as, for example, in building Kokkos via <code>nvcc_wrapper</code>), you will get compilation errors with C++17 constructs. See our documentation on NVIDIA Compilers for a workaround.</p> </li> <li> <p><code>PrgEnv-nvhpc/8.3.3</code> currently loads the <code>nvhpc/21.9</code> module, which erroneously has the following lines: <pre><code>setenv(\"CC\",\"/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilers/bin/nvc\")\nsetenv(\"CXX\",\"/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilers/bin/nvc++\")\nsetenv(\"FC\",\"/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilers/bin/nvfortran\")\nsetenv(\"F90\",\"/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilers/bin/nvfortran\")\nsetenv(\"F77\",\"/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilers/bin/nvfortran\")\nsetenv(\"CC\",\"cpp\")\n</code></pre> In particular, the final line can cause issues for C-based projects (e.g. CMake may complain because the <code>cpp</code> C preprocessor is not a compiler). We recommend running the following in such cases: <pre><code>unset CC\nunset F77\nunset CXX\nunset FC\nunset F90\n</code></pre></p> </li> <li> <p>Cray MPICH may exhibit issues when MPI ranks call <code>fork()</code> and are distributed across multiple nodes. The process may hang or throw a segmentation fault. </p> <p>In particular, this can manifest in hangs with PyTorch+Horovod with a <code>DataLoader</code> with multithreaded workers and distributed data parallel training on multiple nodes. We have built a module <code>conda/2022-09-08-hvd-nccl</code> which includes a Horovod built without support for MPI. It uses NCCL for GPU-GPU communication and Gloo for coordination across nodes.</p> <p><code>export IBV_FORK_SAFE=1</code> may be a workaround for some manifestations of this bug; however it will incur memory registration overheads. It does not fix the hanging experienced with multithreaded dataloading in PyTorch+Horovod across multiple nodes with <code>conda/2022-09-08</code>, however (instead prompting a segfault). </p> <p>This incompatibility also may affect Parsl; see details in the Special notes for Polaris section of the Parsl page.</p> </li> <li> <p>For batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (<code>small</code>, ..., <code>backfill-large</code>) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history <code>qstat -xu &lt;username&gt;</code> (current bug in PBS). E.g. if a user submits a script to the <code>prod</code> routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the <code>small</code> execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted. </p> </li> <li> <p>Job scripts are copied to temporary locations after <code>qsub</code> and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, <code>qalter</code> requires <code>-A &lt;allocation name&gt;</code> when changing job properties. Currently, there is a request for a <code>qalter</code>-like command to trigger a re-copy of the original script to the temporary location. </p> </li> </ol>"},{"location":"polaris/running-jobs/","title":"Running Jobs on Polaris","text":""},{"location":"polaris/running-jobs/#queues","title":"Queues","text":"<p>There are five production queues you can target in your qsub (<code>-q &lt;queue name&gt;</code>):</p> Queue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 8 nodes in use by this queue ay any given time debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs max 20 jobs running/accruing/queued per-project; see note below demand 1 56 5 min 1 hr By request only; max 100 jobs running/accruing/queued per-project <p>Note: Jobs in the demand queue take priority over jobs in the preemptable queue. This means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue. Please use the following command to view details of a queue: <code>qstat -Qf &lt;queuename&gt;</code></p> <p><code>prod</code> is routing queue and routes your job to one of the following six execution queues:</p> Queue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance <ul> <li>Note 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".</li> <li>Note 2: All of these queues have a limit of ten (10) jobs running/accruing per-project</li> <li>Note 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project</li> <li>Note 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).</li> </ul>"},{"location":"polaris/running-jobs/#running-mpiopenmp-applications","title":"Running MPI+OpenMP Applications","text":"<p>Once a submitted job is running calculations can be launched on the compute nodes using <code>mpiexec</code> to start an MPI application. Documentation is accessible via <code>man mpiexec</code> and some helpful options follow.</p> <ul> <li><code>-n</code> total number of MPI ranks</li> <li><code>-ppn</code> number of MPI ranks per node</li> <li><code>--cpu-bind</code> CPU binding for application</li> <li><code>--depth</code> number of cpus per rank (useful with <code>--cpu-bind</code>)</li> <li><code>--env</code> set environment variables (<code>--env OMP_NUM_THREADS=2</code>)</li> <li><code>--hostfile</code> indicate file with hostnames (the default is <code>--hostfile $PBS_NODEFILE</code>)</li> </ul> <p>A sample submission script with directives is below for a 4-node job with 32 MPI ranks on each node and 8 OpenMP threads per rank (1 per CPU).</p> <pre><code>#!/bin/bash -l\n#PBS -N AFFINITY\n#PBS -l select=4:ncpus=256\n#PBS -l walltime=0:10:00\n#PBS -q debug-scaling\n#PBS -A Catalyst\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=32 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\nNTOTRANKS=$(( NNODES * NRANKS ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\ncd /home/knight/affinity\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n</code></pre>"},{"location":"polaris/running-jobs/#running-gpu-enabled-applications","title":"Running GPU-enabled Applications","text":"<p>GPU-enabled applications will similarly run on the compute nodes using the above example script. - The environment variable <code>MPICH_GPU_SUPPORT_ENABLED=1</code> needs to be set if your application requires MPI-GPU support whereby the MPI library sends and receives data directly from GPU buffers. In this case, it will be important to have the <code>craype-accel-nvidia80</code> module loaded both when compiling your application and during runtime to correctly link against a GPU Transport Layer (GTL) MPI library. Otherwise, you'll likely see <code>GPU_SUPPORT_ENABLED is requested, but GTL library is not linked</code> errors during runtime. - If running on a specific GPU or subset of GPUs is desired, then the <code>CUDA_VISIBLE_DEVICES</code> environment variable can be used. For example, if one only wanted an application to access the first two GPUs on a node, then setting <code>CUDA_VISIBLE_DEVICES=0,1</code> could be used.</p>"},{"location":"polaris/running-jobs/#binding-mpi-ranks-to-gpus","title":"Binding MPI ranks to GPUs","text":"<p>The Cray MPI on Polaris does not currently support binding MPI ranks to GPUs. For applications that need this support, this instead can be handled by use of a small helper script that will appropriately set <code>CUDA_VISIBLE_DEVICES</code> for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment.</p> <p>A example <code>set_affinity_gpu_polaris.sh</code> script follows where GPUs are assigned round-robin to MPI ranks.</p> <p><pre><code>#!/bin/bash -l\nnum_gpus=4\n# need to assign GPUs in reverse order due to topology\n# See Polaris Device Affinity Information:\n# https://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho \u201cRANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}\u201d\nexec \"$@\"\n</code></pre> This script can be placed just before the executable in the <code>mpiexec</code> command like so. <pre><code>mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n</code></pre> Users with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.</p>"},{"location":"polaris/running-jobs/#interactive-jobs-on-compute-nodes","title":"Interactive Jobs on Compute Nodes","text":"<p>Here is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes: <pre><code>qsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug\n</code></pre></p> <p>This command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.</p> <p>NOTE: If you want to <code>ssh</code> or <code>scp</code> to one of your assigned compute nodes you will need to make sure your <code>$HOME</code> directory and your <code>$HOME/.ssh</code> directory permissions are both set to <code>700</code>.</p>"},{"location":"polaris/running-jobs/#running-multiple-mpi-applications-on-a-node","title":"Running Multiple MPI Applications on a node","text":"<p>Multiple applications can be run simultaneously on a node by launching several <code>mpiexec</code> commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources and/or targets specific GPUs. One can provide a list of CPUs using the <code>--cpu-bind</code> option, which when combined with <code>CUDA_VISIBLE_DEVICES</code> provides a user with specifying exactly which CPU and GPU resources to run each application on. In the example below, four instances of the application are simultaneously running on a single node. In the first instance, the application is spawning MPI ranks 0-7 on CPUs 24-31 and using GPU 0. This mapping is based on output from the <code>nvidia-smi topo -m</code> command and pairs CPUs with the closest GPU.</p> <pre><code>export CUDA_VISIBLE_DEVICES=0\nmpiexec -n 8 --ppn 8 --cpu-bind list:24:25:26:27:28:29:30:31 ./hello_affinity &amp;\nexport CUDA_VISIBLE_DEVICES=1\nmpiexec -n 8 --ppn 8 --cpu-bind list:16:17:18:19:20:21:22:23 ./hello_affinity &amp;\nexport CUDA_VISIBLE_DEVICES=2\nmpiexec -n 8 --ppn 8 --cpu-bind list:8:9:10:11:12:13:14:15 ./hello_affinity &amp;\nexport CUDA_VISIBLE_DEVICES=3\nmpiexec -n 8 --ppn 8 --cpu-bind list:0:1:2:3:4:5:6:7 ./hello_affinity &amp;\nwait\n</code></pre>"},{"location":"polaris/running-jobs/#compute-node-access-to-the-internet","title":"Compute Node Access to the Internet","text":"<p>Currently, the only access the internet is via a proxy.  Here are the proxy environment variables for Polaris:</p> <pre><code>export http_proxy=\"http://proxy-01.pub.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy-01.pub.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n</code></pre> <p>In the future, though we don't have a timeline on this because it depends on future features in slingshot and internal software development, we intend to have public IP addresses be a schedulable resource.  For instance, if only your head node needed public access your select statement might looks something like: <code>-l select=1:pubnet=True+63</code>.</p>"},{"location":"polaris/running-jobs/#controlling-where-your-job-runs","title":"Controlling Where Your Job Runs","text":"<p>If you wish to have your job run on specific nodes form your select like this: <code>-l select=1:vnode=&lt;node name1&gt;+1:vnode=&lt;node name2&gt;...</code> . Obviously, that gets tedious for large jobs.</p> <p>If you want to control the location of a few nodes, for example 2 out of 64, but the rest don't matter, you can do something like this: <code>-l select=1:vnode=&lt;node name1&gt;+1:vnode=&lt;node name2&gt;+62:system=foo</code></p> <p>Every node has a PBS resource called <code>tier0</code> with a rack identifier and <code>tier1</code> with a dragonfly group identifieer.  If you want all your nodes grouped in a rack, you can add the group specifier <code>-l select=8:system=foo,place=scatter:group=tier0</code>.  If you wanted everything in the same dragonfly group, replace <code>tier0</code> with <code>tier1</code>.  Note that you have to also explicitly specify the place when you use group.  If you wanted a specific rack or dragonfly group instead of any of them, you are back to the select: <code>-l select 10:tier0=x3001-g0</code>.</p>"},{"location":"polaris/running-jobs/#network-rack-and-dragonfly-group-mappings","title":"Network: Rack and Dragonfly Group Mappings","text":"<ul> <li>Racks contain (7) 6U chassis; each chassis has 2 nodes for 14 nodes per rack</li> <li>The hostnames are of the form xRRPPc0sUUb[0|1]n0 where:<ul> <li>RR is the row {30, 31, 32}</li> <li>PP is the position in the row {30 goes 1-16, 31 and 32 go 1-12}</li> <li>c is chassis and is always 0</li> <li>s stands for slot, but in this case is the RU in the rack and values are {1,7,13,19,25,31,37}</li> <li>b is BMC controller and is 0 or 1 (each node has its own BMC)</li> <li>n is node, but is always 0 since there is only one node per BMC</li> </ul> </li> <li>So, 16+12+12 = 40 racks * 14 nodes per rack = 560 nodes.</li> <li>Note that in production group 9 (the last 4 racks) will be the designated on-demand racks</li> <li>The management racks are x3000 and X3100 and are dragonfly group 10</li> <li>The TDS rack is x3200 and is dragonfly group 11</li> <li>Each compute node will have a PBS resource named <code>tier0</code> which will be equal to the values in the table below.  This allows you to group your jobs within a rack if you wish.  There is also a resource called <code>tier1</code> which will be equal to the column headings.  This allows you to group your jobs within a dragonfly group if you wish.</li> </ul> g0 g1 g2 g3 g4 g5 g6 g7 g8 g9 x3001-g0 x3005-g1 x3009-g2 x3013-g3 x3101-g4 x3105-g5 x3109-g6 x3201-g7 x3205-g8 x3209-g9 x3002-g0 x3006-g1 x3010-g2 x3014-g3 x3102-g4 x3106-g5 x3110-g6 x3202-g7 x3206-g8 x3210-g9 x3003-g0 x3007-g1 x3011-g2 x3015-g3 x3103-g4 x3107-g5 x3111-g6 x3203-g7 x3207-g8 x3211-g9 x3004-g0 x3008-g1 x3012-g2 x3016-g3 x3104-g4 x3108-g5 x3112-g6 x3204-g7 x3208-g8 x3212-g9"},{"location":"polaris/applications-and-libraries/applications/gromacs/","title":"Gromacs on Polaris","text":""},{"location":"polaris/applications-and-libraries/applications/gromacs/#what-is-gromacs","title":"What is Gromacs?","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p>"},{"location":"polaris/applications-and-libraries/applications/gromacs/#using-gromacs-at-alcf","title":"Using GROMACS at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for GROMACS. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/gromacs/#building-gromacs","title":"Building Gromacs","text":"<ol> <li>Download latest source code: http://manual.gromacs.org/documentation/2022.1/download.html</li> <li>tar -xzf gromacs-2022.1.tar.gz</li> <li>module swap PrgEnv-nvhpc PrgEnv-gnu</li> <li>module load cudatoolkit-standalone/11.2.2</li> <li>module load gcc/10.3.0</li> <li>module load cmake</li> <li>cd gromacs-2022.1</li> <li>mkdir build</li> <li><pre><code>cmake -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC \\\n      -DBUILD_SHARED_LIBS=OFF -DGMX_BUILD_OWN_FFTW=ON \\\n      -DCMAKE_INSTALL_PREFIX=/path-to/gromacs-2022.1/build \\\n      -DGMX_MPI=ON -DGMX_OPENMP=ON -DGMX_GPU=CUDA \\\n      -DCUDA_TOOLKIT_ROOT_DIR=/soft/compilers/cudatoolkit/cuda-11.2.2\n</code></pre></li> <li>make \u2013j 8</li> <li>make install</li> <li>The installed binary is <code>build/bin/gmx_mpi</code>.</li> </ol>"},{"location":"polaris/applications-and-libraries/applications/gromacs/#running-gromacs-on-polaris","title":"Running Gromacs on Polaris","text":"<p>Prebuilt Gromacs binaries can be found in the directory <code>/soft/applications/Gromacs/gromacs-2022.1</code>.</p> <p>A sample pbs script follows that will run GROMACS on two nodes, using 4 MPI ranks per node, and each rank with four OpenMP threads. The PME kernel owns one MPI rank and one GPU per node, while the nonbonded kernel uses 3 MPI ranks and 3 GPUs per node.</p> <pre><code>#!/bin/sh\n#PBS -l select=2:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A PROJECT\n#PBS -l filesystems=home:grand:eagle\n\ncd ${PBS_O_WORKDIR}\n\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule load cudatoolkit-standalone/11.2.2\n\nexport OMP_NUM_THREADS=4\n\nmpirun --np 8 /soft/applications/Gromacs/gromacs-2022.1/gmx_mpi \\\n      mdrun -gputasks 0123 -nb gpu -pme gpu -npme 1 -ntomp 4 \\\n      -dlb yes -resethway -pin on -v deffnm step5_1 -g test.log\n</code></pre> <p>We strongly suggest that users try combinations of different numbers of nodes, MPI ranks per node, number of GPU tasks/devices, GPU task decomposition between nonbonded and PME kernels, and OMP threads per rank to find the optimal throughput for their particular workload.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/","title":"LAMMPS","text":""},{"location":"polaris/applications-and-libraries/applications/lammps/#overview","title":"Overview","text":"<p>LAMMPS is a general-purpose molecular dynamics software package for massively parallel computers. It is written in an exceptionally clean style that makes it one of the more popular codes for users to extend and it currently has dozens of user-developed extensions.</p> <p>For details bout the code and its usage, see the LAMMPS home page. This page provides information specific to running on Polaris at the ALCF.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#using-lammps-at-alcf","title":"Using LAMMPS at ALCF","text":"<p>ALCF provides assistanc with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries (upon request). A collection of Makefiles and submission scripts are available in the ALCF GettingStarted repo here. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#how-to-obtain-the-code","title":"How to Obtain the Code","text":"<p>LAMMPS is an open-source code, which can be downloaded from the LAMMPS website.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#building-on-polaris-using-kokkos-package","title":"Building on Polaris using KOKKOS package","text":"<p>After LAMMPS has been downloaded and unpacked an ALCF filesystem, users should see a directory whose name is of the form <code>lammps-&lt;version&gt;</code>. One should then see the Makefile <code>lammps-&lt;version&gt;/src/MAKE/MACHINES/Makefile.polaris</code> in recent versions that can be used for compilation on Polaris. A copy of the Makefile is also available in the ALCF GettingStarted repo here. For older versions of LAMMPS, you may need to take an existing Makefile (e.g. Makefile.mpi) for your specific version of LAMMPS used and edit the top portion appropratiately to create a new Makefile.polaris files.</p> <p>The top portion of <code>Makefile.polaris_kokkos_nvidia</code> used to build LAMMPS with the KOKKOS package using the NVIDIA compilers is shown as an example.</p> <pre><code># polaris_nvidia = Flags for NVIDIA A100, NVIDIA Compiler, Cray MPICH, CUDA\n# module load craype-accel-nvidia80\n# make polaris_kokkos_nvidia -j 16\n\nSHELL = /bin/sh\n\n# ---------------------------------------------------------------------\n# compiler/linker settings\n# specify flags and libraries needed for your compiler\n\nKOKKOS_DEVICES = Cuda,OpenMP\nKOKKOS_ARCH = Ampere80\nKOKKOS_ABSOLUTE_PATH = $(shell cd $(KOKKOS_PATH); pwd)\nexport NVCC_WRAPPER_DEFAULT_COMPILER = nvc++\n\nCRAY_INC = $(shell CC --cray-print-opts=cflags)\nCRAY_LIB = $(shell CC --cray-print-opts=libs)\n\nCC =        $(KOKKOS_ABSOLUTE_PATH)/bin/nvcc_wrapper\nCCFLAGS =  -g -O3 -mp -DLAMMPS_MEMALIGN=64 -DLAMMPS_BIGBIG\nCCFLAGS += $(CRAY_INC)\nSHFLAGS =   -fPIC\nDEPFLAGS =  -M\n\nLINK =      $(CC)\nLINKFLAGS = $(CCFLAGS)\nLIB = $(CRAY_LIB)\nSIZE =      size\n</code></pre> <p>With the appropriate LAMMPS Makefile in place an executable can be compiled as in the following example, which uses the NVIDIA compilers.</p> <pre><code>module load craype-accel-nvidia80\ncd lammps-&lt;version&gt;/src\nmake yes-KOKKOS\nmake polaris_kokkos_nvidia -j 16\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/lammps/#running-jobs-on-polaris","title":"Running Jobs on Polaris","text":"<p>An example submission script for running a KOKKOS-enabled LAMMPS executable is below as an example. Additional information on LAMMPS application flags and options is described on the LAMMPS website.</p> <pre><code>#!/bin/sh\n#PBS -l select=64:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:15:00\n#PBS -l filesystems=home:grand:eagle\n#PBS -q prod\n#PBS -A Catalyst\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n\n# per-node settings\nNRANKS=4\nNRANKSSOCKET=2\nNDEPTH=8\nNTHREADS=1\nNGPUS=4\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\nEXE=/home/knight/bin/lammps_polaris_kokkos_nvidia\nEXE_ARG=\"-in in.reaxc.hns -k on g ${NGPUS} -sf kk -pk kokkos neigh half neigh/qeq full newton on \"\n\n# OMP settings mostly to quiet Kokkos messages\n\nMPI_ARG=\"-n ${NTOTRANKS} --ppn ${NRANKS} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} --env OMP_PROC_BIND=spread --env OMP_PLACES=cores\"\n\nCOMMAND=\"mpiexec ${MPI_ARG} ${EXE} ${EXE_ARG}\"\necho \"COMMAND= ${COMMAND}\"\n${COMMAND}\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/lammps/#performance-notes","title":"Performance Notes","text":"<p>Some useful information on accelerator packages and expectations can be found on the LAMMPS website here.</p>"},{"location":"polaris/applications-and-libraries/applications/openmm/","title":"OpenMM on Polaris","text":""},{"location":"polaris/applications-and-libraries/applications/openmm/#what-is-openmm","title":"What is OpenMM?","text":"<p>OpenMM is a high-performance toolkit for molecular simulations that can be used as a stand-alone application or as a library. It provides a combination of flexibility (through custom forces and integrators), openness, and high-performance (especially on recent GPUs). </p>"},{"location":"polaris/applications-and-libraries/applications/openmm/#using-openmm-at-alcf","title":"Using OpenMM at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for OpenMM. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/openmm/#building-openmm-using-conda-module","title":"Building OpenMM using Conda module","text":"<ol> <li>Update environment <pre><code>$ module load conda/2022-07-19\n</code></pre></li> <li>Install OpenMM <pre><code>$ mkdir conda\n$ conda create --prefix /path-to/conda/openmm_env\n$ conda activate /path-to/conda/openmm_env\n$ conda install -c conda-forge openmm cudatoolkit=11.4\n$ conda deactivate /path-to/conda/openmm_env\n</code></pre></li> <li> <p>Validate installation: if successful, then info on code version, platform types, CUDA initialization, and force error tolerance will be shown.</p> <pre><code>$ cd /path-to/conda/openmm_env/share/openmm/examples\n$ python -m openmm.testInstallation\n</code></pre> </li> <li> <p>Benchmark testing using PBS job script below.</p> <pre><code>$ cd /path-to/conda/openmm_env/share/openmm/examples\n$ qsub ./submit.sh\n</code></pre> </li> </ol>"},{"location":"polaris/applications-and-libraries/applications/openmm/#running-openmm-benchmark-on-polaris","title":"Running OpenMM Benchmark on Polaris","text":"<p>A sample pbs script follows that will run OpenMM benchmark on one node.</p> <pre><code>#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A PROJECT\n#PBS -l filesystems=home:grand:eagle\n\ncd ${PBS_O_WORKDIR}\n\nmodule load cudatoolkit-standalone/11.4.4\n\npython benchmark.py --platform=CUDA --test=pme --precision=mixed --seconds=30 --heavy-hydrogens &gt; test.output\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/openmm/#building-openmm-from-source","title":"Building OpenMM from Source","text":"<ol> <li>Update environment <pre><code>$ module load cudatoolkit-standalone/11.4.4\n$ module load cray-python/3.9.12.1\n</code></pre></li> <li>Download OpenMM <pre><code>$ git checkout https://github.com/openmm/openmm.git\n$ cd openmm ; mkdir build\n</code></pre></li> <li>Download and build doxygen <pre><code>$ git clone https://github.com/doxygen/doxygen.git\n$ cd doxygen ; cmake ; make ; make install ; cd ../\n</code></pre></li> <li>Download and install swig in OpenMM directory. <pre><code>$ tar xzf swig-4.0.2.tar.gz\n$ cd swig-4.0.2\n$ ./configure --prefix=/path-to/openmm/swig-4.0.2 ; make -j 8 ; make install\n</code></pre></li> <li>Build OpenMM <pre><code>$ cmake -DDOXYGEN_EXECUTABLE=/path-to/openmm/doxygen/bin/doxygen \\\n        -DSWIG_EXECUTABLE=/path-to/openmm/swig-4.0.2/bin/swig \\\n        -DCMAKE_INSTALL_PREFIX=/path-to/openmm/build \\\n         -DCUDA_HOME=/soft/compilers/cudatoolkit/cuda-11.4.4 \\\n         -DCUDA_INCLUDE_DIR=/soft/compilers/cudatoolkit/cuda-11.4.4/include \\\n         -DCUDA_LIB_DIR=/soft/compilers/cudatoolkit/cuda-11.4.4/lib64\n$ make -j 8\n$ make install\n</code></pre></li> <li> <p>Validate installation: if successful, then info on code version, platform types, CUDA initialization, and force error tolerance will be shown. </p> <pre><code>$ cd /path-to/openmm/examples\n$ python -m openmm.testInstallation\n</code></pre> </li> <li> <p>Benchmark testing using the PBS job script above.</p> <pre><code>$ cd /path-to/openmm/examples\n$ qsub ./submit.sh\n</code></pre> </li> </ol>"},{"location":"polaris/applications-and-libraries/applications/vasp/","title":"VASP","text":""},{"location":"polaris/applications-and-libraries/applications/vasp/#vasp-6xx-in-polaris-nvhpcopenaccopenmpcuda-mathcraympi","title":"VASP 6.x.x in Polaris (NVHPC+OpenACC+OpenMP+CUDA math+CrayMPI)","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a software package for performing electronic structure calculations with periodic boundary conditions. It is most commonly used that to perform density functional theory (DFT) calculations in a planewave basis using the projector augemented wave (PAW) method. A more complete description of VASP can be found here.</p> <p>Users must have a license to use this code on ALCF systems. More information on how to get access to VASP binaries can be found here.</p>"},{"location":"polaris/applications-and-libraries/applications/vasp/#general-compilinginstalling-instructions-provided-by-vasp-support","title":"General compiling/installing instructions provided by VASP support","text":"<p>Instructions and samples of <code>makefile.include</code> could be found in the <code>vasp.at</code> wiki page.</p> <p>The follow <code>makefile.include</code> was tailored for Polaris, originally taken from here.</p> <pre><code># Precompiler options\nCPP_OPTIONS = -DHOST=\\\"LinuxNV\\\" \\\n-DMPI -DMPI_BLOCK=8000 -Duse_collective \\\n-DscaLAPACK \\\n-DCACHE_SIZE=4000 \\\n-Davoidalloc \\\n-Dvasp6 \\\n-Duse_bse_te \\\n-Dtbdyn \\\n-Dqd_emulate \\\n-Dfock_dblbuf \\\n-D_OPENMP \\\n-D_OPENACC \\\n-DUSENCCL -DUSENCCLP2P\\\nCPP        = nvfortran -Mpreprocess -Mfree -Mextend -E $(CPP_OPTIONS) $*$(FUFFIX)  &gt; $*$(SUFFIX)\nFC         = ftn -acc -gpu=cc80 -mp -target-accel=nvidia80\nFCL        = ftn -acc -gpu=cc80 -c++libs -target-accel=nvidia80\n\nFREE       = -Mfree\n\nFFLAGS     = -Mbackslash -Mlarge_arrays\n\nOFLAG      = -fast\n\nDEBUG      = -Mfree -O0 -traceback\n\n# Specify your NV HPC-SDK installation, try to set NVROOT automatically\nNVROOT     =$(shell which nvfortran | awk -F /compilers/bin/nvfortran '{ print $$1 }')\n# ...or set NVROOT manually\nNVHPC      ?= /opt/nvidia/hpc_sdk\nNVVERSION  = 20.9\n#NVROOT     = $(NVHPC)/Linux_x86_64/$(NVVERSION)\n# Use NV HPC-SDK provided BLAS and LAPACK libraries\nLIBAOCL=/soft/libraries/aocl/3.2.0\nBLAS       = ${LIBAOCL}/lib/libblis-mt.a\nLAPACK     = ${LIBAOCL}/lib/libflame.a\n\nBLACS      =\nSCALAPACK  =\n#SCALAPACK  = -Mscalapack\n#SCALAPACK  = ${LIBAOCL}/lib/libscalapack.a\nCUDA       = -cudalib=cublas,cusolver,cufft,nccl -cuda\n\nLLIBS      = $(SCALAPACK) $(LAPACK) $(BLAS) $(CUDA)\n# Software emulation of quadruple precsion\nQD         ?= $(NVROOT)/compilers/extras/qd\nLLIBS      += -L$(QD)/lib -lqdmod -lqd\nINCS       += -I$(QD)/include/qd\n\n#INCS       += -I/usr/include/linux \n#INCS       += -I/usr/include/c++/7/tr1 \n#INCS       += -I/usr/include/c++/7 \n#INCS       += -I/usr/include/x86_64-linux-gnu/c++/7\n#INCS       += -I/lus/theta-fs0/software/spack/spack-dev/opt/spack/linux-sles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/lib/gcc/x86_64-pc-linux-gnu/10.2.0/include/\n# Use the FFTs from fftw\nFFTW       ?= ${LIBAOCL}\nLLIBS      += -L$(FFTW)/lib -lfftw3 -lfftw3_omp -lomp\n#INCS       += -I/soft/libraries/aocl/3.2.0/include_LP64/\nINCS       += -I$(FFTW)/include\n\nOBJECTS    = fftmpiw.o fftmpi_map.o fftw3d.o fft3dlib.o\n\n# Redefine the standard list of O1 and O2 objects\nSOURCE_O1  := pade_fit.o\nSOURCE_O2  := pead.o\n\n# For what used to be vasp.5.lib\nCPP_LIB    = $(CPP)\nFC_LIB     = nvfortran\nCC_LIB     = cc\nCFLAGS_LIB = -O $(INCS) -c++libs -cuda\nFFLAGS_LIB = -O1 -Mfixed\nFREE_LIB   = $(FREE)\nOBJECTS_LIB= linpack_double.o getshmem.o\n\n# For the parser library\n#CXX_PARS   = nvc++ --no_warnings -I/lus/theta-fs0/software/spack/spack-dev/opt/spack/linux-sles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/include/c++/10.2.0/ -I/lus/theta-fs0/software/spack/spack-dev/opt/spack/linux-s\nles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/include/c++/10.2.0/x86_64-pc-linux-gnu -I/lus/theta-fs0/software/spack/spack-dev/opt/spack/linux-sles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/lib/gcc\n/x86_64-pc-linux-gnu/10.2.0/include -I/lus/theta-fs0/software/spack/spack-dev/opt/spack/linux-sles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/lib/gcc/x86_64-pc-linux-gnu/10.2.0/include-fixed/\nCXX_PARS   = nvc++ --no_warnings # Normally no need to change this\nSRCDIR     = ../../src\nBINDIR     = ../../bin\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#setting-up-compiler-and-libraries-with-module","title":"Setting up compiler and libraries with <code>module</code>","text":"<p>The follow modules will update the include and libraries paths used by the Cray compiler wrapper <code>ftn</code> to load additional math libraries for the CPU.</p> <pre><code>module purge\nmodule load PrgEnv-nvhpc\nmodule load cray-libsci\nmodule load craype-accel-nvidia8\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#compiling-vasp","title":"Compiling VASP","text":"<p>Once the <code>modules</code> are loaded and a <code>makefile.include</code> is in the <code>vasp</code> folder, compiling all the object files and binaries is done with:</p> <pre><code>make -j1\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#running-vasp-in-polaris","title":"Running VASP in Polaris","text":"<p>An example of a submission script could be found here <code>/soft/applications/vasp/submit-polaris2023-2.sh</code> , which would looks something similar to:</p> <pre><code>#!/bin/sh\n#PBS -l select=1:system=polaris  \n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:grand:eagle\n#PBS -q debug\n#PBS -A Catalyst\n\nmodule load PrgEnv-nvhpc\nmodule load cray-libsci\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=2\nNDEPTH=4\nNTHREADS=4\nNGPUS=2\nNTOTRANKS=$(( NNODES * NRANKS ))\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS} --depth ${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} /path_to_vasp/bin/vasp_std\n</code></pre> <p>Submission scripts should have executable attibutes to be used with <code>qsub</code> script mode.</p> <pre><code>chmod +x example-script.sh\nqsub  example-script.sh\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#known-issues-versions-640-in-polaris","title":"Known issues versions: &gt;= 6.4.0 in Polaris","text":"<ul> <li>Undefined <code>MPIX_Query_cuda_support</code> function at linking binary: This function is called in <code>src/openacc.F</code>. The  <code>MPIX_Query_cuda_support</code> is not included in<code>cray-mpich</code>. One workaround to this issue is to comment this function call. See the follow suggested changes marked by <code>!!!!!CHANGE HERE</code> in the <code>file:src/openacc.F</code></li> </ul> <pre><code>+!!!!!CHANGE HERE \n-      INTERFACE\n-        INTEGER(c_int) FUNCTION MPIX_Query_cuda_support() BIND(C, name=\"MPIX_Query_cuda_support\")\n-        END FUNCTION\n-      END INTERFACE\nCHARACTER(LEN=1) :: ENVVAR_VALUE\nINTEGER :: ENVVAR_STAT\n! This should tell us if MPI is CUDA-aware\n+!!!!!CHANGE HERE \n-       CUDA_AWARE_SUPPORT = MPIX_Query_cuda_support() == 1\n+       CUDA_AWARE_SUPPORT = .TRUE.\n! However, for OpenMPI some env variables can still deactivate it even though the previous\n! check was positive\nCALL GET_ENVIRONMENT_VARIABLE(\"OMPI_MCA_mpi_cuda_support\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\nIF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\nCALL GET_ENVIRONMENT_VARIABLE(\"OMPI_MCA_opal_cuda_support\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\nIF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n! Just in case we might be non-OpenMPI, and their MPIX_Query_cuda_support behaves similarly\nCALL GET_ENVIRONMENT_VARIABLE(\"MV2_USE_CUDA\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\nIF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\nCALL GET_ENVIRONMENT_VARIABLE(\"MPICH_RDMA_ENABLED_CUDA\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\nIF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\nCALL GET_ENVIRONMENT_VARIABLE(\"PMPI_GPU_AWARE\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\nIF (ENVVAR_STAT==0) CUDA_AWARE_SUPPORT =(ENVVAR_VALUE == '1')\n+!!!!!CHANGE HERE \n+       CALL GET_ENVIRONMENT_VARIABLE(\"MPICH_GPU_SUPPORT_ENABLED\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n+       IF (ENVVAR_STAT==0) CUDA_AWARE_SUPPORT =(ENVVAR_VALUE == '1')\n</code></pre>"},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/","title":"Cabana","text":""},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/#cabana_1","title":"Cabana","text":"<p>Cabana is built atop Kokkos. It provides class templates useful for implementing particle codes</p>"},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/#cabana-documentation","title":"Cabana Documentation","text":"<ul> <li>Cabana Wiki</li> <li>Cabana github</li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/#cabana-on-polaris","title":"Cabana on Polaris","text":"<p>Built against the prebuilt Kokkos on polaris, the prebuilt Cabana includes 3 backends: Serial and OpenMP for CPU execution and CUDA for GPU execution. To use it, run</p> <pre><code>module use /soft/modulefiles\nmodule load cabana\n</code></pre> <p>Currently, Cabana is a headers-only installation; there are no libraries per se.</p>"},{"location":"polaris/applications-and-libraries/libraries/math-libraries/","title":"Math Libraries","text":""},{"location":"polaris/applications-and-libraries/libraries/math-libraries/#blas-lapack-and-scalapack-for-cpus","title":"BLAS, LAPACK, and ScaLAPACK for CPUs","text":"<p>Some math libraries targeting CPUs are made available as part of the <code>nvhpc</code> modules and are based on the OpenBLAS project. Additional documentation is available from NVIDIA. </p> <ul> <li>BLAS &amp; LAPACK can be found in the <code>$NVIDIA_PATH/compilers/lib</code> directory.</li> <li>ScaLAPACK can be found in the <code>$NVIDIA_PATH/comm_libs</code> directory.</li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/math-libraries/#nvidia-math-libraries-for-gpus","title":"NVIDIA Math Libraries for GPUs","text":"<p>Math libraries from NVIDIA are made available via the <code>nvhpc</code> modules. Many of the libraries users typically use can be found in the <code>$NVIDIA_PATH/math_libs</code> directory. Some examples follow and additional documentation is available from NVIDIA.</p> <ul> <li>libcublas</li> <li>libcufft</li> <li>libcurand</li> <li>libcusolver</li> <li>libcusparse</li> </ul>"},{"location":"polaris/build-tools/cmake-polaris/","title":"CMake","text":""},{"location":"polaris/build-tools/cmake-polaris/#cmake_1","title":"CMake","text":"<p>CMake is a build configuration system that uses higher-level description files to automatically generate Makefiles.</p>"},{"location":"polaris/build-tools/cmake-polaris/#cmake-documentation","title":"CMake Documentation","text":"<ul> <li>CMake website</li> </ul>"},{"location":"polaris/build-tools/cmake-polaris/#cmake-on-polaris","title":"CMake on Polaris","text":"<p>To use CMake on Polaris, run</p> <pre><code>module use /soft/modulefiles\nmodule load cmake\n</code></pre>"},{"location":"polaris/compiling-and-linking/cce-compilers-polaris/","title":"CCE Compilers on Polaris","text":"<p>The Cray Compiling Environment (CCE) compilers are available on Polaris via the <code>PrgEnv-cray</code> module. </p> <p>The CCE compilers currently on Polaris only support AMD GPU targets for HIP and are thus not usable with the A100 GPUs. </p> <p>The <code>nvhpc</code> and <code>llvm</code> compilers can be used for compiling GPU-enabled applications.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/","title":"Compiling and Linking Overview on Polaris","text":""},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#compiling-on-polaris-login-and-compute-nodes","title":"Compiling on Polaris Login and Compute Nodes","text":"<p>If your build system does not require GPUs for the build process, as is usually the case, compilation of GPU-accelerated codes is generally expected to work well on the Polaris login nodes. If your build system does require GPUs, you cannot yet compile on the Polaris login nodes, as they do not currently have GPUs installed. You may in this case compile your applications on the Polaris compute nodes. Do this by submitting an interactive single-node job, or running your build system in a batch job.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#home-file-system","title":"Home File System","text":"<p>Is it helpful to realize that there is a single <code>HOME</code> filesystem for users that can be accessed from the login and computes of each production resource at ALCF. Thus, users should be mindful of modifications to their environments (e.g. <code>.bashrc</code>) that may cause issues to arise due to differences between the systems. </p> <p>An example is creating an alias for the <code>qstat</code> command to, for example, change the order of columns printed to screen. Users with such an alias that works well on Theta may run into issues using <code>qstat</code> on Polaris as the two system use different schedulers: Cobalt (Theta) and PBS (Polaris). Users with such modifications to their environments are encouraged to modify their scripts appropriately depending on <code>$hostname</code>.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#cray-programming-environment","title":"Cray Programming Environment","text":"<p>The Cray Programming Environment (PE) uses three compiler wrappers for building software. These compiler wrappers should be used when building MPI-enabled applications.</p> <ul> <li><code>cc</code> - C compiler</li> <li><code>CC</code> - C++ compiler</li> <li><code>ftn</code> - Fortran compiler</li> </ul> <p>Each of these wrappers can select a specific vendor compiler based on the PrgEnv module loaded in the environment. The following are some helpful options to understand what the compiler wrapper is invoking.</p> <ul> <li><code>--craype-verbose</code> : Print the command which is forwarded to the compiler invocation</li> <li><code>--cray-print-opts=libs</code> : Print library information</li> <li><code>--cray-print-opts=cflags</code> : Print include information</li> </ul> <p>The output from these commands may be useful in build scripts where a compiler other than that invoked by a compiler wrapper is desired. Defining some variables as such may prove useful in those situations. <pre><code>CRAY_CFLAGS=$(cc --cray-print-opts=cflags)\nCRAY_LIB=$(cc --cray-print-opts=libs)\n</code></pre> Further documentation and options are available via <code>man cc</code> and similar. </p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#compilers-provided-by-cray-programming-environments","title":"Compilers provided by Cray Programming Environments","text":"<p>The default programming environment on Polaris is currently <code>NVHPC</code>. The <code>GNU</code> compilers are available via another programming environment. The following sequence of <code>module</code> commands can be used to switch to the <code>GNU</code> programming environment (gcc, g++, gfortran) and also have <code>NVIDIA</code> compilers available in your path.</p> <pre><code>module swap PrgEnv-nvhpc PrgEnv-gnu\nmodule load nvhpc-mixed\n</code></pre> <p>The compilers invoked by the Cray MPI wrappers are listed for each programming environment in the following table.</p> module C C++ Fortran MPI Compiler Wrapper cc CC ftn PrgEnv-nvhpc nvc nvc++ nvfortran PrgEnv-gnu gcc g++ gfortran <p>Note, while gcc and g++ may be available in the default environment, the <code>PrgEnv-gnu</code> module is needed to provide gfortran.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#additional-compilers-provided-by-alcf","title":"Additional Compilers Provided by ALCF","text":"<p>The ALCF additionally provides compilers to enable the OpenMP and SYCL programming models for GPUs via<code>LLVM</code> as documented here</p> <p>Additional documentation for using compilers is available on the respective programming model pages: OpenMP and SYCL.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#linking","title":"Linking","text":"<p>Dynamic linking of libraries is currently the default on Polaris. The Cray MPI wrappers will handle this automatically.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#notes-on-default-modules","title":"Notes on Default Modules","text":"<ul> <li> <p><code>craype-x86-rome</code>: While the Polaris compute nodes currently have Milan CPUs, this module is loaded by default to avoid the <code>craype-x86-milan</code> module from adding a <code>zen3</code> target not supported in the default <code>nvhpc/21.9</code> compilers. The <code>craype-x86-milan</code> module is expected to be made default once a newer <code>nvhpc</code> version (e.g. 22.5) is made the default.</p> </li> <li> <p><code>craype-accel-nvidia80</code>: This module adds compiler flags to enable GPU acceleration for <code>NVHPC</code> compilers along with gpu-enabled MPI libraries as it is assumed that the majority of applications to be compiled on Polaris will target the GPUs for acceleration. Users building cpu-only applications may find it useful to unload this module to silence \"gpu code generation\" warnings.</p> </li> </ul>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#mixed-cc-fortran-applications","title":"Mixed C/C++ &amp; Fortran Applications","text":"<p>For applications consisting of a mix of C/C++ and Fortran that also uses MPI, it is suggested that the programming environment chosen for Fortran be used to build the full application because of mpi.mod (and similar) incompatibilities. </p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#compiling-for-gpus","title":"Compiling for GPUs","text":"<p>It is assumed the majority of applications to be built on Polaris will make use of the GPUs. As such, the <code>craype-accel-nvidia80</code> module is in the default environment. This has the effect of the Cray compiler wrappers adding <code>-gpu</code> to the compiler invocation along with additional include paths and libraries. Additional compilers flags may be needed depending on the compiler and GPU programming model used (e.g. <code>-cuda</code>, <code>-acc</code>, or <code>-mp=gpu</code>).</p> <p>This module also adds GPU Transport Layer (GTL) libraries to the link-line to support GPU-aware MPI applications.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#man-pages","title":"Man Pages","text":"<p>For additional information on the Cray wrappers, please refer to the man pages. <pre><code>man cc\nman CC\nman ftn\n</code></pre></p>"},{"location":"polaris/compiling-and-linking/gnu-compilers-polaris/","title":"GNU Compilers on Polaris","text":"<p>The GNU compilers are available on Polaris via the <code>PrgEnv-gnu</code> and <code>gcc-mixed</code> modules. The <code>gcc-mixed</code> module can be useful when, for example, the <code>PrgEnv-nvhpc</code> compilers are used to compile C/C++ MPI-enabled code and gfortran is needed.</p> <p>The GNU compilers currently on Polaris do not support GPU code generation and thus can only be used for compiling CPU codes.</p> <p>The <code>nvhpc</code> and <code>llvm</code> compilers can be used for compiling GPU-enabled applications.</p>"},{"location":"polaris/compiling-and-linking/llvm-compilers-polaris/","title":"LLVM Compilers on Polaris","text":"<p>This page is not about LLVM-based Cray Compiling Environment (CCE) compilers from <code>PrgEnv-cray</code> but about open source LLVM compilers. If LLVM compilers are needed without MPI support, simply load the <code>llvm</code> module.</p> <p>Cray Programming Environment does not offer LLVM compiler support. Thus cc/CC/ftn compiler wrappers using LLVM compilers currently are not available. To use Clang with MPI, one can load the <code>mpiwrappers/cray-mpich-llvm</code> module which loads the following modules.</p> <ul> <li><code>llvm</code>, upstream llvm compilers</li> <li><code>cray-mpich</code>, MPI compiler wrappers mpicc/mpicxx/mpif90. mpif90 uses gfortran because flang is not ready for production use.</li> <li><code>cray-pals</code>, MPI launchers mpiexec/aprun/mpirun</li> </ul> <p>Limitation There is no GPU-aware MPI library linking support by default. If needed, users should manually add the GTL (GPU Transport Layer) library to the application link line.</p>"},{"location":"polaris/compiling-and-linking/llvm-compilers-polaris/#openmp-offload","title":"OpenMP offload","text":"<p>When targeting the OpenMP or CUDA programming models for GPUs, the <code>cudatoolkit-standalone</code> module should also be loaded.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/","title":"NVIDIA Compilers on Polaris","text":"<p>The NVIDIA compilers (<code>nvc</code>, <code>nvc++</code>, <code>nvcc</code>, and <code>nvfortran</code>) are available on Polaris via the <code>PrgEnv-nvhpc</code> and <code>nvhpc</code> modules. There is currently a <code>PrgEnv-nvidia</code> module available, but that will soon be deprecated in Cray's PE, thus it is not recommend for use.</p> <p>The Cray compiler wrappers map to NVIDIA compilers as follows.</p> <pre><code>cc -&gt; nvc\nCC -&gt; nvc++\nftn -&gt; nvfortran\n</code></pre> <p>Users are encouraged to look through NVIDIA's documentation for the NVHPC SDK and specific information on the compilers, tools, and libraries.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#notes-on-nvidia-compilers","title":"Notes on NVIDIA Compilers","text":""},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#pgi-compilers","title":"PGI compilers","text":"<p>The NVIDIA programming environments makes available compilers from the NVIDIA HPC SDK. While the PGI compilers are available in this programming environment, it should be noted they are actually symlinks to the corresponding <code>NVIDIA</code> compilers. <pre><code>pgcc -&gt; nvc\npgc++ -&gt; nvc++\npgf90 -&gt; nvfortran\npgfortran -&gt; nvfortran\n</code></pre> While <code>nvcc</code> is the traditional CUDA C and CUDA C++ compiler for NVIDIA GPUs, the <code>nvc</code>, <code>nvc++</code>, and <code>nvfortran</code> compilers additionally target CPUs.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#nvhpc-sdk-directory-structure","title":"NVHPC SDK Directory Structure","text":"<p>Users migrating from CUDA toolkits to the NVHPC SDK may find it beneficial to review the directory structure of the <code>hpc-sdk</code> directory to find the location of commonly used libraries (including math libraries for the CPU). With the <code>PrgEnv-nvhpc</code> module loaded, the <code>NVIDIA_PATH</code> environment variable can be used to locate the path to various NVIDIA tools, libraries, and examples.</p> <ul> <li><code>compiler/bin</code> - cuda-gdb, ncu, nsys, ...</li> <li><code>examples</code> - CUDA-Fortran, OpenMP, ...</li> <li><code>comm_libs</code> - nccl, nvshmem, ...</li> <li><code>compiler/libs</code> - blas, lapack, ...</li> <li><code>cuda/lib64</code> - cudart, OpenCL, ...</li> <li><code>math_libs/lib64</code> - cublas, cufft, ...</li> </ul>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#differences-between-nvcc-and-nvcnvc","title":"Differences between nvcc and nvc/nvc++","text":"<p>For users that want to continue using <code>nvcc</code> it is important to be mindful of differences with the newer <code>nvc</code> and <code>nvc++</code> compilers. For example, the <code>-cuda</code> flag instructs <code>nvcc</code> to compile <code>.cu</code> input files to <code>.cu.cpp.ii</code> output files which are to be separately compiled, whereas the same <code>-cuda</code> flag instructs <code>nvc</code>, <code>nvc++</code>, and <code>nvfortran</code> to enable CUDA C/C++ or CUDA Fortran code generation. The resulting output file in each case is different (text vs. object) and one may see <code>unrecognized format error</code> when <code>-cuda</code> is incorrectly passed to <code>nvcc</code>.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#known-issues-and-workarounds","title":"Known Issues and Workarounds","text":"<p>If you are using <code>nvcc</code> to invoke <code>nvc++</code> and compiling C++17 code, and are seeing the following warning and unable to compile C++17 constructs:</p> <pre><code>polaris-login-01(~)&gt; nvcc --std=c++17 -ccbin nvc++ ~/smalltests/bool_constant.cpp\nnvcc warning : The -std=c++17 flag is not supported with the configured host compiler. Flag will be ignored.\n\"/home/zippy/smalltests/bool_constant.cpp\", line 10: error: namespace \"std\" has no member class \"bool_constant\"\n      : std::bool_constant&lt;(UnaryPred&lt;Ts&gt;::value || ...)&gt; {};\n             ^\n\n\"/home/zippy/smalltests/bool_constant.cpp\", line 10: error: class or struct definition is missing\n      : std::bool_constant&lt;(UnaryPred&lt;Ts&gt;::value || ...)&gt; {};\n                          ^\n\n2 errors detected in the compilation of \"/home/zippy/smalltests/bool_constant.cpp\".\npolaris-login-01(~)&gt;\n</code></pre> <p>you will need to work around it by loading the latest cudatoolkit module atop PrgEnv-nvhpc:</p> <pre><code>module load cudatoolkit-standalone/11.6.2\n</code></pre>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/","title":"OneAPI Compilers and Support","text":"<p>The Intel OneAPI compiler and Codeplay plugins for Nvidia GPUs are available on Polaris. The oneAPI compilers are not enabled under the Cray Programming Environment system but can be used separately. Two oneAPI variants are provided, the first being a \"release\" version based on Intel's officially released oneAPI toolkit. Intel Release Notes</p> <p>Note</p> <p>The 2023.1 release of oneAPI Toolkit does not support oneMKL or oneDPL on Nvidia.</p> <p>The other variant being a build of main from the open source. This variant will be more up-to-date at the risk of bugs and breakages based on code that has not undergone a full release cycle. The documentation is located on the SYCL page.</p>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#compile-and-link","title":"Compile and Link","text":"<p>OneAPI uses the clang (or icx/icpx wrapper) for compiling and linking for the Nvidia A100 SM80 architecture.</p> <pre><code>module load PrgEnv-nvhpc\nmodule use /soft/compilers/oneapi/release/modulefiles\nmodule load compiler\nicpx -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda  -Xsycl-target-backend --cuda-gpu-arch=sm_80 test.cpp\n</code></pre> <pre><code>harms@polaris-login-04:~/working/polaris/oneapi&gt; icpx --version\nIntel(R) oneAPI DPC++/C++ Compiler 2023.1.0 (2023.1.0.20230320)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /soft/compilers/oneapi/release/2023.1/compiler/2023.1.0/linux/bin-llvm\nConfiguration file: /soft/compilers/oneapi/release/2023.1/compiler/2023.1.0/linux/bin-llvm/../bin/icpx.cfg\n</code></pre>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#running","title":"Running","text":"<p>The library should select the GPU by default, but selection of the GPUs can be forced via the ONEAPI_DEVICE_SELECTOR <pre><code>$ ONEAPI_DEVICE_SELECTOR=ext_oneapi_cuda:gpu ./a.out\n</code></pre> or a specific GPU. <pre><code>$ ONEAPI_DEVICE_SELECTOR=ext_oneapi_cuda:gpu:3 ./a.out\n</code></pre></p>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#sycl-ls","title":"sycl-ls","text":"<p>Expected output of sycl-ls and which platforms are available.</p> <pre><code>harms@x3004c0s7b0n0:~&gt; which sycl-ls\n/soft/compilers/oneapi/release/2023.1/compiler/2023.1.0/linux/bin/sycl-ls\n\nharms@x3004c0s7b0n0:~&gt; sycl-ls\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2023.15.3.0.20_160000]\n[opencl:cpu:1] Intel(R) OpenCL, AMD EPYC 7543P 32-Core Processor                3.0 [2023.15.3.0.20_160000]\n[ext_oneapi_cuda:gpu:0] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 0.0 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:1] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 0.0 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:2] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 0.0 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:3] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 0.0 [CUDA 11.4]\n</code></pre>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/","title":"Example Programs and Makefiles for Polaris","text":"<p>Several simple examples of building CPU and GPU-enabled codes on Polaris are available in the ALCF GettingStart repo for several programming models. If build your application is problematic for some reason (e.g. absence of a GPU), then users are encouraged to build and test applications directly on one of the Polaris compute nodes via an interactive job. The discussion below makes use of the <code>NVHPC</code> compilers in the default environment as illustrative examples. Similar examples for other compilers on Polaris are available in the ALCF GettingStarted repo.</p>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#cpu-mpiopenmp-example","title":"CPU MPI+OpenMP Example","text":"<p>One of the first useful tasks with any new machine, scheduler, and job launcher is to ensure one is binding MPI ranks and OpenMP threads to the host cpu as intended. A simple HelloWorld MPI+OpenMP example is available here to get started with.</p> <p>The application can be straightforwardly compiled using the Cray compiler wrappers. <pre><code>CC -fopenmp main.cpp -o hello_affinity\n</code></pre></p> <p>The executable <code>hello_affinity</code> can then be launched in a job script (or directly in shell of interactive job) using <code>mpiexec</code> as discussed here.</p> <pre><code>#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home\n\n# MPI example w/ 16 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=16\nNDEPTH=4\nNTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity\n</code></pre>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#cuda","title":"CUDA","text":"<p>Several variants of C/C++ and Fortran CUDA examples are available here that include MPI and multi-gpu examples.</p> <p>One can use the Cray compiler wrappers to compile GPU-enabled applications as well. This example of simple vector addition uses the NVIDIA compilers.</p> <pre><code>CC -g -O3 -std=c++0x -cuda main.cpp -o vecadd\n</code></pre> <p>The <code>craype-accel-nvidia80</code> module in the default environment will add the <code>-gpu</code> compiler flag for <code>nvhpc</code> compilers along with appropriate include directories and libraries. It is left to the user to provide an additional flag to the <code>nvhpc</code> compilers to select the target GPU programming model. In this case, <code>-cuda</code> is used to indicate compilation of CUDA code. The application can then be launched within a batch job submission script or as follows on one of the compute nodes.</p> <pre><code>$ ./vecadd \n# of devices= 4\n  [0] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [1] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [2] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [3] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\nRunning on GPU 0!\nUsing single-precision\n\n  Name= NVIDIA A100-SXM4-40GB\n  Locally unique identifier= \n  Clock Frequency(KHz)= 1410000\n  Compute Mode= 0\n  Major compute capability= 8\n  Minor compute capability= 0\n  Number of multiprocessors on device= 108\n  Warp size in threads= 32\n  Single precision performance ratio= 2\n\nResult is CORRECT!! :)\n</code></pre>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#gpu-openacc","title":"GPU OpenACC","text":"<p>A simple MPI-parallel OpenACC example is available here. Compilation proceeds similar to the above CUDA example except for the use of the <code>-acc=gpu</code> compiler flag to indicate compilation of OpenACC code for GPUs. <pre><code>CC -g -O3 -std=c++0x -acc=gpu -gpu=cc80,cuda11.0 main.cpp -o vecadd\n</code></pre> In this example, each MPI rank sees all four GPUs on a Polaris node and GPUs are bound to MPI ranks round-robin within the application.</p> <p><pre><code>$ mpiexec -n 4 ./vecadd\n# of devices= 4\nUsing single-precision\n\nRank 0 running on GPU 0!\nRank 1 running on GPU 1!\nRank 2 running on GPU 2!\nRank 3 running on GPU 3!\n\nResult is CORRECT!! :)\n</code></pre> If the application instead relies on the job launcher to bind MPI ranks to available GPUs, then a small helper script can be used to explicitly set <code>CUDA_VISIBLE_DEVICES</code> appropriately for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment. The binding of MPI ranks to GPUs is discussed in more detail here.</p>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#gpu-opencl","title":"GPU OpenCL","text":"<p>A simple OpenCL example is available here. The OpenCL headers and library are available in the NVHPC SDK and cuda toolkits. The environment variable <code>NVIDIA_PATH</code> is defined for the <code>PrgEnv-nvhpc</code> programming environment.  <pre><code>CC -o vecadd -g -O3 -std=c++0x  -I${NVIDIA_PATH}/cuda/include main.o -L${NVIDIA_PATH}/cuda/lib64 -lOpenCL\n</code></pre></p> <p>This simple example can be run on a Polaris compute node as follows. <pre><code>$ ./vecadd\nRunning on GPU!\nUsing single-precision\n\n    CL_DEVICE_NAME: NVIDIA A100-SXM4-40GB\n    CL_DEVICE_VERSION: OpenCL 3.0 CUDA\n    CL_DEVICE_OPENCL_C_VERSION: OpenCL C 1.2 \n    CL_DEVICE_MAX_COMPUTE_UNITS: 108\n    CL_DEVICE_MAX_CLOCK_FREQUENCY: 1410\n    CL_DEVICE_MAX_WORK_GROUP_SIZE: 1024\n\nResult is CORRECT!! :)\n</code></pre></p>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#gpu-openmp","title":"GPU OpenMP","text":"<p>A simple MPI-parallel OpenMP example is available here. Compilation proceeds similar to the above examples except for use of the <code>-mp=gpu</code> compiler flag to indicated compilation of OpenMP code for GPUs.</p> <pre><code>CC -g -O3 -std=c++0x -mp=gpu -gpu=cc80,cuda11.0 -c main.cpp -o vecadd\n</code></pre> <p>Similar to the OpenACC example above, this code binds MPI ranks to GPUs in a round-robin fashion.  <pre><code>$ mpiexec -n 4 ./vecadd\n# of devices= 4\nRank 0 running on GPU 0!\nRank 1 running on GPU 1!\nRank 2 running on GPU 2!\nRank 3 running on GPU 3!\n\nResult is CORRECT!! :)\n</code></pre></p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/","title":"Programming Models on Polaris","text":"<p>The software environment on Polaris supports several parallel programming models targeting the CPUs and GPUs.</p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/#cpu-parallel-programming-models","title":"CPU Parallel Programming Models","text":"<p>The Cray compiler wrappers <code>cc</code>, <code>CC</code>, and <code>ftn</code> are recommended for MPI applications as they provide the needed include paths and libraries for each programming environment. A summary of available CPU parallel programming models and relevant compiler flags is shown below. Users are encouraged to review the corresponding man pages and documentation.</p> Programming Model GNU NVHPC LLVM OpenMP -fopenmp -mp -fopenmp OpenACC -- -acc=multicore -- <p>Higher-level programming models such as Kokkos and Raja may also be used for CPU programming.</p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/#gpu-programming-models","title":"GPU Programming Models","text":"<p>A summary of available GPU programming models and relevant compiler flags is shown below for compilers that generate offloadable code. Users are encouraged to review the corresponding man pages and documentation.</p> Programming Model GNU NVHPC LLVM ONEAPI CUDA -- -cuda [-gpu=cuda8.0,cc11.0] -- -- HIP* -- -- -- -- OpenACC -- -acc -- -- OpenCL* -- -- -- -- OpenMP -- -mp=gpu -fopenmp-targets=nvptx64 -- SYCL -- -- -- -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 <p>Note, the <code>llvm</code> and <code>oneapi</code> modules are provided by ALCF to complement the compilers provided by the Cray PE on Polaris.</p> <p>Higher-level programming models such as Kokkos and Raja may also be used for GPU programming.</p> <p>OpenCL is supported, but does not require specific compiler flags per-se as the offloaded kernels are just-in-time compiled. Abstraction programming models, such as Kokkos, can be built on top of some of these programming models (see below).</p> <p>A HIP compiler supporting the A100 GPUs is still to be installed on Polaris.</p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/#mapping-programming-models-to-polaris-modules","title":"Mapping Programming Models to Polaris Modules","text":"<p>The table below offers some suggestions for how to get started setting up your environment on Polaris depending on the programming language and model. Note, mixed C/C++ and Fortran applications should choose the programming environment for the Fortran compiler because of mpi.mod and similar incompatibilities between Fortran-generated files from different compilers. Several simple examples for testing the software environment on Polaris for different programming models are available in the ALCF GettingStart repo.</p> <p>Note, users are encouraged to use <code>PrgEnv-nvhpc</code> instead of <code>PrgEnv-nvidia</code> as the latter will soon be deprecated in Cray's PE. They are otherwise identical pointing to compilers from the same NVIDIA SDK version.</p> Programming Language GPU Programming Model Likely used Modules/Compilers Notes C/C++ CUDA PrgEnv-nvhpc, PrgEnv-gnu, llvm NVIDIA (nvcc, nvc, nvc++) and clang compilers do GPU code generation C/C++ HIP N/A need to install with support for A100 C/C++ Kokkos See CUDA HIP, OpenMP, and SYCL/DPC++ also candidates C/C++ OpenACC PrgEnv-nvhpc C/C++ OpenCL PrgEnv-nvhpc, PrgEnv-gnu, llvm JIT GPU code generation C/C++ OpenMP PrgEnv-nvhpc, llvm C/C++ RAJA See CUDA HIP, OpenMP, and SYCL/DPC++ also candidates C/C++ SYCL/DPC++ llvm-sycl Fortran CUDA PrgEnv-nvhpc NVIDIA compiler (nvfortran) does GPU code generation; <code>gfortran</code> can be loaded via <code>gcc-mixed</code> Fortran HIP N/A need to install with support for A100 Fortran OpenACC PrgEnv-nvhpc Fortran OpenCL PrgEnv-nvhpc, PrgEnv-gnu JIT GPU code generation Fortran OpenMP PrgEnv-nvhpc"},{"location":"polaris/data-science-workflows/julia/","title":"Julia","text":"<p>Julia is a high-level, high-performance dynamic programming language for technical computing. It has a syntax familiar to users of many other technical computing environments. Designed at MIT to tackle large-scale partial-differential equation simulation and distributed linear algebra, Julia features a robust ecosystem of tools for optimization, statistics, parallel programming, and data visualization. Julia is actively developed by the Julia Labs team at MIT and in industry, along with hundreds of domain-expert scientists and programmers worldwide.</p>"},{"location":"polaris/data-science-workflows/julia/#contributing","title":"Contributing","text":"<p>This guide is a first draft of the Julia documentation for Polaris. If you have any suggestions or contributions, please open a pull request or contact us by opening a ticket at the ALCF Helpdesk.</p>"},{"location":"polaris/data-science-workflows/julia/#julia-installation","title":"Julia Installation","text":"<p>Using the official Julia 1.9 binaries from the Julia webpage is recommended. Juliaup provides a convenient way to install Julia and manage the various Julia versions.</p> <pre><code>curl -fsSL https://install.julialang.org | sh\n</code></pre> <p>You may then list the available Julia versions with <code>juliaup list</code> and install a specific version with <code>juliaup install &lt;version&gt;</code>. You can then activate a specific version with <code>juliaup use &lt;version&gt;</code> and set the default version with <code>juliaup default &lt;version&gt;</code>. <code>juliaup update</code> will update the installed Julia versions. In general, the latest stable release of Julia should be used.</p> <pre><code>juliaup add release\n</code></pre>"},{"location":"polaris/data-science-workflows/julia/#julia-project-environment","title":"Julia Project Environment","text":"<p>The Julia built-in package manager allows you to create a project and enable project-specific dependencies. Julia manages packages in the Julia depot located by default in <code>~/.julia</code>. However, that NFS filesystem is not meant for high-speed access. Therefore, this Julia depot folder should be located on a fast filesystem of your choice (grand, eagle). The Julia depot directory is set via the environment variable <code>JULIA_DEPOT_PATH</code>. For example, you can set the Julia depot to a directory on Polaris grand filesystem by adding the following line to your <code>~/.bashrc</code> file:</p> <pre><code>export /lus/grand/projects/$PROJECT/$USER/julia_depot\n</code></pre>"},{"location":"polaris/data-science-workflows/julia/#programming-julia-on-polaris","title":"Programming Julia on Polaris","text":"<p>There are three key components to using Julia for large-scale computations:</p> <ol> <li>MPI support through MPI.jl</li> <li>GPU support through CUDA.jl</li> <li>HDF5 support through HDF5.jl</li> </ol> <p>In addition, we recommend VSCode with the Julia extension for a modern IDE experience, together with the ssh-remote extension for remote interactive development.</p>"},{"location":"polaris/data-science-workflows/julia/#mpi-support","title":"MPI Support","text":"<p>MPI support is provided through the MPI.jl. <pre><code>julia&gt; ] add MPI\n</code></pre> This will install the MPI.jl package and default MPI prebuilt binaries provided by an artifact. For on-node debugging purposes the default artifact is sufficient. However, for large-scale computations, it is to use the system MPI library that is loaded via <code>module</code>. As of MPI.jl v0.20 this is handled through MPIPrefences.jl. <pre><code>julia --project -e 'using MPIPreferences; MPIPreferences.use_system_binary()'\n</code></pre></p> <p>Check that the correct MPI library is targeted with Julia. <pre><code>julia --project -e 'using MPI; MPI.versioninfo()'\nMPIPreferences:\n  binary:  system\n  abi:     MPICH\n  libmpi:  libmpi_cray\n  mpiexec: mpiexec\n\nPackage versions\n  MPI.jl:             0.20.11\n  MPIPreferences.jl:  0.1.8\n\nLibrary information:\n  libmpi:  libmpi_cray\n  MPI version:  3.1.0\n  Library version:\n    MPI VERSION    : CRAY MPICH version 8.1.16.5 (ANL base 3.4a2)\n    MPI BUILD INFO : Mon Apr 18 12:05 2022 (git hash 4f56723)\n</code></pre> When running on the login node, switch back to the default provided MPI binaries in <code>MPI_jll.jl</code> by removing the <code>LocalPreferences.toml</code> file.</p>"},{"location":"polaris/data-science-workflows/julia/#gpu-support","title":"GPU Support","text":"<p>NVIDIA GPU support is provided through the CUDA.jl package. <pre><code>julia&gt; ] add CUDA\n</code></pre> In case you want write portable GPU kernels we highly recommend the KernelAbstractions.jl package. It provides a high-level abstraction for writing GPU kernels that can be compiled for different GPU backends. <pre><code>julia&gt; ] add KernelAbstractions\n</code></pre> By loading either oneAPI.jl, AMDGPU.jl, or CUDA.jl (see quickstart guide below).</p>"},{"location":"polaris/data-science-workflows/julia/#hdf5-support","title":"HDF5 Support","text":"<p>Parallel HDF5 support is provided by <pre><code>module load cray-hdf5-parallel\n</code></pre> After setting <code>export JULIA_HDF5_PATH=$HDF5_DIR</code> we can install the HDF5.jl package. <pre><code>julia&gt; ] add HDF5\n</code></pre></p>"},{"location":"polaris/data-science-workflows/julia/#quickstart-guide","title":"Quickstart Guide","text":"<p>The following example shows how to use MPI.jl, CUDA.jl, and HDF5.jl to write a parallel program that computes the sum of two vectors on the GPU and writes the result to an HDF5 file. A repository with an example code computing an approximation of pi can be found at Polaris.jl. In this repository, you will also find a <code>setup_polaris.sh</code> script that will build the HDF5.jl and MPI.jl package for the system libraries. The dependencies are installed with the following commands: <pre><code>julia --project\n</code></pre></p> <pre><code>julia&gt; ] up\n</code></pre> <pre><code>using CUDA\nusing HDF5\nusing MPI\nusing Printf\nusing Random\nfunction pi_kernel(x, y, d, n)\nidx = (blockIdx().x-1) * blockDim().x + threadIdx().x\nif idx &lt;= n\nd[idx] = (x[idx] - 0.5)^2 + (y[idx] - 0.5)^2 &lt;= 0.25 ? 1 : 0\nend\nreturn nothing\nend\nfunction approximate_pi_gpu(n::Integer)\nx = CUDA.rand(Float64, n)\ny = CUDA.rand(Float64, n)\nd = CUDA.zeros(Float64, n)\nnblocks = ceil(Int64, n/32)\n@cuda threads=32 blocks=nblocks pi_kernel(x,y,d,n)\nreturn sum(d)\nend\nfunction main()\nn = 100000  # Number of points to generate per rank\nRandom.seed!(1234)  # Set a fixed random seed for reproducibility\ndsum = MPI.Allreduce(approximate_pi_gpu(n), MPI.SUM, MPI.COMM_WORLD)\npi_approx = (4 * dsum) / (n * MPI.Comm_size(MPI.COMM_WORLD))\nif MPI.Comm_rank(MPI.COMM_WORLD) == 0\n@printf \"Approximation of \u03c0 using Monte Carlo method: %.10f\\n\" pi_approx\n@printf \"Error: %.10f\\n\" abs(pi_approx - \u03c0)\nend\nreturn pi_approx\nend\nMPI.Init()\nif !isinteractive()\npi_approx = main()\nh5open(\"pi.h5\", \"w\") do file\nwrite(file, \"pi\", pi_approx)\nend\nend\n</code></pre>"},{"location":"polaris/data-science-workflows/julia/#job-submission-script","title":"Job submission script","text":"<p>This example can be run on Polaris with the following job submission script: <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:grand\n#PBS -q debug\n#PBS -A PROJECT\ncd ${PBS_O_WORKDIR}\n# MPI example w/ 4 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=4\nNDEPTH=8\nNTHREADS=1\nmodule load cray-hdf5-parallel\n# Put in your Julia depot path\nexport JULIA_DEPOT_PATH=MY_JULIA_DEPOT_PATH\n# Path to Julia executable. When using juliaup, it's in your julia_depot folder\nJULIA_PATH=$JULIA_DEPOT_PATH/juliaup/julia-1.9.1+0.x64.linux.gnu/bin/julia\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE=${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth julia --check-bounds=no --project pi.jl\n</code></pre> Verify that <code>JULIA_DEPOT_PATH</code> is set to the correct path and <code>JULIA_PATH</code> points to the Julia executable. When using <code>juliaup</code>, the Julia executable is located in the <code>juliaup</code> folder of your <code>JULIA_DEPOT_PATH</code>.</p>"},{"location":"polaris/data-science-workflows/julia/#advanced-features","title":"Advanced features","text":""},{"location":"polaris/data-science-workflows/julia/#cuda-aware-mpi","title":"CUDA-aware MPI","text":"<p>MPI.jl supports CUDA-aware MPI. This is enabled by setting the following environment variables</p> <pre><code>export JULIA_CUDA_MEMORY_POOL=none\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport JULIA_MPI_PATH=$PATH_TO_CUDA_MPI # /opt/cray/pe/mpich/8.1.16/ofi/nvidia/20.7\nexport JULIA_MPI_HAS_CUDA=1\n</code></pre> <p>Note that <code>MPI.jl</code> needs to be rebuilt for the changes to take effect.</p> <pre><code>julia --project -e 'using Pkg; Pkg.build(\"MPI\"; verbose=true)'\n</code></pre>"},{"location":"polaris/data-science-workflows/julia/#large-scale-parallelism","title":"Large-scale parallelism","text":"<p><code>CUDA.jl</code> uses the <code>nvcc</code> compiler to compile GPU kernels. This will create object files in the <code>TEMP</code> filesystem. Per default, the <code>tempdir</code> is a global directory that can lead to name clashes of the compiled kernel object files. To avoid this, we recommend setting the <code>tempdir</code> to a local directory on the compute node. <pre><code>export TMPDIR=/local/scratch\n</code></pre></p>"},{"location":"polaris/data-science-workflows/python/","title":"Python","text":""},{"location":"polaris/data-science-workflows/python/#conda","title":"Conda","text":"<p>We provide prebuilt <code>conda</code> environments containing GPU-supported builds of <code>torch</code>, <code>tensorflow</code> (both with <code>horovod</code> support for multi-node calculations), <code>jax</code>, and many other commonly-used Python modules. </p> <p>Users can activate this environment by first loading the <code>conda</code> module, and then activating the base environment.</p> <p>Explicitly (either from an interactive job, or inside a job script):</p> <p><pre><code>$ module load conda\n$ conda activate base\n(base) $ which python3\n/soft/datascience/conda/2022-09-08/mconda3/bin/python3\n</code></pre> In one line, <code>module load conda; conda activate</code>. This can be performed on a compute node, as well as a login node. </p> <p>As of writing, the latest <code>conda</code> module on Polaris is built on Miniconda3 version 4.14.0 and contains Python 3.8.13. Future modules may contain entirely different major versions of Python, PyTorch, TensorFlow, etc.; however, the existing modules will be maintained as-is as long as feasible. </p> <p>While the shared Anaconda environment encapsulated in the module contains many of the most commonly used Python libraries for our users, you may still encounter a scenario in which you need to extend the functionality of the environment (i.e. install additional packages)</p> <p>There are two different approaches that are currently recommended.</p>"},{"location":"polaris/data-science-workflows/python/#virtual-environments-via-venv","title":"Virtual environments via <code>venv</code>","text":"<p>Creating your own (empty) virtual Python environment in a directory that is writable to you is simple: <pre><code>python3 -m venv /path/to/new/virtual/environment\n</code></pre> This creates a new folder that is fairly lightweight folder (&lt;20 MB) with its own Python interpreter where you can install whatever packages you'd like. First, you must activate the virtual environment to make this Python interpreter the default interpreter in your shell session.</p> <p>You activate the new environment whenever you want to start using it via running the activate script in that folder: <pre><code>/path/to/new/virtual/environment/bin/activate\n</code></pre></p> <p>In many cases, you do not want an empty virtual environment, but instead want to start from the <code>conda</code> base environment's installed packages, only adding and/or changing a few modules.</p> <p>To extend the base Anaconda environment with <code>venv</code> (e.g. <code>my_env</code> in the current directory) and inherit the base enviroment packages, one can use the <code>--system-site-packages</code> flag:</p> <p><pre><code>module load conda; conda activate\npython -m venv --system-site-packages my_env\nsource my_env/bin/activate\n# Install additional packages here...\n</code></pre> You can always retroactively change the <code>--system-site-packages</code> flag state for this virtual environment by editing <code>my_env/pyvenv.cfg</code> and changing the value of the line <code>include-system-site-packages = false</code>.</p> <p>To install a different version of a package that is already installed in the base environment, you can use: <pre><code>pip install --ignore-installed  ... # or -I\n</code></pre> The shared base environment is not writable, so it is impossible to remove or uninstall packages from it. The packages installed with the above <code>pip</code> command should shadow those installed in the base environment.</p>"},{"location":"polaris/data-science-workflows/python/#cloning-the-base-anaconda-environment","title":"Cloning the base Anaconda environment","text":"<p>If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via <code>conda install &lt;module&gt;</code> or <code>pip install &lt;module&gt;</code>. Unlike the <code>venv</code> approach, using a cloned Anaconda environment requires you to copy the entirety of the base environment, which can use significant storage space. </p> <p>This can be performed by:</p> <p><pre><code>$ module load conda\n$ conda activate base\n(base) $ conda create --clone base --prefix /path/to/envs/base-clone\n(base) $ conda activate /path/to/envs/base-clone\n(base-clone) $ which python3\n/path/to/base-clone/bin/python3\n</code></pre> The cloning process can be quite slow. </p> <p>Warning</p> <p>In the above commands, <code>path/to/envs/base-clone</code> should be replaced by a suitably chosen path.</p>"},{"location":"polaris/data-science-workflows/python/#using-pip-install-user-not-recommended","title":"Using <code>pip install --user</code> (not recommended)","text":"<p>With the conda environment setup, one can install common Python modules using <code>pip install --users &lt;module-name&gt;</code> which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>. The <code>$PYTHONUSERBASE</code> environment variable is automatically set when you load the base conda module, and is equal to  <code>/home/$USER/.local/polaris/conda/YYYY-MM-DD</code>.</p> <p>Note, Python modules installed this way that contain command line binaries will not have those binaries automatically added to the shell's <code>$PATH</code>. To manually add the path: <pre><code>export PATH=$PYTHONUSERBASE/bin:$PATH\n</code></pre> Be sure to remove this location from <code>$PATH</code> if you deactivate the base Anaconda environment or unload the module. </p> <p>Cloning the Anaconda environment, or using <code>venv</code> are both more flexible and transparent when compared to <code>--user</code> installs. </p>"},{"location":"polaris/data-science-workflows/applications/gpt-neox/","title":"Instructions for <code>gpt-neox</code>:","text":"<p>We include below a set of instructions to get <code>EleutherAI/gpt-neox</code> running on Polaris.</p> <p>A batch submission script for the following example is available here.</p> <p>Warning</p> <p>The instructions below should be ran directly from a compute node.</p> <p>Explicitly, to request an interactive job (from <code>polaris-login</code>): <pre><code>$ qsub -A &lt;project&gt; -q debug-scaling -l select=2 -l walltime=01:00:00\n</code></pre></p> <p>Refer to job scheduling and execution for additional information.</p> <ol> <li> <p>Load and activate the base <code>conda</code> environment:   <pre><code>module load conda\nconda activate base\n</code></pre></p> </li> <li> <p>We've installed the requirements for running <code>gpt-neox</code> into a virtual    environment. To activate this environment,   <pre><code>source /soft/datascience/venvs/polaris/2022-09-08/bin/activate\n</code></pre></p> </li> <li> <p>Clone the <code>EleutherAI/gpt-neox</code> repository if it doesn't already exist:   <pre><code>git clone https://github.com/EleutherAI/gpt-neox\n</code></pre></p> </li> <li> <p>Navigate into the <code>gpt-neox</code> directory:   <pre><code>cd gpt-neox\n</code></pre> <p>Note</p> <p>The remaining instructions assume you're inside the <code>gpt-neox</code> directory   </p> </p> </li> <li> <p>Create a DeepSpeed compliant <code>hostfile</code> (each line is formatted as <code>hostname, slots=N</code>):   <pre><code>cat $PBS_NODEFILE &gt; hostfile\nsed -e 's/$/ slots=4/' -i hostfile\nexport DLTS_HOSTFILE=hostfile </code></pre></p> </li> <li> <p>Create a <code>.deepspeed_env</code> file to ensure a consistent environment across all    workers    <pre><code>echo \"PATH=${PATH} &gt; .deepspeed_env\"\necho \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH} &gt;&gt; .deepspeed_env\"\necho \"http_proxy=${http_proxy} &gt;&gt; .deepspeed_env\"\necho \"https_proxy=${https_proxy} &gt;&gt; .deepspeed_env\"\n</code></pre></p> </li> <li> <p>Prepare data:   <pre><code>python3 prepare_data.py -d ./data\n</code></pre></p> </li> <li> <p>Train:   <pre><code>python3 ./deepy.py train.py -d configs small.yml local_setup.yml\n</code></pre></p> </li> </ol> <p>Danger</p> <p>If your training seems to be getting stuck at</p> <pre><code>Using /home/user/.cache/torch_extensions as PyTorch extensions root...\n</code></pre> <p>there may be a leftover <code>.lock</code> file from an aborted build. Cleaning either the whole <code>.cache</code> or the extensions' sub-directory should force a clean build on the next attempt.</p>"},{"location":"polaris/data-science-workflows/containers/containers/","title":"Containers on Polaris","text":"<p>Since Polaris is using NVIDIA A100 GPUs, there can be portability advantages with other NVIDIA-based systems if your workloads use containers.  In this document, we'll outline some information about containers on Polaris including how to build custom containers, how to run containers at scale, and common gotchas. </p> <p>Container creation can be achieved one of two ways either by using Docker on your local machine as mentioned in Docker section of Theta(KNL) and publishing it to DockerHub, or by using a Singularity recipe file and building on a Polaris worker node. If you are not interested in building a container and only want to use the available containers, you can read the section on available containers.</p>"},{"location":"polaris/data-science-workflows/containers/containers/#singularity","title":"Singularity","text":"<p>The container system on Polaris is <code>singularity</code>.  You can set up singularity with a module (this is different than, for example, ThetaGPU!):</p> <pre><code># To see what versions of singularity are available:\nmodule avail singularity\n\n# To load the Default version:\nmodule load singularity\n\n# To load a specific version:\nmodule load singularity/3.8.7 # the default at the time of writing these docs.\n</code></pre>"},{"location":"polaris/data-science-workflows/containers/containers/#which-singularity","title":"Which Singularity?","text":"<p>There used to be a single <code>singularity</code> tool, which in 2021 split after some turmoil.  There are now two <code>singularity</code>s: one developed by Sylabs, and the other as part of the Linux Foundation.  Both are open source, and the split happened around version 3.10.  The version on Polaris is from Sylabs but for completeness, here is the Linux Foundation's version.  Note that the Linux Foundation version is renamed to <code>apptainer</code> - different name, roughly the same thing though divergence may happen after 2021's split.</p>"},{"location":"polaris/data-science-workflows/containers/containers/#build-from-docker-images-or-argonne-github-container-registry","title":"Build from Docker Images or Argonne Github container registry","text":"<p>Docker containers require root privileges, which users do not have on Polaris.  That doesn't mean all your docker containers aren't useful, though.  If you have an existing docker container, you can convert it to singularity pretty easily on the login node. To build the latest NVIDIA container for PyTorch you can run the following:</p> <pre><code>module load singularity\nsingularity build pytorch:22.06-py3.sing docker://nvcr.io/nvidia/pytorch:22.06-py3\n</code></pre> <p>Note that <code>latest</code> here mean when these docs were written, summer 2022.  It may be useful to get a newer container if you need the latest features.  You can find the PyTorch container site here.  The tensorflow containers are here (though note that LCF doesn't prebuild the TF-1 containers typically).  You can search the full container registry here.</p> <p>You can also use our custom built containers using Github OCI container registry. Here's a list of containers distributed by ALCF staff tailored for Polaris.</p> <pre><code>module load singularity\nsingularity pull IMAGE_NAME oras://ghcr.io/argonne-lcf/IMAGE_NAME:latest\n</code></pre>"},{"location":"polaris/data-science-workflows/containers/containers/#build-with-a-recipe","title":"Build with a Recipe","text":"<p>You can also build a singularity container using a recipe file. Detailed instructions for recipe construction are available on the Singularity Recipe Page. You can also check our singularity recipe example for building a mpich version 4 container on Polaris.</p> <p>Once you have a recipe file, you can build it on Polaris, but only on compute nodes. You can launch an interactive job using the attribute <code>singularity_fakeroot=true</code> to build on a compute node. </p> <pre><code>qsub -I -A &lt;project_name&gt; -q &lt;queue&gt; -l select=1 -l walltime=60:00 -l singularity_fakeroot=true -l filesystems=home:eagle:grand\n</code></pre> <p>You need to replace the <code>&lt;project_name&gt;</code> with the appropriate project to charge and <code>&lt;queue&gt;</code> with <code>debug</code>, or <code>preemptable</code> queues since we only request a single node. </p> <p>After your interactive job has started, you need to load the <code>singularity</code> module on the compute node and export the proxy variables for internet access. Then you can build the container as shown below.</p> <pre><code>module load singularity\nexport HTTP_PROXY=http://proxy.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=http://proxy.alcf.anl.gov:3128\nsingularity build --fakeroot &lt;image_name&gt;.sif &lt;def_filename&gt;.def </code></pre> <p>Alternatively, you can just pull the mpich 4 image distributed by us and build on top of it</p> <pre><code>singularity pull oras://ghcr.io/argonne-lcf/mpich-4:latest\n</code></pre>"},{"location":"polaris/data-science-workflows/containers/containers/#running-singularity-container-on-polaris","title":"Running Singularity container on Polaris","text":""},{"location":"polaris/data-science-workflows/containers/containers/#example-submission-script-on-polaris","title":"Example submission script on Polaris","text":"<p>To run a container on Polaris you can use the submission script described here. Below we have described the submission script for your understanding.</p> <p>First we define our job and our script takes the container name as an input parameter.</p> <pre><code>#!/bin/sh\n#PBS -l select=2:system=polaris\n#PBS -q debug\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:grand\n#PBS -A &lt;project_name&gt;\ncd ${PBS_O_WORKDIR}\necho $CONTAINER\n</code></pre> <p>We move to current working directory and enable network access at run time by setting the proxy. We also load singularity.</p> <pre><code># SET proxy for internet access\nmodule load singularity\nexport HTTP_PROXY=http://proxy.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=http://proxy.alcf.anl.gov:3128\n</code></pre> <p>This is important for system (Polaris - Cray) mpich to bind to containers mpich. Set the following environment variables</p> <pre><code>ADDITIONAL_PATH=/opt/cray/pe/pals/1.1.7/lib/\nmodule load cray-mpich-abi\nexport SINGULARITYENV_LD_LIBRARY_PATH=\"$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH:$ADDITIONAL_PATH\"\n</code></pre> <p>Set the number of ranks per node spread as per your scaling requirements</p> <pre><code># MPI example w/ 16 MPI ranks per node spread evenly across cores\nNODES=`wc -l &lt; $PBS_NODEFILE`\nPPN=16\nPROCS=$((NODES * PPN))\necho \"NUM_OF_NODES= ${NODES} TOTAL_NUM_RANKS= ${PROCS} RANKS_PER_NODE= ${PPN}\"\n</code></pre> <p>Finally launch your script</p> <pre><code>echo C++ MPI\nmpiexec -hostfile $PBS_NODEFILE -n $PROCS -ppn $PPN singularity exec -B /opt -B /var/run/palsd/ $CONTAINER /usr/source/mpi_hello_world\n\necho Python MPI\nmpiexec -hostfile $PBS_NODEFILE -n $PROCS -ppn $PPN singularity exec -B /opt -B /var/run/palsd/ $CONTAINER python3 /usr/source/mpi_hello_world.py\n</code></pre> <p>The job can be submitted using:</p> <pre><code>qsub -v CONTAINER=mpich-4_latest.sif job_submission.sh\n</code></pre>"},{"location":"polaris/data-science-workflows/containers/containers/#available-containers","title":"Available containers","text":"<p>If you just want to know what containers are available, here you go. </p> <ul> <li>For running mpich/MPI containers on Polaris, it can be found here</li> <li>For running databases on Polaris. It can be found here</li> <li>For using shpc - that allows for running containers as modules. It can be found here</li> <li>Some containers are found in /soft/containers</li> </ul> <p>The latest containers are updated periodically. If you have trouble using containers, or request a newer or a different container please contact ALCF support at <code>support@alcf.anl.gov</code>.</p>"},{"location":"polaris/data-science-workflows/containers/containers/#troubleshooting","title":"Troubleshooting","text":"<ol> <li> <p>Permission Denied Error: One may get a <code>permission denied</code> error during the build process, due to a nasty permission setting, quota limitations, or simply due to an unresolved symbolic link. You can try one of the solutions below:</p> <ul> <li>Check your quota and delete any unnecessary files. </li> <li>Clean-up singularity cache, <code>~/.singularity/cache</code>, and set the singularity tmp and cache directories as below:     <pre><code>export SINGULARITY_TMPDIR=/tmp/singularity-tmpdir\nmkdir $SINGULARITY_TMPDIR\nexport SINGULARITY_CACHEDIR=/tmp/singularity-cachedir/\nmkdir $SINGULARITY_CACHEDIR\n</code></pre></li> <li>Make sure you are not on a directory accessed with a symlink, i.e. check if <code>pwd</code> and <code>pwd -P</code> returns the same path.</li> <li>If any of the above doesn't work, try running the build in your home directory.</li> </ul> </li> <li> <p>Mapping to rank 0 on all nodes: This is mainly due to container mpich not binding to system mpich. It is imperative for the container to have mpich which can bind dynamically to system mpich at runtime. Ensure your submission script has the following variables and modules loaded (see below). If this does not resolve, ensure the containers mpich is built with the '--disable-wrapper-rpath' flag. Please refer to this link to find examples of building a mpich based container from scratch and running on Polaris.</p> </li> </ol> <pre><code>ADDITIONAL_PATH=/opt/cray/pe/pals/1.1.7/lib/\nmodule load cray-mpich-abi\nexport SINGULARITYENV_LD_LIBRARY_PATH=\"$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH:$ADDITIONAL_PATH\"\nsingularity exec -B /opt -B /var/run/palsd/\n</code></pre> <ol> <li> <p>libmpi.so.40 not found: This may be due to mpich binding to the wrong system mpich. Try removing .conda &amp; .cache &amp; .local folders from your home directory. Also rebuild your container and try again.</p> </li> <li> <p>Containers built with openmpi may not work correctly. Please ensure your container is built with mpich and the base image is of Debian architecture (For e.g. Ubuntu) image.</p> </li> </ol>"},{"location":"polaris/data-science-workflows/frameworks/deepspeed/","title":"DeepSpeed","text":"<p>The base <code>conda</code> environment on Polaris comes with Microsoft's DeepSpeed pre-installed. Instructions for using / cloning the base environment can be found here.</p> <p>A batch submission script for the following example is available here.</p> <p>We describe below the steps needed to get started with DeepSpeed on Polaris.</p> <p>We focus on the <code>cifar</code> example provided in the DeepSpeedExamples repository, though this approach should be generally applicable for running any model with DeepSpeed support.</p>"},{"location":"polaris/data-science-workflows/frameworks/deepspeed/#running-deepspeed-on-polaris","title":"Running DeepSpeed on Polaris","text":"<p>Note</p> <p>The instructions below should be ran directly from a compute node.</p> <p>Explicitly, to request an interactive job (from <code>polaris-login</code>): <pre><code>qsub -A &lt;project&gt; -q debug-scaling -l select=2 -l walltime=01:00:00 -I\n</code></pre></p> <p>Refer to job scheduling and execution for additional information.</p> <ol> <li> <p>Load <code>conda</code> module and activate base environment:     <pre><code>module load conda ; conda activate base\n</code></pre></p> </li> <li> <p>Clone    microsoft/DeepSpeedExamples    and navigate into the directory:     <pre><code>git clone https://github.com/microsoft/DeepSpeedExamples.git\ncd DeepSpeedExamples/cifar\n</code></pre></p> </li> </ol> <p>Launching DeepSpeed</p> Launching with DeepSpeedLaunching with MPICH <ol> <li> <p>Create a DeepSpeed compliant <code>hostfile</code>, specifying the <code>hostname</code> and    number of GPUs (<code>slots</code>) for each of our available workers: <pre><code>cat $PBS_NODEFILE &gt; hostfile\nsed -e 's/$/ slots=4/' -i hostfile\n</code></pre></p> </li> <li> <p>Create a <code>.deepspeed_env</code> containing the environment variables our    workers will need access to: <pre><code>echo \"PATH=${PATH}\" &gt;&gt; .deepspeed_env\necho \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" &gt;&gt; .deepspeed_env\necho \"http_proxy=${http_proxy}\" &gt;&gt; .deepspeed_env\necho \"https_proxy=${https_proxy}\" &gt;&gt; .deepspeed_env\n</code></pre></p> </li> </ol> <p>Warning</p> <p>The <code>.deepspeed_env</code> file expects each line to be of the form <code>KEY=VALUE</code>. Each of these will then be set as environment variables on each available worker specified in our <code>hostfile</code>.</p> <p>We can then run the <code>cifar10_deepspeed.py</code> module using DeepSpeed: <pre><code>deepspeed --hostfile=hostfile cifar10_deepspeed.py \\\n--deepspeed \\\n--deepspeed_config ds_config.json\n</code></pre></p> <ol> <li> <p>Get total number of available GPUs:</p> <ol> <li>Count number of lines in <code>$PBS_NODEFILE</code> (1 host per line)</li> <li>Count number of GPUs available on current host</li> <li><code>NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"</code> <pre><code>NHOSTS=$(wc -l &lt; \"${PBS_NODEFILE}\")\nNGPU_PER_HOST=$(nvidia-smi -L | wc -l)\nNGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n</code></pre></li> </ol> </li> <li> <p>Launch with <code>mpiexec</code>: <pre><code>mpiexec \\\n--verbose \\\n--envall \\\n-n \"${NGPUS}\" \\\n--ppn \"${NGPU_PER_HOST}\" \\\n--hostfile=\"${PBS_NODEFILE}\" \\\npython3 \\\ncifar10_deepspeed.py \\\n--deepspeed_config ds_config.json\n</code></pre></p> </li> </ol> <code>AssertionError: Micro batch sizer per gpu: 0 has to be greater than 0</code> <p>Depending on the details of your specific job, it may be necessary to modify the provided <code>ds_config.json</code>.</p> <p>If you encounter an error: <pre><code>x3202c0s31b0n0: AssertionError: Micro batch size per gpu: 0 has to be greater than 0\n</code></pre> you can modify the <code>\"train_batch_size\": 16</code> variable in the provided <code>ds_config.json</code> to the (total) number of available GPUs, and explicitly set <code>\"gradient_accumulation_steps\": 1</code>, as shown below. <pre><code>$ export NHOSTS=$(wc -l &lt; \"${PBS_NODEFILE}\")\n$ export NGPU_PER_HOST=$(nvidia-smi -L | wc -l)\n$ export NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n$ echo $NHOSTS $NGPU_PER_HOST $NGPUS\n24 4 96\n$ # replace \"train_batch_size\" with $NGPUS in ds_config.json\n$ # and write to `ds_config-polaris.json`\n$ sed \\\n\"s/$(cat ds_config.json| grep batch | cut -d ':' -f 2)/ ${NGPUS},/\" \\\nds_config.json \\\n&gt; ds_config-polaris.json\n$ cat ds_config-polaris.json\n{\n\"train_batch_size\": 96,\n    \"gradient_accumulation_steps\": 1,\n    ...\n}\n</code></pre></p>"},{"location":"polaris/data-science-workflows/frameworks/jax/","title":"JAX","text":"<p>JAX is another popular python package for accelerated computing.  JAX is built on XLA (the same XLA TensorFlow uses) as well as AutoGrad, and additionally has acceleration tools that operate on functions such as <code>vmap</code>, <code>jit</code>, etc.  JAX is not as widespread in machine learning as TensorFlow and PyTorch for traditional models (Computer Vision, Language Models) though it is quickly gaining promienence.  JAX is very powerful when a program needs non-traditional autodifferentiation or vectorizatoin, such as: forward-mode AD, higher order derivatives, Jacobians, Hessians, or any combination of the above.  Users of JAX on Polaris are encouraged to read the user documentation in detail, particularly the details about pure-functional programming, no in-place operations, and the common mistakes in writing functions for the <code>@jit</code> decorator.</p>"},{"location":"polaris/data-science-workflows/frameworks/jax/#jax-on-polaris","title":"JAX on Polaris","text":"<p>JAX is installed on Polaris via the <code>conda</code> module, available with: <pre><code>module load conda; conda activate\n</code></pre></p> <p>Then, you can load JAX in <code>python</code> as usual (below showing results from the <code>conda/2022-07-19</code> module):</p> <pre><code>&gt;&gt;&gt; import jax\n&gt;&gt;&gt; jax.__version__\n'0.3.15'\n&gt;&gt;&gt;\n</code></pre>"},{"location":"polaris/data-science-workflows/frameworks/jax/#notes-on-jax-0315","title":"Notes on JAX 0.3.15","text":"<p>On Polaris, due to a bug, an environment variable must be set to use JAX on GPUs.  The following code will crash: <pre><code>import jax.numpy as numpy\na = numpy.zeros(1000)\n</code></pre> outputting an error that looks like: <pre><code>jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: no kernel image is available for execution on the device\n</code></pre></p> <p>You can fix this by setting an environment variable: <pre><code>export XLA_FLAGS=\"--xla_gpu_force_compilation_parallelism=1\"\n</code></pre></p>"},{"location":"polaris/data-science-workflows/frameworks/jax/#scaling-jax-to-multiple-gpus-and-multiple-nodes","title":"Scaling JAX to multiple GPUs and multiple Nodes","text":"<p>Jax has intrinsic scaling tools to use multiple GPUs on a single node, via the <code>pmap</code> function.  If this is sufficient for your needs, excellent.  If not, another alternative is to use the newer package mpi4jax.</p> <p>mpi4Jax is a relatively new project and requires setting some environment variables for good performance and usability: - Set <code>MPI4JAX_USE_CUDA_MPI=1</code> to use CUDA-Aware MPI, supported in the <code>conda</code> module, to do operations directly from the GPU. - Set <code>MPICH_GPU_SUPPORT_ENABLED=1</code> to use CUDA-Aware MPI.</p> <p>The following code, based off of a test script from the mpi4jax repository, can help you verify you are using mpi4jax properly:</p> <pre><code>import os\nfrom mpi4py import MPI\nimport jax\nimport jax.numpy as jnp\nimport mpi4jax\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nlocal_rank = int(os.environ[\"PMI_LOCAL_RANK\"])\navailable_devices = jax.devices(\"gpu\")\nif len(available_devices) &lt;= local_rank:\nraise Exception(\"Could not find enough GPUs\")\ntarget_device = available_devices[local_rank]\n@jax.jit\ndef foo(arr):\narr = arr + rank\narr_sum, _ = mpi4jax.allreduce(arr, op=MPI.SUM, comm=comm)\nreturn arr_sum\nwith jax.default_device(target_device):\na = jnp.zeros((3, 3))\nprint(f\"Rank {rank}, local rank {local_rank}, a.device is {a.device()}\")\nresult = foo(a)\nprint(f\"Rank {rank}, local rank {local_rank}, result.device is {result.device()}\")\nimport time\nprint(\"Sleeping for 5 seconds if you want to look at nvidia-smi ... \")\nimport time\ntime.sleep(5)\nprint(\"Done sleeping\")\nif rank == 0:\nprint(result)\n</code></pre> <p>JAX and mpi4jax are both still somewhat early in their software lifecycles.  Updates are frequent, and if you require assistance please contact support@alcf.anl.gov.</p>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/","title":"PyTorch on Polaris","text":"<p>PyTorch is a popular, open source deep learning framework developed and released by Facebook.  The PyTorch home page has more information about PyTorch, which you can refer to.  For trouble shooting on Polaris, please contact support@alcf.anl.gov.</p>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/#installation-on-polaris","title":"Installation on Polaris","text":"<p>PyTorch is installed on Polaris already, available in the <code>conda</code> module.  To use it from a compute node, please do:</p> <pre><code>module load conda\nconda activate\n</code></pre> <p>Then, you can load PyTorch in <code>python</code> as usual (below showing results from the <code>conda/2022-07-19</code> module):</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.__version__\n'1.12.0a0+git67ece03'\n&gt;&gt;&gt;\n</code></pre> <p>This installation of PyTorch was built from source and the cuda libraries it uses are found via the <code>CUDA_HOME</code> environment variable (below showing results from the <code>conda/2022-07-19</code> module):</p> <pre><code>$ echo $CUDA_HOME\n/soft/datascience/cuda/cuda_11.5.2_495.29.05_linux\n</code></pre> <p>If you need to build applications that use this version of PyTorch and CUDA, we recommend using these cuda libraries to ensure compatibility.  We periodically update the PyTorch release, though updates will come in the form of new versions of the <code>conda</code> module.</p> <p>PyTorch is also available through nvidia containers that have been translated to Singularity containers.  For more information about containers, please see the containers documentation page.</p>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/#pytorch-best-practices-on-polaris","title":"PyTorch Best Practices on Polaris","text":""},{"location":"polaris/data-science-workflows/frameworks/pytorch/#single-node-performance","title":"Single Node Performance","text":"<p>When running PyTorch applications, we have found the following practices to be generally, if not universally, useful and encourage you to try some of these techniques to boost performance of your own applications.</p> <ol> <li> <p>Use Reduced Precision. Reduced Precision is available on A100 via tensorcores and is supported with PyTorch operations.  In general, the way to do this is via the PyTorch Automatic Mixed Precision package (AMP), as descibed in the mixed precision documentation.  In PyTorch, users generally need to manage casting and loss scaling manually,  though context managers and function decorators can provide easy tools to do this.</p> </li> <li> <p>PyTorch has a <code>JIT</code> module as well as backends to support op fusion, similar to TensorFlow's <code>tf.function</code> tools.  However, PyTorch JIT capabilities are newer and may not yield performance improvements.  Please see TorchScript for more information.</p> </li> </ol>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/#multi-gpu-multi-node-scale-up","title":"Multi-GPU / Multi-Node Scale up","text":"<p>PyTorch is compatible with scaling up to multiple GPUs per node, and across multiple nodes.  Good scaling performance has been seen up to the entire Polaris system, &gt; 2048 GPUs.  Good performance with PyTorch has been seen with both DDP and Horovod.  For details, please see the Horovod documentation or the Distributed Data Parallel documentation.  Some Polaris-specific details that may be helpful to you:</p> <ol> <li>CPU affinity and NCCL settings can improve scaling performance, particularly at the largest scales.  In particular, we encourage users to try their scaling measurements with the following settings:</li> <li>Set the environment variable <code>NCCL_COLLNET_ENABLE=1</code></li> <li>Set the environment varialbe <code>NCCL_NET_GDR_LEVEL=PHB</code></li> <li> <p>Manually set the CPU affinity via mpiexec, such as with <code>--cpu-bind verbose,list:0,8,16,24</code></p> </li> <li> <p>Horovod and DDP work best when you limit the visible devices to only one GPU.  Note that if you import <code>mpi4py</code> or <code>horovod</code>, and then do something like <code>os.environ[\"CUDA_VISIBLE_DEVICES\"] = hvd.local_rank()</code>, it may not actually work!  You must set the <code>CUDA_VISIBLE_DEVICES</code> environment variable prior to doing <code>MPI.COMM_WORLD.init()</code>, which is done in <code>horovod.init()</code> as well as implicitly in <code>from mpi4py import MPI</code>.   On Polaris specifically, you can use the environment variable <code>PMI_LOCAL_RANK</code> (as well as <code>PMI_LOCAL_SIZE</code>) to learn information about the node-local MPI ranks.  </p> </li> </ol>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/#deepspeed","title":"DeepSpeed","text":"<p>DeepSpeed is also available and usable on Polaris.  For more information, please see the DeepSpeed documentation directly.</p>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/#pytorch-dataloader-and-multi-node-horovod","title":"PyTorch <code>DataLoader</code> and multi-node Horovod","text":"<p>Please note there is a bug that causes a hang when using PyTorch's multithreaded data loaders with distributed training across multiple nodes. To workaround this, NVIDIA recommends setting <code>num_workers=0</code> in the dataloader configuration, which serializes data loading. </p> <p>For more details, see Polaris Known Issues.</p>"},{"location":"polaris/data-science-workflows/frameworks/tensorflow/","title":"TensorFlow on Polaris","text":"<p>TensorFlow is a popular, open-source deep learning framework developed and released by Google.  The TensorFlow home page has more information about TensorFlow, which you can refer to.  For trouble shooting on Polaris, please contact support@alcf.anl.gov.</p>"},{"location":"polaris/data-science-workflows/frameworks/tensorflow/#installation-on-polaris","title":"Installation on Polaris","text":"<p>TensorFlow is already pre-installed on Polaris, available in the <code>conda</code> module.  To use it from a compute node, please do:</p> <pre><code>module load conda\nconda activate\n</code></pre> <p>Then, you can load TensorFlow in <code>python</code> as usual (below showing results from the <code>conda/2022-07-19</code> module):</p> <pre><code>&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.__version__\n'2.9.1'\n&gt;&gt;&gt;\n</code></pre> <p>This installation of TensorFlow was built from source and the CUDA libraries it uses are found via the <code>CUDA_HOME</code> environment variable (below showing results from the <code>conda/2022-07-19</code> module):</p> <pre><code>$ echo $CUDA_HOME\n/soft/datascience/cuda/cuda_11.5.2_495.29.05_linux\n</code></pre> <p>If you need to build applications that use this version of TensorFlow and CUDA, we recommend using these cuda libraries to ensure compatibility.  We periodically update the TensorFlow release, though updates will come in the form of new versions of the <code>conda</code> module.</p> <p>TensorFlow is also available through NVIDIA containers that have been translated to Singularity containers.  For more information about containers, please see the Containers documentation page.</p>"},{"location":"polaris/data-science-workflows/frameworks/tensorflow/#tensorflow-best-practices-on-polaris","title":"TensorFlow Best Practices on Polaris","text":""},{"location":"polaris/data-science-workflows/frameworks/tensorflow/#single-node-performance","title":"Single Node Performance","text":"<p>When running TensorFlow applications, we have found the following practices to be generally, if not universally, useful and encourage you to try some of these techniques to boost performance of your own applications.</p> <ol> <li> <p>Use Reduced Precision. Reduced Precision is available on A100 via tensorcores and is supported with TensorFlow operations.  In general, the way to do this is via the <code>tf.keras.mixed_precision</code> Policy, as descibed in the mixed precision documentation.  If you use a custom training loop (and not <code>keras.Model.fit</code>), you will also need to apply loss scaling.</p> </li> <li> <p>Use TensorFlow's graph API to improve efficiency of operations.  TensorFlow is, in general, an imperative language but with function decorators like <code>@tf.function</code> you can trace functions in your code.  Tracing replaces your python function with a lower-level, semi-compiled TensorFlow Graph. More information about the <code>tf.function</code> interface is available here.  When possible, use jit_compile, but be aware of sharp bits when using <code>tf.function</code>: python expressions that aren't tensors are often replaced as constants in the graph, which may or may not be your intention.</p> </li> <li> <p>Use XLA compilation on your code.  XLA is the Accelerated Linear Algebra library that is available in tensorFlow and critical in software like JAX.  XLA will compile a <code>tf.Graph</code> object, generated with <code>tf.function</code> or similar, and perform optimizations like operation-fusion.  XLA can give impressive performance boosts with almost no user changes except to set an environment variable <code>TF_XLA_FLAGS=--tf_xla_auto_jit=2</code>.  If your code is complex, or has dynamically sized tensors (tensors where the shape changes every iteration), XLA can be detrimental: the overhead for compiling functions can be large enough to mitigate performance improvements.  XLA is particularly powerful when combined with reduced precision, yielding speedups &gt; 100% in some models.</p> </li> </ol>"},{"location":"polaris/data-science-workflows/frameworks/tensorflow/#multi-gpu-multi-node-scale-up","title":"Multi-GPU / Multi-Node Scale up","text":"<p>TensorFlow is compatible with scaling up to multiple GPUs per node, and across multiple nodes.  Good scaling performance has been seen up to the entire Polaris system, &gt; 2048 GPUs.  Good performance with tensorFlow has been seen with horovod in particular.  For details, please see the Horovod documentation.  Some polaris specific details that may be helpful to you:</p> <ol> <li>CPU affinity and NCCL settings can improve scaling performance, particularly at the largest scales.  In particular, we encourage users to try their scaling measurements with the following settings:</li> <li>Set the environment variable <code>NCCL_COLLNET_ENABLE=1</code></li> <li>Set the environment varialbe <code>NCCL_NET_GDR_LEVEL=PHB</code></li> <li> <p>Manually set the CPU affinity via mpiexec, such as with <code>--cpu-bind verbose,list:0,8,16,24</code></p> </li> <li> <p>Horovod works best when you limit the visible devices to only one GPU.  Note that if you import <code>mpi4py</code> or <code>horovod</code>, and then do something like <code>os.environ[\"CUDA_VISIBLE_DEVICES\"] = hvd.local_rank()</code>, it may not actually work!  You must set the <code>CUDA_VISIBLE_DEVICES</code> environment variable prior to doing <code>MPI.COMM_WORLD.init()</code>, which is done in <code>horovod.init()</code> as well as implicitly in <code>from mpi4py import MPI</code>.   On Polaris specifically, you can use the environment variable <code>PMI_LOCAL_RANK</code> (as well as <code>PMI_LOCAL_SIZE</code>) to learn information about the node-local MPI ranks.  </p> </li> </ol>"},{"location":"polaris/data-science-workflows/frameworks/tensorflow/#tensorflow-dataloaders","title":"TensorFlow Dataloaders","text":"<p>Additional information to be provided.</p>"},{"location":"polaris/debugging-tools/CUDA-GDB/","title":"CUDA-GDB","text":""},{"location":"polaris/debugging-tools/CUDA-GDB/#references","title":"References","text":"<p>NVIDIA CUDA-GDB Documentation </p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#introduction","title":"Introduction","text":"<p>CUDA-GDB is the NVIDIA tool for debugging CUDA applications running on Polaris. CUDA-GDB is an extension to GDB, the GNU Project debugger. The tool provides developers with a mechanism for debugging CUDA applications running on actual hardware. This enables developers to debug applications without the potential variations introduced by simulation and emulation environments.</p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"polaris/debugging-tools/CUDA-GDB/#debug-compilation","title":"Debug Compilation","text":"<p>NVCC, the NVIDIA CUDA compiler driver, provides a mechanism for generating the debugging information necessary for CUDA-GDB to work properly. The -g -G option pair must be passed to NVCC when an application is compiled for ease of debugging with CUDA-GDB; for example, <pre><code>nvcc -g -G foo.cu -o foo\n</code></pre> Using this line to compile the CUDA application <code>foo.cu</code> * forces <code>-O0</code> compilation, with the exception of very limited dead-code eliminations and register-spilling optimizations. * makes the compiler include debug information in the executable</p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#running-cuda-gdb-on-polaris-compute-nodes","title":"Running CUDA-gdb on Polaris compute nodes","text":"<p>Start an interactive job mode on Polaris as follows: <pre><code>$ qsub -I -l select=1 -l walltime=1:00:00\n\n$ cuda-gdb --version\nNVIDIA (R) CUDA Debugger\n11.4 release\nPortions Copyright (C) 2007-2021 NVIDIA Corporation\nGNU gdb (GDB) 10.1\nCopyright (C) 2020 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n$ cuda-gdb foo\n</code></pre></p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#a-quick-example-with-a-stream-benchmark-on-a-polaris-compute-node","title":"A quick example with a stream benchmark on a Polaris compute node","text":"<pre><code>jkwack@polaris-login-02:~&gt; qsub -I -l select=1 -l walltime=1:00:00\nqsub: waiting for job 308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov to start\nqsub: job 308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov ready\n\nCurrently Loaded Modules:\n  1) craype-x86-rome          4) perftools-base/22.05.0   7) cray-dsmml/0.2.2   10) cray-pmi-lib/6.0.17  13) PrgEnv-nvhpc/8.3.3\n  2) libfabric/1.11.0.4.125   5) nvhpc/21.9               8) cray-mpich/8.1.16  11) cray-pals/1.1.7      14) craype-accel-nvidia80\n  3) craype-network-ofi       6) craype/2.7.15            9) cray-pmi/6.1.2     12) cray-libpals/1.1.7\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; nvcc -g -G -c ../src/cuda/CUDAStream.cu  -I ../src/\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; nvcc -g -G -c ../src/main.cpp -DCUDA -I ../src/cuda/ -I ../src/\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; nvcc -g -G main.o CUDAStream.o -o cuda-stream-debug\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; ./cuda-stream-debug \nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1313940.694 0.00041     0.00047     0.00047     \nMul         1302000.791 0.00041     0.00048     0.00047     \nAdd         1296217.720 0.00062     0.00070     0.00069     \nTriad       1296027.887 0.00062     0.00070     0.00069     \nDot         823405.227  0.00065     0.00076     0.00075     \n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; cuda-gdb ./cuda-stream-debug \nNVIDIA (R) CUDA Debugger\n11.4 release\nPortions Copyright (C) 2007-2021 NVIDIA Corporation\nGNU gdb (GDB) 10.1\nCopyright (C) 2020 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\nType \"show copying\" and \"show warranty\" for details.\nThis GDB was configured as \"x86_64-pc-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n&lt;https://www.gnu.org/software/gdb/bugs/&gt;.\nFind the GDB manual and other documentation resources online at:\n    &lt;http://www.gnu.org/software/gdb/documentation/&gt;.\n\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from ./cuda-stream-debug...\n(cuda-gdb) b CUDAStream.cu:203\nBreakpoint 1 at 0x412598: CUDAStream.cu:203. (2 locations)\n(cuda-gdb) r      \nStarting program: /home/jkwack/BabelStream/build_polaris_debug/cuda-stream-debug \n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\n[Detaching after fork from child process 58459]\n[New Thread 0x15554c6bb000 (LWP 58475)]\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\n[New Thread 0x15554c4ba000 (LWP 58476)]\n[Switching focus to CUDA kernel 0, grid 5, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 3, lane 0]\n\nThread 1 \"cuda-stream-deb\" hit Breakpoint 1, triad_kernel&lt;double&gt;&lt;&lt;&lt;(32768,1,1),(1024,1,1)&gt;&gt;&gt; (a=0x155506000000, b=0x1554f6000000, c=0x1554e6000000)\n    at ../src/cuda/CUDAStream.cu:203\n203   a[i] = b[i] + scalar * c[i];\n(cuda-gdb) c\nContinuing.\n[Switching focus to CUDA kernel 0, grid 5, block (1,0,0), thread (0,0,0), device 0, sm 0, warp 32, lane 0]\n\nThread 1 \"cuda-stream-deb\" hit Breakpoint 1, triad_kernel&lt;double&gt;&lt;&lt;&lt;(32768,1,1),(1024,1,1)&gt;&gt;&gt; (a=0x155506000000, b=0x1554f6000000, c=0x1554e6000000)\n    at ../src/cuda/CUDAStream.cu:203\n203   a[i] = b[i] + scalar * c[i];\n(cuda-gdb) info locals\ni = 1024\n(cuda-gdb) p b[i]\n$1 = 0.040000000000000008\n(cuda-gdb) p scalar\n$2 = 0.40000000000000002\n(cuda-gdb) p c[i]\n$3 = 0.14000000000000001\n(cuda-gdb) d 1\n(cuda-gdb) c\nContinuing.\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1314941.553 0.00041     0.00041     0.00041     \nMul         1301022.680 0.00041     0.00042     0.00041     \nAdd         1293858.147 0.00062     0.00063     0.00063     \nTriad       1297681.929 0.00062     0.00063     0.00062     \nDot         828446.963  0.00065     0.00066     0.00065     \n[Thread 0x15554c4ba000 (LWP 58476) exited]\n[Thread 0x15554c6bb000 (LWP 58475) exited]\n[Inferior 1 (process 58454) exited normally]\n(cuda-gdb) q\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; \n</code></pre>"},{"location":"polaris/hardware-overview/machine-overview/","title":"Polaris","text":"<p>Polaris is a 560 node HPE Apollo 6500 Gen 10+ based system.  Each node has a single 2.8 GHz AMD EPYC Milan 7543P 32 core CPU with 512 GB of DDR4 RAM and four NVIDIA A100 GPUs connected via NVLink, a pair of local 1.6TB of SSDs in RAID0 for the users use, and a pair of Slingshot network adapters.  They are currently Slingshot 10, but are scheduled to be upgraded to Slingshot 11 in  2023.  There are two nodes per chassis, seven chassis per rack, and 40 racks for a total of 560 nodes.  More detailed specifications are as follows:</p>"},{"location":"polaris/hardware-overview/machine-overview/#polaris-compute-nodes","title":"Polaris Compute Nodes","text":"POLARIS COMPUTE DESCRIPTION PER NODE AGGREGATE Processor (Note 1) 2.8 GHz 7543P 1 560 Cores/Threads AMD Zen 3 (Milan) 32/64 17,920/35,840 RAM (Note 2) DDR4 512 GiB 280 TiB GPUS NVIDIA A100 4 2240 Local SSD 1.6 TB 2/3.2 TB 1120/1.8PB <p>Note 1: 256MB shared L3 cache, 512KB L2 cache per core, 32 KB L1 cache per core Note 2: 8 memory channels rated at 204.8 GiB/s</p>"},{"location":"polaris/hardware-overview/machine-overview/#polaris-a100-gpu-information","title":"Polaris A100 GPU Information","text":"DESCRIPTION A100 PCIe A100 HGX (Polaris) GPU Memory 40 GiB HBM2 160 GiB HBM2 GPU Memory BW 1.6 TB/s 6.4 TB/s Interconnect PCIe Gen4 64 GB/s NVLink 600 GB/s FP 64 9.7 TF 38.8 TF FP64 Tensor Core 19.5 TF 78 TF FP 32 19.5 TF 78 TF BF16 Tensor Core 312 TF 1.3 PF FP16 Tensor Core 312 TF 1.3 PF INT8 Tensor Core 624 TOPS 2496 TOPS Max TDP Power 250 W 400 W"},{"location":"polaris/hardware-overview/machine-overview/#polaris-device-affinity-information","title":"Polaris Device Affinity Information","text":"CPU Affinity NUMA Affinity GPU0 GPU1 GPU2 GPU3 mlx5_0 mlx5_1 24-31,56-63 3 GPU0 X NV4 NV4 NV4 SYS SYS 16-23,48-55 2 GPU1 NV4 X NV4 NV4 SYS PHB 8-15,40-47 1 GPU2 NV4 NV4 X NV4 SYS SYS 0-7,32-39 0 GPU3 NV4 NV4 NV4 X PHB SYS mlx5_0 SYS SYS SYS PHB X SYS mlx5_1 SYS PHB SYS SYS SYS X"},{"location":"polaris/hardware-overview/machine-overview/#legend","title":"Legend:","text":"<p>X    = Self SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX  = Connection traversing at most a single PCIe bridge NV#  = Connection traversing a bonded set of # NVLinks</p> <p>Links to detailed NVIDIA A100 documentation:     - NVIDIA A100 Tensor Core GPU Architecture     - NVIDIA Ampere Architecture In-Depth</p>"},{"location":"polaris/hardware-overview/machine-overview/#login-nodes","title":"Login nodes","text":"<p>There are four login nodes available to users for editing code, building code, submitting / monitoring jobs, checking usage (<code>sbank</code>), etc..  Their full hostnames are <code>polaris-login-N.hsn.cm.polaris.alcf.anl.gov</code>  for <code>N</code> equal to <code>01</code> through <code>04</code>; there are an additional two login nodes that are not user-accessible which are used for running services such as JupyterHub. The various compilers and libraries are present on the logins, so most users should be able to build their code.  However, if your build requires the physical presence of the GPU, you will need to build on a compute node.</p> <p>All users share the same login nodes so please be courteous and respectful of your fellow users.  For example, please do not run computationally or IO intensive pre- or post-processing on the logins and keep the parallelism of your builds to a reasonable level.</p> POLARIS LOGIN DESCRIPTION PER NODE AGGREGATE Processor (Note 1) 2.0 GHz 7713 2 12 Cores/Threads AMD Zen 3 (Milan) 128/256 768/1536 RAM (Note 2) DDR4 512 GiB 3 TiB GPUs (Note 3) No GPUs 0 0 Local SSD None 0 0 <p>Note 1: 256MB shared L3 cache, 512KB L2 cache per core, 32 KB L1 cache per core Note 2: 8 memory channels rated at 204.8 GiB/s per socket Note 3: If your build requires the physical presence of a GPU you will need to build on a compute node.</p>"},{"location":"polaris/hardware-overview/machine-overview/#gateway-nodes","title":"Gateway nodes","text":"<p>There are 50 gateway nodes.  These nodes are not user accessible, but are used transparently for access to the storage systems.  Each node has a single 200 Gbps HDR IB card for access to the storage area network.  This gives a theoretical peak bandwidth of 1250 GB/s which is approximately the aggregate bandwidth of the global file systems (1300 GB/s).</p>"},{"location":"polaris/hardware-overview/machine-overview/#storage","title":"Storage","text":"<p>Polaris has access to the ALCF global file systems.  Details on storage can be found here.</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/","title":"NVIDIA Nsight tools","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#references","title":"References","text":"<p>NVIDIA Nsight Systems Documentation NVIDIA Nsight Compute Documentation</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#introduction","title":"Introduction","text":"<p>NVIDIA\u00ae Nsight\u2122 Systems provides developers a system-wide visualization of an applications performance. Developers can optimize bottlenecks to scale efficiently across any number or size of CPUs and GPUs on Polaris. For further optimizations to compute kernels developers should use Nsight Compute.</p> <p>The NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. </p> <p>In addition, the baseline feature of this tool allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface,  metric collection, and can be extended with analysis scripts for post-processing results.</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#common-part-on-polaris","title":"Common part on Polaris","text":"<p>Build your application for Polaris, and then submit your job script to Polaris or start an interactive job mode on Polaris as follows: <pre><code>$ qsub -I -l select=1 -l walltime=1:00:00 -l filesystems=home:grand -q debug -A &lt;project-name&gt;\n\n$ module load cudatoolkit-standalone/11.8.0 \n$ module li\n\nCurrently Loaded Modules:\n  1) craype-x86-rome          6) craype/2.7.15        11) cray-pals/1.1.7\n  2) libfabric/1.11.0.4.125   7) cray-dsmml/0.2.2     12) cray-libpals/1.1.7\n  3) craype-network-ofi       8) cray-mpich/8.1.16    13) PrgEnv-nvhpc/8.3.3\n  4) perftools-base/22.05.0   9) cray-pmi/6.1.2       14) craype-accel-nvidia80\n  5) nvhpc/21.9              10) cray-pmi-lib/6.0.17  15) cudatoolkit-standalone/11.8.0\n\n$ nsys --version\nNVIDIA Nsight Systems version 2022.4.2.1-df9881f\n\n$ ncu --version\nNVIDIA (R) Nsight Compute Command Line Profiler\nCopyright (c) 2018-2022 NVIDIA Corporation\nVersion 2022.3.0.0 (build 31729285) (public-release)\n</code></pre></p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-systems","title":"Nsight Systems","text":"<p>Run your application with Nsight Systems as follows: <pre><code>$ nsys profile -o {output_filename} --stats=true ./{your_application}\n</code></pre></p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-compute","title":"Nsight Compute","text":"<p>Run your application with Nsight Compute. <pre><code>$ ncu --set detailed -k {kernel_name} -o {output_filename} ./{your_application}\n</code></pre></p> <p>Remark: Without -o option, Nsight Compute provides performance data as a standard output</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#post-processing-the-profiled-data","title":"Post-processing the profiled data","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#post-processing-via-cli","title":"Post-processing via CLI","text":"<pre><code>$ nsys stats {output_filename}.qdrep\n$ ncu -i {output_filename}.ncu-rep  \n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#post-processing-on-your-local-system-via-gui","title":"Post-processing on your local system via GUI","text":"<ul> <li>Install NVIDIA Nsight Systems and NVIDIA Nsight Compute after downloading both of them from the  NVIDIA Developer Zone.  Remark: Local client version should be the same as or newer than NVIDIA Nsight tools on Polaris. </li> <li>Download nsys output files (i.e., ending with .qdrep and . sqlite) to your local system, and then open them with NVIDIA Nsight Systems on your local system.  </li> <li>Download ncu output files (i.e., ending with .ncu-rep) to your local system, and then open them with NVIDIA Nsight Compute on your local system.  </li> </ul>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#more-options-for-performance-analysis-with-nsight-systems-and-nsight-compute","title":"More options for performance analysis with Nsight Systems and Nsight Compute","text":"<pre><code>$ nsys --help\n$ ncu --help\n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#a-quick-example","title":"A quick example","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-systems_1","title":"Nsight Systems","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#running-a-stream-benchmark-with-nsight-systems","title":"Running a stream benchmark with Nsight Systems","text":"<pre><code>jkwack@x3008c0s13b1n0:~/BabelStream/build_polaris&gt; nsys profile -o JKreport-nsys-BableStream --stats=true ./cuda-stream\nWarning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\nCollecting data...\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1368294.603 0.00039     0.00044     0.00039     \nMul         1334324.779 0.00040     0.00051     0.00041     \nAdd         1358476.737 0.00059     0.00060     0.00059     \nTriad       1366095.332 0.00059     0.00059     0.00059     \nDot         1190200.569 0.00045     0.00047     0.00046     \nProcessing events...\nSaving temporary \"/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.qdstrm\" file to disk...\n\nCreating final output files...\nProcessing [===============================================================100%]\nSaved report file to \"/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.qdrep\"\nExporting 7675 events: [===================================================100%]\n\nExported successfully to\n/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.sqlite\n\nCUDA API Statistics:\n\n Time(%)  Total Time (ns)  Num Calls  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)           Name         \n -------  ---------------  ---------  ------------  ------------  ------------  ------------  ---------------------\n    41.5      197,225,738        401     491,834.8       386,695       592,751      96,647.5  cudaDeviceSynchronize\n    35.4      168,294,004          4  42,073,501.0       144,211   167,547,885  83,649,622.0  cudaMalloc           \n    22.5      106,822,589        103   1,037,112.5       446,617    20,588,840   3,380,727.4  cudaMemcpy           \n     0.4        1,823,597        501       3,639.9         3,166        24,125       1,228.9  cudaLaunchKernel     \n     0.2        1,166,186          4     291,546.5       130,595       431,599     123,479.8  cudaFree             \n\nCUDA Kernel Statistics:\n\n Time(%)  Total Time (ns)  Instances  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)                             Name                           \n -------  ---------------  ---------  ------------  ------------  ------------  -----------  ----------------------------------------------------------\n    24.5       58,415,138        100     584,151.4       582,522       585,817        543.0  void add_kernel&lt;double&gt;(const T1 *, const T1 *, T1 *)     \n    24.4       58,080,329        100     580,803.3       579,802       582,586        520.5  void triad_kernel&lt;double&gt;(T1 *, const T1 *, const T1 *)   \n    18.3       43,602,345        100     436,023.5       430,555       445,979      2,619.5  void dot_kernel&lt;double&gt;(const T1 *, const T1 *, T1 *, int)\n    16.5       39,402,677        100     394,026.8       392,444       395,708        611.5  void mul_kernel&lt;double&gt;(T1 *, const T1 *)                 \n    16.1       38,393,119        100     383,931.2       382,556       396,892      1,434.1  void copy_kernel&lt;double&gt;(const T1 *, T1 *)                \n     0.2          523,355          1     523,355.0       523,355       523,355          0.0  void init_kernel&lt;double&gt;(T1 *, T1 *, T1 *, T1, T1, T1)    \n\nCUDA Memory Operation Statistics (by time):\n\n Time(%)  Total Time (ns)  Count  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)      Operation     \n -------  ---------------  -----  ------------  ------------  ------------  -----------  ------------------\n   100.0       61,323,171    103     595,370.6         2,399    20,470,146  3,439,982.0  [CUDA memcpy DtoH]\n\nCUDA Memory Operation Statistics (by size):\n\n Total (MB)  Count  Average (MB)  Minimum (MB)  Maximum (MB)  StdDev (MB)      Operation     \n ----------  -----  ------------  ------------  ------------  -----------  ------------------\n    805.511    103         7.820         0.002       268.435       45.361  [CUDA memcpy DtoH]\n\nOperating System Runtime API Statistics:\n\n Time(%)  Total Time (ns)  Num Calls  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)        Name     \n -------  ---------------  ---------  ------------  ------------  ------------  ------------  --------------\n    85.9      600,896,697         20  30,044,834.9         3,477   100,141,768  42,475,064.1  poll          \n    13.5       94,610,402      1,201      78,776.4         1,002    11,348,375     402,562.6  ioctl         \n     0.2        1,374,312         79      17,396.4         3,486       434,715      48,015.2  mmap64        \n     0.1          877,705         51      17,209.9         1,031       748,723     104,491.6  fopen         \n     0.1          741,969         12      61,830.8        17,272       256,852      64,706.5  sem_timedwait \n     0.1          529,563        120       4,413.0         1,292        20,579       2,134.3  open64        \n     0.0          251,602          4      62,900.5        57,337        72,126       6,412.6  pthread_create\n     0.0           93,461         18       5,192.3         1,011        19,386       4,401.0  mmap          \n     0.0           37,621         11       3,420.1         1,302        11,672       2,867.6  munmap        \n     0.0           35,735          9       3,970.6         1,723         6,251       1,477.2  fgetc         \n     0.0           33,533          1      33,533.0        33,533        33,533           0.0  fgets         \n     0.0           26,832         13       2,064.0         1,452         3,366         542.6  write         \n     0.0           21,341          5       4,268.2         1,213         9,738       3,378.3  putc          \n     0.0           20,838          6       3,473.0         1,763         6,853       1,801.1  open          \n     0.0           17,016         10       1,701.6         1,523         1,834          96.9  read          \n     0.0           11,430          8       1,428.8         1,082         1,583         151.9  fclose        \n     0.0            6,202          1       6,202.0         6,202         6,202           0.0  pipe2         \n     0.0            5,961          2       2,980.5         2,254         3,707       1,027.4  socket        \n     0.0            5,670          2       2,835.0         2,795         2,875          56.6  fwrite        \n     0.0            5,481          1       5,481.0         5,481         5,481           0.0  connect       \n     0.0            5,279          2       2,639.5         1,743         3,536       1,267.8  fread         \n     0.0            1,082          1       1,082.0         1,082         1,082           0.0  bind          \n\nReport file moved to \"/home/jkwack/BabelStream/build_polaris/JKreport-nsys-BableStream.qdrep\"\nReport file moved to \"/home/jkwack/BabelStream/build_polaris/JKreport-nsys-BableStream.sqlite\"\n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#reviewing-the-nsight-systems-data-via-gui","title":"Reviewing the Nsight Systems data via GUI","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-compute_1","title":"Nsight Compute","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#running-a-stream-benchmark-with-nsight-compute-for-triad_kernel","title":"Running a stream benchmark with Nsight Compute for triad_kernel","text":"<pre><code>jkwack@x3008c0s13b1n0:~/BabelStream/build_polaris&gt; ncu --set detailed -k triad_kernel -o JKreport-ncu_detailed-triad_kernel-BableStream ./cuda-stream\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\n==PROF== Connected to process 56600 (/home/jkwack/BabelStream/build_polaris/cuda-stream)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1331076.105 0.00040     0.00042     0.00041     \nMul         1304696.608 0.00041     0.00043     0.00042     \nAdd         1322600.587 0.00061     0.00062     0.00061     \nTriad       1327.700    0.60654     0.62352     0.61106     \nDot         850376.762  0.00063     0.00070     0.00065     \n==PROF== Disconnected from process 56600\n==PROF== Report: /home/jkwack/BabelStream/build_polaris/JKreport-ncu_detailed-triad_kernel-BableStream.ncu-rep\n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#reviewing-the-nsight-compute-data-via-gui","title":"Reviewing the Nsight Compute data via GUI","text":""},{"location":"polaris/programming-models/kokkos-polaris/","title":"Kokkos","text":""},{"location":"polaris/programming-models/kokkos-polaris/#kokkos_1","title":"Kokkos","text":"<p>Kokkos Core implements a programming model in C++ for writing performance portable applications targeting all major HPC platforms. For that purpose it provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. It currently can use Serial and OpenMP (threads) for CPU execution spaces (\"backends\") and CUDA, HIP, SYCL, and OpenMPTarget for GPU execution spaces. By convention, Kokkos only allows one GPU backend at a time.</p>"},{"location":"polaris/programming-models/kokkos-polaris/#kokkos-documentation","title":"Kokkos Documentation","text":"<ul> <li>Kokkos-core Wiki</li> <li>Kokkos github</li> </ul>"},{"location":"polaris/programming-models/kokkos-polaris/#kokkos-on-polaris","title":"Kokkos on Polaris","text":"<p>The prebuilt Kokkos on polaris includes 3 backends: Serial and OpenMP for CPU execution and CUDA for GPU execution. To use it, run</p> <p><pre><code>module use /soft/modulefiles\nmodule load kokkos\n</code></pre> This sets the following environment variables, some of which are used by <code>cmake</code>:</p> <ul> <li><code>KOKKOS_HOME</code> - path to the <code>lib64/</code>, <code>include/</code> files installed</li> <li><code>LIBRARY_PATH</code> - prepends <code>$KOKKOS_HOME/lib64</code> to this variable used by <code>cmake</code></li> <li><code>CPATH</code> - prepends <code>$KOKKOS_HOME/include</code> to this variable used by <code>cmake</code></li> <li><code>LD_LIBRARY_PATH</code> - prepends <code>$KOKKOS_HOME/lib64</code> to this variable</li> </ul>"},{"location":"polaris/programming-models/kokkos-polaris/#building-a-kokkos-application-using-cmake","title":"Building a Kokkos Application Using <code>cmake</code>","text":"<p>Add these lines to <code>CMakeLists.txt</code>:</p> <pre><code>find_package(Kokkos REQUIRED)\ntarget_link_libraries(myTarget Kokkos::kokkoscore)\n</code></pre> <p>Here is a simple example <code>CMakeLists.txt</code> to compile an example program:</p> <pre><code>cmake_minimum_required(VERSION 3.22)\nproject(buildExample)\nfind_package(Kokkos REQUIRED)\n\nset(buildExample_SOURCE_DIR \".\")\n\nset(top_SRCS\n  ${buildExample_SOURCE_DIR}/example1.cpp)\n\nset(SOURCE_FILES ${top_SRCS})\n\nadd_executable(example1_sycl_aot ${SOURCE_FILES})\ntarget_link_libraries(example1_sycl_aot Kokkos::kokkoscore)\ntarget_include_directories(example1_sycl_aot PUBLIC ${buildExample_SOURCE_DIR})\n</code></pre> <p>Configure and build it like this:</p> <pre><code>mkdir build\ncd build\ncmake -DCMAKE_CXX_COMPILER=CC -DCMAKE_C_COMPILER=cc ..\nmake\n</code></pre>"},{"location":"polaris/programming-models/kokkos-polaris/#building-a-kokkos-application-using-make","title":"Building a Kokkos Application Using <code>make</code>","text":"<p>Here's an example <code>Makefile</code>:</p> <pre><code># KOKKOS_HOME set via:\n#   module load kokkos\n\n# You can look at the first lines of $KOKKOS_HOME/KokkosConfigCommon.cmake to\n# see the flags used in cmake configuration of the kokkos library build. The\n# default Kokkos module on Polaris was built with PrgEnv-nvhpc and includes\n# Serial, OpenMP (threads) and CUDA backends. So you should have that\n# environment module loaded and include compiler flags for cuda and openmp:\n\n# Cray MPI wrapper for C++ and C compilers:\nCXX=CC\nCC=cc\n\nCPPFLAGS=-cuda -fopenmp\nLDFLAGS=\n\nLDFLAGS=$(CPPFLAGS) $(LDFLAGS)\nLDLIBS=-L$(KOKKOS_HOME)/lib64 -lkokkoscore -lkokkossimd -lpthread\n\nSRCS=example1.cpp\nOBJS=$(subst .cpp,.o,$(SRCS))\n\nall: example1_polaris\n\nexample1_polaris: $(OBJS)\n        $(CXX) $(LDFLAGS) -o example1_polaris $(OBJS) $(LDLIBS)\n\nexample1.o: example1.cpp\n\nclean:\n        rm -f $(OBJS)\n\ndistclean: clean\n        rm -f example1_polaris\n</code></pre>"},{"location":"polaris/programming-models/kokkos-polaris/#configuring-your-own-kokkos-build-on-polaris","title":"Configuring Your Own Kokkos Build on Polaris","text":"<p>Here are recommended environment settings and configuration to build your own kokkos libraries on Polaris:</p>"},{"location":"polaris/programming-models/kokkos-polaris/#environment","title":"Environment","text":"<p>To match what was done in the centrally-built kokkos associated with the modules discussed above, use the default programming environment <code>PrgEnv-nvhpc</code>, and use the Cray wrapper <code>CC</code> as the C++ compiler. To build Kokkos, you'll need cmake. You may also use <code>PrgEnv-gnu</code> to build kokkos (also using the Cray wrapper <code>CC</code> as the C++ compiler).</p> <p>To use C++17, you'll need to work around a bug with the current <code>PrgEnv-nvhpc/8.3.3</code> environment by loading a cudatoolkit-standalone module:</p> <pre><code>module load cmake cudatoolkit-standalone/11.6.2\n</code></pre>"},{"location":"polaris/programming-models/kokkos-polaris/#cmake-configuration","title":"CMake Configuration","text":"<p>This example builds three backends: OpenMP, Serial, and Cuda.</p> <pre><code>git clone git@github.com:kokkos/kokkos.git\ncd kokkos\nmkdir build\ncd build\n\ncmake\\\n -DCMAKE_BUILD_TYPE=RelWithDebInfo\\\n -DCMAKE_INSTALL_PREFIX=\"./install\"\\\n -DCMAKE_CXX_COMPILER=CC\\\n -DKokkos_ENABLE_OPENMP=ON\\\n -DKokkos_ENABLE_SERIAL=ON\\\n -DKokkos_ARCH_ZEN3=ON\\\n -DKokkos_ARCH_AMPERE80=ON\\\n -DKokkos_ENABLE_CUDA=ON\\\n -DKokkos_ENABLE_AGGRESSIVE_VECTORIZATION=ON\\\n -DKokkos_ENABLE_TESTS=OFF\\\n -DBUILD_TESTING=OFF\\\n -DKokkos_ENABLE_CUDA_LAMBDA=ON\\\n -DKokkos_ENABLE_IMPL_DESUL_ATOMICS=OFF\\\n -DCMAKE_CXX_STANDARD=17\\\n ..\n\nmake -j16 -l16 install\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/","title":"OpenMP","text":""},{"location":"polaris/programming-models/openmp-polaris/#overview","title":"Overview","text":"<p>The OpenMP API is an open standard for parallel programming. The specification document can be found here: https://www.openmp.org. The specification describes directives, runtime routines, and environment variables that allow an application developer to express parallelism (e.g. shared memory multiprocessing and device offloading). Many compiler vendors provide implementations of the OpenMP specification (https://www.openmp.org/specifications).</p>"},{"location":"polaris/programming-models/openmp-polaris/#setting-the-environment-to-use-openmp-on-polaris","title":"Setting the environment to use OpenMP on Polaris","text":"<p>Many of the programming environments available on Polaris have OpenMP support.</p> module OpenMP CPU support? OpenMP GPU support? PrgEnv-nvhpc yes yes llvm yes yes PrgEnv-gnu yes no PrgEnv-cray yes yes* <p>*Currently PrgEnv-cray is not recommended for OpenMP offload.</p> <p>By default, the PrgEnv-nvhpc module is loaded. To switch to other modules, you can use <code>module switch</code>.</p>"},{"location":"polaris/programming-models/openmp-polaris/#using-prgenv-nvhpc","title":"Using PrgEnv-nvhpc","text":"<p>This is loaded by default, so there's no need to load additional modules. You can confirm that it is loaded by running <code>module list</code> to check that PrgEnv-nvhpc is in the list.</p>"},{"location":"polaris/programming-models/openmp-polaris/#using-llvm","title":"Using LLVM","text":"<p>To use the LLVM module, load the following. <pre><code>module load mpiwrappers/cray-mpich-llvm\nmodule load cudatoolkit-standalone\n</code></pre></p> <p>See the the LLVM compiling page here for more information.</p>"},{"location":"polaris/programming-models/openmp-polaris/#using-prgenv-gnu","title":"Using PrgEnv-gnu","text":"<p>To switch from PrgEnv-nvhpc to PrgEnv-gnu you can run:</p> <pre><code>module switch PrgEnv-nvhpc PrgEnv-gnu\n</code></pre> <p>The gcc/gfortran on Polaris was not built with GPU support. To use OpenMP on the CPU, you need to unload craype-accel-nvidia80:</p> <pre><code>module unload craype-accel-nvidia80\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#using-prgenv-cray","title":"Using PrgEnv-cray","text":"<p>To switch from PrgEnv-nvhpc to PrgEnv-cray you can run:</p> <pre><code>module switch PrgEnv-nvhpc PrgEnv-cray\n</code></pre> <p>To use OpenMP on the CPU only, also unload craype-accel-nvidia80:</p> <pre><code>module unload craype-accel-nvidia80\n</code></pre> <p>To use OpenMP on the GPU, load cudatoolkit-standalone, although this is not recommended at the moment. <pre><code>module load cudatoolkit-standalone\n</code></pre></p>"},{"location":"polaris/programming-models/openmp-polaris/#building-on-polaris","title":"Building on Polaris","text":"<p>The following table shows what compiler and flags to use with which PrgEnv:</p> module compiler flags PrgEnv-nvhpc cc/CC/ftn (nvc/nvc++/nvfortran) -mp=gpu -gpu=cc80 llvm mpicc/mpicxx (clang/clang++) -fopenmp -fopenmp-targets=nvptx64-nvidia-cuda PrgEnv-gnu cc/CC/ftn (gcc/g++/gfortran) -fopenmp PrgEnv-cray cc/CC/ftn -fopenmp <p>For example to compile a simple code hello.cpp:</p>"},{"location":"polaris/programming-models/openmp-polaris/#for-prgenv-nvhpc-after-loading-the-modules-as-discussed-above-we-would-use","title":"For PrgEnv-nvhpc, after loading the modules as discussed above we would use:","text":"<pre><code>CC -mp=gpu -gpu=cc80 hello.cpp\nftn -mp=gpu -gpu=cc80 hello.F90\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#for-llvm-after-loading-the-modules-as-discussed-above","title":"For LLVM, after loading the modules as discussed above:","text":"<pre><code>mpicxx -fopenmp -fopenmp-targets=nvptx64-nvidia-cuda hello.cpp \n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#for-prgenv-gnu-after-loading-the-modules-as-discussed-above-we-would-use","title":"For PrgEnv-gnu, after loading the modules as discussed above we would use:","text":"<pre><code>CC -fopenmp hello.cpp\nftn -fopenmp hello.F90\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#for-prgenv-cray-after-loading-the-modules-as-discussed-above-we-would-use","title":"For PrgEnv-cray, after loading the modules as discussed above we would use:","text":"<pre><code>CC -fopenmp hello.cpp\nftn -fopenmp hello.F90\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#running-on-polaris","title":"Running on Polaris","text":"<p>To run, you can run the produced executable or with mpiexec in a job script, and then submit the script to the Polaris queue, like:</p> <pre><code>$ cat submit.sh\n#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l walltime=0:30:00\n#PBS -q debug \n#PBS -A Catalyst\n#PBS -l filesystems=home:eagle\n\ncd ${PBS_O_WORKDIR}\n mpiexec -n 1 ./executable\n$ # submit to the queue:\n$ qsub -l select=1:system=polaris -l walltime=0:30:00 -l filesystems=home:eagle -q debug -A Catalyst ./submit.sh\n</code></pre> <p>In the above, having the PBS options in the script and on the command line is redundant, but we put it there to show both ways of launching. This submits the script to one node in the debug queue on Polaris, requesting 30 min and the eagle and home filesystems. It will charge project Catalyst for the time.</p> <p>More details for setting up the job script are in Job Scheduling and Execution section.</p>"},{"location":"polaris/programming-models/openmp-polaris/#example","title":"Example","text":"<pre><code>$ cat hello.cpp\n#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt;\n\nint main( int argv, char** argc ) {\n\n  printf( \"Number of devices: %d\\n\", omp_get_num_devices() );\n\n  #pragma omp target\n  {\n    if( !omp_is_initial_device() )\n      printf( \"Hello world from accelerator.\\n\" );\n    else\n      printf( \"Hello world from host.\\n\" );\n  }\n  return 0;\n}\n\n$ cat hello.F90\nprogram  main\n  use omp_lib\n  implicit none\n  integer flag\n\n  write(*,*) \"Number of devices:\", omp_get_num_devices()\n\n  !$omp target map(from:flag)\n    if( .not. omp_is_initial_device() ) then\n      flag = 1\n    else\n      flag = 0\n   endif\n  !$omp end target\n\n   if( flag == 1 ) then\n      print *, \"Hello world from accelerator\"\n   else\n      print *, \"Hello world from host\"\n   endif\n\n end program main\n\n$ # To compile\n$ CC -mp=gpu -gpu=cc80 hello.cpp -o c_test\n$ ftn -mp=gpu -gpu=cc80 hello.F90 -o f_test\n\n$ # To run \n$ mpiexec -n 1 ./c_test\nNumber of devices: 4\nHello world from accelerator.\n$ mpiexec -n 1 ./f_test\n Number of devices:            4\n Hello world from accelerator\n</code></pre>"},{"location":"polaris/programming-models/sycl-polaris/","title":"SYCL","text":"<p>SYCL (pronounced \u2018sickle\u2019) is a royalty-free, cross-platform abstraction layer that enables code for heterogeneous processors to be written using standard ISO C++ with the host and kernel code for an application contained in the same source file.</p> <ul> <li>Specification: https://www.khronos.org/sycl/</li> <li>Source code of the compiler: https://github.com/intel/llvm</li> <li>ALCF Tutorial: https://github.com/argonne-lcf/sycltrain</li> </ul> <pre><code>module load oneapi\n</code></pre> <p>:warning: This module (compilers, libraries) gets built periodically from the latest open-source rather than releases. As such, these compilers will get new features and updates quickly that may break on occasion.</p>"},{"location":"polaris/programming-models/sycl-polaris/#dependencies","title":"Dependencies","text":"<ul> <li>SYCL programming model is supported through <code>oneapi</code> compilers that were built from source-code</li> <li>Loading this module switches the default programming environment to GNU and with the following dependencies</li> <li>PrgEnv-gnu</li> <li>cudatoolkit-standalone</li> <li>Environment Variable set: <code>SYCL_DEVICE_SELECTOR=ext_oneapi_cuda:gpu</code></li> </ul>"},{"location":"polaris/programming-models/sycl-polaris/#example-memory-intilization","title":"Example (memory intilization)","text":"<pre><code>#include &lt;sycl/sycl.hpp&gt;\nint main(){\nconst int N= 100;\nsycl::queue Q;\nfloat *A = sycl::malloc_shared&lt;float&gt;(N, Q);\nstd::cout &lt;&lt; \"Running on \"\n&lt;&lt; Q.get_device().get_info&lt;sycl::info::device::name&gt;()\n&lt;&lt; \"\\n\";\n// Create a command_group to issue command to the group\nQ.parallel_for(N, [=](sycl::item&lt;1&gt; id) { A[id] = 0.1 * id; }).wait();\nfor (size_t i = 0; i &lt; N; i++)\nstd::cout &lt;&lt; \"A[ \" &lt;&lt; i &lt;&lt; \" ] = \" &lt;&lt; A[i] &lt;&lt; std::endl;\nreturn 0;\n}\n</code></pre> <p>Compile and Run <pre><code>$ clang++ -std=c++17 -sycl-std=2020 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 main.cpp\n$ ./a.out\n</code></pre></p>"},{"location":"polaris/programming-models/sycl-polaris/#example-using-gpu-aware-mpi","title":"Example (using GPU-aware MPI)","text":"<pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;sycl/sycl.hpp&gt;\n// Modified from NERSC website:\n// https://docs.nersc.gov/development/programming-models/mpi\nint main(int argc, char *argv[]) {\nint myrank, num_ranks;\ndouble *val_device;\ndouble *val_host;\nchar machine_name[MPI_MAX_PROCESSOR_NAME];\nint name_len=0;\nMPI_Init(&amp;argc, &amp;argv);\nMPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);\nMPI_Comm_size(MPI_COMM_WORLD, &amp;num_ranks);\nMPI_Get_processor_name(machine_name, &amp;name_len);\nsycl::queue q{sycl::gpu_selector_v};\nstd::cout &lt;&lt; \"Rank #\" &lt;&lt; myrank &lt;&lt; \" runs on: \" &lt;&lt; machine_name\n&lt;&lt; \", uses device: \"\n&lt;&lt; q.get_device().get_info&lt;sycl::info::device::name&gt;() &lt;&lt; \"\\n\";\nMPI_Barrier(MPI_COMM_WORLD);\nint one=1;\nval_host = (double *)malloc(one*sizeof(double));\nval_device = sycl::malloc_device&lt;double&gt;(one,q);\nconst size_t size_of_double = sizeof(double);\n*val_host = -1.0;\nif (myrank != 0) {\nstd::cout &lt;&lt; \"I am rank \" &lt;&lt; myrank\n&lt;&lt; \" and my initial value is: \" &lt;&lt; *val_host &lt;&lt; \"\\n\";\n}\nif (myrank == 0) {\n*val_host = 42.0;\nq.memcpy(val_device,val_host,size_of_double).wait();\nstd::cout &lt;&lt; \"I am rank \" &lt;&lt; myrank\n&lt;&lt; \" and will broadcast value: \" &lt;&lt; *val_host &lt;&lt; \"\\n\";\n}\nMPI_Bcast(val_device, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\ndouble check = 42.0;\nif (myrank != 0) {\n//Device to Host\nq.memcpy(val_host,val_device,size_of_double).wait();\nassert(*val_host == check);\nstd::cout &lt;&lt; \"I am rank \" &lt;&lt; myrank\n&lt;&lt; \" and received broadcast value: \" &lt;&lt; *val_host &lt;&lt; \"\\n\";\n}\nsycl::free(val_device,q);\nfree(val_host);\nMPI_Finalize();\nreturn 0;\n}\n</code></pre> <p>Load Modules</p> <pre><code>module load oneapi\nmodule load mpiwrappers/cray-mpich-oneapi\nexport MPICH_GPU_SUPPORT_ENABLED=1\n</code></pre> <p>Compile and Run</p> <p><pre><code>$ mpicxx -L/opt/cray/pe/mpich/8.1.16/gtl/lib -lmpi_gtl_cuda -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 main.cpp\n$ mpiexec -n 2 --ppn 2 --depth=1 --cpu-bind depth ./set_affinity_gpu_polaris.sh ./a.out\n</code></pre> For further details regarding the arguments passed to <code>mpiexec</code> command shown above, please visit the Job Scheduling and Execution section. A simple example describing the details and execution of the <code>set_affinity_gpu_polaris.sh</code> file can be found here.</p> <p>Note: By default, there is no GPU-aware MPI library linking support.  The example above shows how the user can enable the linking by specifying the path to the GTL (GPU Transport Layer) library (<code>libmpi_gtl_cuda</code>) to the link line.</p>"},{"location":"polaris/programming-models/sycl-polaris/#oneapi-math-kernel-library-onemkl-interfaces","title":"oneAPI Math Kernel Library (oneMKL) Interfaces","text":"<p>oneMKL Interfaces is an open-source implementation of the oneMKL Data Parallel C++ (DPC++) interface according to the oneMKL specification. It works with multiple devices (backends) using device-specific libraries underneath.</p> <p>oneMKL is part of oneAPI. Various backend supported are shown below. More Information here. | User Application | Third-Party Library                                          | |------------------|--------------------------------------------------------------| |                  | cuBLAS     | | oneMKL interface | cuSOLVER | |                  | cuRAND     |</p>"},{"location":"polaris/programming-models/sycl-polaris/#example-using-onemklgemm","title":"Example (using onemkl::gemm)","text":"<p>The following snippet shows how to compile and run a SYCL code with oneMKL library. For instance, a GPU-based GEMM is performed using <code>mkl::gemm</code> API and the results are compared to a CPU-based GEMM performed using the traditional blas (e.g., AOCL-BLIS) library. <pre><code>#include &lt;limits&gt;\n#include &lt;random&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;oneapi/mkl.hpp&gt;  // ONEMKL GPU header\n#include &lt;cblas.h&gt;         // BLIS   CPU header\n// Matrix size constants\n#define SIZE 4800 // Must be a multiple of 8.\n#define M SIZE / 8\n#define N SIZE / 4\n#define P SIZE / 2\n//////////////////////////////////////////////////////////////////////////////////////////\nbool ValueSame(double a, double b) { return std::fabs(a - b) &lt; 1.0e-08; }\nint VerifyResult(double *c_A, double *c_B) {\nbool MismatchFound = false;\nfor (size_t i = 0; i &lt; M; i++) {\nfor (size_t j = 0; j &lt; P; j++) {\nif (!ValueSame(c_A[i * P + j], c_B[i * P + j])) {\nstd::cout &lt;&lt; \"fail - The result is incorrect for element: [\" &lt;&lt; i &lt;&lt; \", \" &lt;&lt; j\n&lt;&lt; \"], expected: \" &lt;&lt; c_A[i * P + j] &lt;&lt; \" , but got: \" &lt;&lt; c_B[i * P + j]\n&lt;&lt; std::endl;\nMismatchFound = true;\n}\n}\n}\nif (!MismatchFound) {\nstd::cout &lt;&lt; \"SUCCESS - The results are correct!\" &lt;&lt; std::endl;\nreturn 0;\n} else {\nstd::cout &lt;&lt; \"FAIL - The results mis-match!\" &lt;&lt; std::endl;\nreturn -1;\n}\n}\n//////////////////////////////////////////////////////////////////////////////////////////\nint main() {\nstd::random_device rd;  // Will be used to obtain a seed for the random number engine\nstd::mt19937 gen(rd()); // Standard mersenne_twister_engine seeded with rd()\nstd::uniform_real_distribution&lt;&gt; dis(1.0, 2.0);\n// C = alpha * op(A) * op(B)  + beta * C\noneapi::mkl::transpose transA = oneapi::mkl::transpose::nontrans;\noneapi::mkl::transpose transB = oneapi::mkl::transpose::nontrans;\n// matrix data sizes\nint m = M;\nint n = P;\nint k = N;\n// leading dimensions of data\nint ldA = k;\nint ldB = n;\nint ldC = n;\n// set scalar fp values\ndouble alpha = 1.0;\ndouble beta = 0.0;\n// 1D arrays on host side\ndouble *A;\ndouble *B;\ndouble *C_host_onemkl, *C_cblas;\nA = new double[M * N]{};\nB = new double[N * P]{};\nC_cblas = new double[M * P]{};\nC_host_onemkl = new double[M * P]{};\n// prepare matrix data with ROW-major style\n// A(M, N)\nfor (size_t i = 0; i &lt; M; i++)\nfor (size_t j = 0; j &lt; N; j++)\nA[i * N + j] = dis(gen);\n// B(N, P)\nfor (size_t i = 0; i &lt; N; i++)\nfor (size_t j = 0; j &lt; P; j++)\nB[i * P + j] = dis(gen);\nstd::cout &lt;&lt; \"Problem size: c(\" &lt;&lt; M &lt;&lt; \",\" &lt;&lt; P &lt;&lt; \") = a(\" &lt;&lt; M &lt;&lt; \",\" &lt;&lt; N &lt;&lt; \") * b(\" &lt;&lt; N\n&lt;&lt; \",\" &lt;&lt; P &lt;&lt; \")\" &lt;&lt; std::endl;\n// Resultant matrix: C_cblas\ncblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, m, n, k, alpha, A, ldA, B, ldB, beta,\nC_cblas, ldC);\n// Resultant matrix: C_onemkl\nsycl::queue q(sycl::property_list{sycl::property::queue::in_order{}});\nstd::cout &lt;&lt; \"Device: \" &lt;&lt; q.get_device().get_info&lt;info::device::name&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\ndouble* A_dev        = sycl::malloc_device&lt;double&gt;(M*N, q);\ndouble* B_dev        = sycl::malloc_device&lt;double&gt;(N*P, q);\ndouble* C_dev_onemkl = sycl::malloc_device&lt;double&gt;(M*P, q);\nq.memcpy(A_dev, A, (M*N) * sizeof(double));\nq.memcpy(B_dev, B, (N*P) * sizeof(double));\nauto gemm_event = oneapi::mkl::blas::column_major::gemm(q, transB, transA, n, m, k, alpha, B_dev, ldB, A_dev, ldA, beta, C_dev_onemkl, ldC);\nq.memcpy(C_host_onemkl, C_dev_onemkl, (M*P) * sizeof(double));\nq.wait();\nstd::cout &lt;&lt; \"Verify results between OneMKL &amp; CBLAS: \";\nint result_cblas = VerifyResult(C_cblas, C_host_onemkl);\ndelete[] A;\ndelete[] B;\ndelete[] C_cblas;\ndelete[] C_host_onemkl;\nsycl::free(A_dev, q);\nsycl::free(B_dev, q);\nsycl::free(C_dev_onemkl, q);\nreturn result_cblas;\n}\n</code></pre></p> <p>Compile and Run The user would need to provide paths the math-libraris as shown below. Also please provide AOCL library for CPU GEMM by <code>module load aocl</code>. Environment variables <code>MKLROOT</code> is defined with <code>oneapi</code> module &amp; <code>AOCL_ROOT</code> is defined with <code>aocl</code> module. Note: Please pay attention to the linker options for AOCL &amp; oneMKL libraries. <pre><code>$ clang++ -std=c++17 -sycl-std=2020 -O3 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 -L$AOCL_ROOT/lib -lblis -L$MKLROOT/lib -lonemkl sycl_onemkl_gemm.cpp -o sycl_onemkl_gemm.out\n</code></pre></p>"},{"location":"polaris/visualization/paraview/","title":"Paraview on Polaris","text":"<p>Note</p> <p>At this time, we only support client/server mode where the user must manually launch the server on Polaris.</p>"},{"location":"polaris/visualization/paraview/#setting-up-paraview","title":"Setting up Paraview","text":"<p>From your local client select Connect, either from the File menu, or by clicking on the icon circled below:</p> <p> </p> <p>A new window will open where you can configure a server. Click on Add Server:</p> <p></p> <p>Give your server a name, select Client/Server, localhost, and a TCP port (8000 in this example)</p> <p></p> <p>Click \"Configure\". In the next window there is an option to set up how Paraview server will be launched, and the default is \"Manual\". Leave it on \"Manual\" and click \"Save\".</p> <p>You will use these settings when establishing the connection.</p>"},{"location":"polaris/visualization/paraview/#launching-the-paraview-server-on-polaris","title":"Launching the Paraview server on Polaris","text":"<p>You can launch an interactive session on Polaris compute nodes with the following command (adjust parameters as needed to match your allocation, desired number of nodes, queue, walltime, and filesystems):</p> <pre><code>qsub -l walltime=01:00:00 -l select=2 -A yourallocation -q debug -I -l filesystems=home:grand\n</code></pre> <p>When the job starts you will receive a prompt on your head node like this:</p> <pre><code>username@x3005c0s7b0n0:~&gt;\n</code></pre> <p>Make a note of the node hostname (<code>x3005c0s7b0n0</code> in the example above). You can also get this information from <code>qstat -fx jobID</code></p> <p>Now load the Paraview module</p> <pre><code>username@x3005c0s7b0n0:~&gt; module load paraview\n\nLmod is automatically replacing \"nvhpc/21.9\" with \"gcc/11.2.0\".\n\n----------------------------------------------------------------------------------\n     Paraview v5.11.0 successfully loaded\n----------------------------------------------------------------------------------\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-mpich/8.1.16\n</code></pre> <p>and launch the Paraview server with</p> <pre><code>srizzi@x3005c0s7b0n0:~&gt; mpirun -n 8 pvserver --server-port=8000\nWaiting for client...\nConnection URL: cs://x3005c0s7b0n0:8000\nAccepting connection(s): x3005c0s7b0n0:8000\n</code></pre> <p>In this case <code>pvserver</code> will be listening on TCP port 8000 of your head node. You can change this port if you want.</p>"},{"location":"polaris/visualization/paraview/#creating-a-tunnel-over-ssh","title":"Creating a tunnel over ssh","text":"<p>We need to establish an ssh tunnel to connect client to server. On your local machine open a new terminal and type:</p> <pre><code>ssh -v -N -L 8000:x3005c0s7b0n0:8000 polaris.alcf.anl.gov\n</code></pre> <p>where 8000 is a TCP port and <code>x3005c0s7b0n0</code> the name of your head node. Adjust these values accordingly.</p> <p>Among multiple lines with debug information,  you should see something like:</p> <pre><code>debug1: Local connections to LOCALHOST:8000 forwarded to remote address x3005c0s7b0n0:8000\n</code></pre> <p>Keep this terminal open for the duration of your session to keep the ssh tunnel active.</p> <p>Now you are ready to launch your Paraview client locally. Keep in mind that client and servers versions must match. The Paraview version currently deployed on Polaris is 5.11.0</p>"},{"location":"polaris/visualization/paraview/#connecting-to-paraview-server","title":"Connecting to Paraview server","text":"<p>Connect your Paraview client to the server configuration you created above. You can select Connect, either from the File menu, or the icon circled in the figure:</p> <p> </p> <p>and selecting the configuration you created in a previous step.</p> <p>The connection should point to:</p> <pre><code>localhost:8000\n</code></pre> <p>In the terminal where you launched the server you will see now that the connection is established. Note that Paraview may take a few seconds to connect. This is normal behavior.</p> <pre><code>username@x3005c0s7b0n0:~&gt; mpirun -n 8 pvserver --server-port=8000\nWaiting for client...\nConnection URL: cs://x3005c0s7b0n0:8000\nAccepting connection(s): x3005c0s7b0n0:8000\nClient connected.\n</code></pre> <p>At this point you can use Paraview normally.</p>"},{"location":"polaris/workflows/balsam/","title":"Balsam","text":"<p>Balsam is a Python-based workflow manager that helps users execute large numbers of jobs, potentially with interjob dependencies, track job outcomes, and manage postprocessing analysis. A Balsam Site runs on a node with access to the job scheduler, where it can submit and monitor jobs. Overall job state is aggregated on the Balsam Server, making job data from all Sites accessible from any individual site (or the user's laptop), via the command-line interface or the Python API. To get information on how to use the command line tool, you can type <code>balsam --help</code> in your shell.</p> <p>Full documentation for Balsam is available online.</p> <p>Balsam requires Python 3.7+. To install Balsam on Polaris, first set up a virtual Python environment:</p> <pre><code>module load conda\nconda activate base\npython -m venv env\nsource env/bin/activate\npip install --upgrade pip\npip install --pre balsam\n</code></pre> <p>To use Balsam, users need an account on the Balsam server.  Users can get an account by contacting the ALCF Help Desk.  Once a user has an account, they can login and make a new site.  A Balsam site is a project space for your workflow. You will be prompted to select what machine (Polaris) you are working on when creating a new site:</p> <pre><code>balsam login\nbalsam site init -n new-site new-site\ncd new-site\nbalsam site start\n</code></pre> <p>See the Balsam documentation for full details.</p>"},{"location":"polaris/workflows/libensemble/","title":"libEnsemble","text":"<p>libEnsemble is a Python toolkit for running dynamic ensembles of calculations. Users provide generator and simulator functions to express their ensembles, where the generator can steer the ensemble based on previous results. A library of example functions is available which can be modified as needed. These functions can submit external executables at any scale and in a portable way. System details are detected, and dynamic resource management is provided. libEnsemble can be used in a consistent manner on laptops, clusters, and supercomputers with minimal required dependencies.</p>"},{"location":"polaris/workflows/libensemble/#getting-libensemble-on-polaris","title":"Getting libEnsemble on Polaris","text":"<p>libEnsemble is provided on Polaris in the conda module:</p> <pre><code>module load conda\nconda activate base\n</code></pre> <p>See the docs for more details on using python on Polaris.</p> Example: creating virtual environment and updating libEnsemble      E.g., to create a virtual environment that allows installation of     further packages with pip:      <pre><code>python -m venv /path/to-venv --system-site-packages\n. /path/to-venv/bin/activate\n</code></pre>      Where /path/to-venv can be anywhere you have write access.     For future uses just load the conda module and run the activate line.      You can also ensure you are using the latest version of libEnsemble:      <pre><code>pip install libensemble\n</code></pre>"},{"location":"polaris/workflows/libensemble/#libensemble-examples","title":"libEnsemble examples","text":"<p>For a very simple example of using libEnsemble see the Simple Sine tutorial</p> <p>For an example that runs a small ensemble using a C application (offloading work to the GPU), see the GPU app tutorial. The required files for the this tutorial can be found in this directory. Also, see the video demo.</p> <p>Note that when initializing the MPIExecutor on Polaris (run_libe_forces.py in the example), you currently need to use the following options to pick up the correct MPI runner:</p> <pre><code>exctr = MPIExecutor(custom_info={'mpi_runner':'mpich', 'runner_name':'mpiexec'})\n</code></pre>"},{"location":"polaris/workflows/libensemble/#job-submission","title":"Job Submission","text":"<p>libEnsemble runs on the compute nodes on Polaris using either <code>multi-processing</code> or <code>mpi4py</code>. The user can set the number of workers for maximum concurrency. libEnsemble will detect the nodes available from the PBS environment and use these for running simulations. Polaris supports running multiple concurrent simulations on each node if desired,</p> <p>A simple example batch script for a libEnsemble use case that runs four workers on one node:</p> <pre><code>    #!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l walltime=00:15:00\n#PBS -l filesystems=home:grand\n#PBS -q debug\n#PBS -A &lt;myproject&gt;\nexport MPICH_GPU_SUPPORT_ENABLED=1\ncd $PBS_O_WORKDIR\npython run_libe_forces.py --comms local --nworkers 4\n</code></pre> <p>The script can be run with:</p> <pre><code>qsub submit_libe.sh\n</code></pre> <p>Or you can run an interactive session with:</p> <pre><code>qsub -A &lt;myproject&gt; -l select=1 -l walltime=15:00 -lfilesystems=home:grand -qdebug -I\n</code></pre>"},{"location":"polaris/workflows/libensemble/#further-links","title":"Further links","text":"<p>Docs: https://libensemble.readthedocs.io  GitHub: https://github.com/Libensemble/libensemble</p>"},{"location":"polaris/workflows/parsl/","title":"Parsl on Polaris","text":"<p>Parsl is a flexible and scalable parallel programming library for Python.</p> <p>-- Parsl Documentation</p> <p>For many applications, managing an ensemble of jobs into a workflow is a critical step that can easily become a performance bottleneck.  Many tools exist to address this, of which <code>parsl</code> is just one.  On this page, we'll highlight some of the key pieces of information about <code>parsl</code> that are relevant to Polaris.  <code>Parsl</code> is also extensively documented, has a dedicated Slack Channel, and a large community of users and developers beyond ALCF.  We encourage you to engage with the <code>parsl</code> community for support with <code>parsl</code> specific questions, and for Polaris-specific questions or problems, please contact support@alcf.anl.gov.</p>"},{"location":"polaris/workflows/parsl/#getting-parsl-on-polaris","title":"Getting Parsl on Polaris","text":"<p>You can install parsl building off of the <code>conda</code> modules.  You have some flexibility in how you want to extend the <code>conda</code> module to include parsl, but here is an example way to do it:</p> <pre><code># Load the Conda Module (needed everytime you use parsl)\nmodule load conda\nconda activate\n# Create a virtual env that uses the conda env as the system packages.\n# Only do the next line on initial set up:\npython -m venv --system-site-packages /path/to/your/virtualenv\n# Load the virtual env (every time):\nsource /path/to/your/virtualenv/bin/activate\n# Install parsl (only once)\npip install parsl\n</code></pre>"},{"location":"polaris/workflows/parsl/#using-parsl-on-polaris","title":"Using Parsl on Polaris","text":"<p>Parsl has a variety of possible configuration settings.  As an example, we provide the configuration below that will run one task per GPU:</p> <pre><code>from parsl.config import Config\n# PBSPro is the right provider for Polaris:\nfrom parsl.providers import PBSProProvider\n# The high throughput executor is for scaling to HPC systems:\nfrom parsl.executors import HighThroughputExecutor\n# You can use the MPI launcher, but may want the Gnu Parallel launcher, see below\nfrom parsl.launchers import MpiExecLauncher, GnuParallelLauncher\n# address_by_interface is needed for the HighThroughputExecutor:\nfrom parsl.addresses import address_by_interface\n# For checkpointing:\nfrom parsl.utils import get_all_checkpoints\n# Adjust your user-specific options here:\nrun_dir=\"/lus/grand/projects/yourproject/yourrundir/\"\nuser_opts = {\n\"worker_init\":      f\"source /path/to/your/virtualenv/bin/activate; cd {run_dir}\", # load the environment where parsl is installed\n\"scheduler_options\":\"#PBS -l filesystems=home:eagle:grand\" , # specify any PBS options here, like filesystems\n\"account\":          \"YOURPROJECT\",\n\"queue\":            \"debug-scaling\",\n\"walltime\":         \"1:00:00\",\n\"nodes_per_block\":  3, # think of a block as one job on polaris, so to run on the main queues, set this &gt;= 10\n\"cpus_per_node\":    32, # Up to 64 with multithreading\n\"available_accelerators\": 4, # Each Polaris node has 4 GPUs, setting this ensures one worker per GPU\n\"cores_per_worker\": 8, # this will set the number of cpu hardware threads per worker.  \n}\ncheckpoints = get_all_checkpoints(run_dir)\nprint(\"Found the following checkpoints: \", checkpoints)\nconfig = Config(\nexecutors=[\nHighThroughputExecutor(\nlabel=\"htex\",\nheartbeat_period=15,\nheartbeat_threshold=120,\nworker_debug=True,\navailable_accelerators=user_opts[\"available_accelerators\"], # if this is set, it will override other settings for max_workers if set\ncores_per_worker=user_opts[\"cores_per_worker\"],\naddress=address_by_interface(\"bond0\"),\ncpu_affinity=\"block-reverse\",\nprefetch_capacity=0,\nstart_method=\"spawn\",  # Needed to avoid interactions between MPI and os.fork\nprovider=PBSProProvider(\nlauncher=MpiExecLauncher(bind_cmd=\"--cpu-bind\", overrides=\"--depth=64 --ppn 1\"),\n# Which launcher to use?  Check out the note below for some details.  Try MPI first!\n# launcher=GnuParallelLauncher(),\naccount=user_opts[\"account\"],\nqueue=user_opts[\"queue\"],\nselect_options=\"ngpus=4\",\n# PBS directives (header lines): for array jobs pass '-J' option\nscheduler_options=user_opts[\"scheduler_options\"],\n# Command to be run before starting a worker, such as:\nworker_init=user_opts[\"worker_init\"],\n# number of compute nodes allocated for each block\nnodes_per_block=user_opts[\"nodes_per_block\"],\ninit_blocks=1,\nmin_blocks=0,\nmax_blocks=1, # Can increase more to have more parallel jobs\ncpus_per_node=user_opts[\"cpus_per_node\"],\nwalltime=user_opts[\"walltime\"]\n),\n),\n],\ncheckpoint_files = checkpoints,\nrun_dir=run_dir,\ncheckpoint_mode = 'task_exit',\nretries=2,\napp_cache=True,\n)\n</code></pre>"},{"location":"polaris/workflows/parsl/#special-notes-for-polaris","title":"Special notes for Polaris","text":"<p>On Polaris, there is a known bug where python applications launched with <code>mpi</code> and that use <code>fork</code> to spawn processes can sometimes have unexplaned hangs.  For this reason, it is recommended to use <code>start_method=\"spawn\"</code> on Polaris when using the <code>MpiExecLauncher</code> as is shown in the example config above.  Alternatively, another solution is to use the <code>GNUParallelLauncher</code> which uses <code>GNU Parallel</code> to spawn processes.  <code>GNU Parallel</code> can be loaded in your environment with the command <code>module load gnu-parallel</code>.  Both of these approaches will circumvent the hang issue from using <code>fork</code>.</p>"},{"location":"polaris/workflows/parsl/#updates","title":"Updates","text":"<p>For <code>parsl</code> versions after July 2023, the <code>address</code> passed in the <code>HighThroughputExecutor</code> needs to be set to <code>address = address_by_interface(\"bond0\")</code>.  With <code>parsl</code> versions prior to July 2023, it was recommended to use <code>address = address_by_hostname()</code> on Polaris, but with later versions this will not work on Polaris (or any other machine).</p>"},{"location":"policies/alcf-acknowledgement-policy/","title":"ALCF Acknowledgement Policy","text":"<p>As a U.S. Department of Energy user facility dedicated to the advancement of scientific discoveries, the Argonne Leadership Computing Facility (ALCF) provides unique computing resources and expertise to a user community that is bound by certain policies designed to acknowledge and promote the work of others as well as the resources used to accomplish this work.</p> <p>The ALCF requests your continued compliance with the terms of your program or discretionary award, specifically with regard to acknowledgments in publications and presentations based on work done with ALCF resources. Also, please forward your accepted publication citations to pubs@alcf.anl.gov.</p>"},{"location":"policies/alcf-acknowledgement-policy/#ai-testbeds-publication-guidance","title":"AI Testbeds Publication Guidance","text":"<p>To publish technical reports and research papers using the ALCF AI testbeds, we request you to provide us with a draft of your paper prior to submission by emailing a copy to us at support@alcf.anl.gov. We will work closely with the AI testbed vendors to provide feedback in a timely manner. We strongly recommend you engage us and the vendors early and often in this process to help us facilitate your research objectives.</p> <p>For guidance on acknowledgements, please see the following sample policies:</p>"},{"location":"policies/alcf-acknowledgement-policy/#alcf-only-acknowledgement","title":"ALCF Only Acknowledgement","text":"<p>This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357.</p>"},{"location":"policies/alcf-acknowledgement-policy/#incitealcf-acknowledgement","title":"INCITE/ALCF Acknowledgement","text":"<p>An award of computer time was provided by the Innovative and Novel Computational Impact on Theory and Experiment (INCITE) program. This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357.</p>"},{"location":"policies/alcf-acknowledgement-policy/#incitealcfolcf-acknowledgement","title":"INCITE/ALCF/OLCF Acknowledgement","text":"<p>An award of computer time was provided by the Innovative and Novel Computational Impact on Theory and Experiment (INCITE) program. This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under contract DE-AC02-06CH11357. This research also used resources of the Oak Ridge Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725.</p>"},{"location":"policies/facility-policies/","title":"ALCF Facility Policies","text":"<p>Be sure to familiarize yourself with the various policies and procedures for ALCF users, categorized below.</p>"},{"location":"policies/facility-policies/#accounts","title":"Accounts","text":"<p>All holders of user accounts must comply with ALCF and Argonne National Laboratory computing usage policies, including meeting certain security requirements and executing specific science- or engineering-related computing jobs.</p> <ul> <li>Accounts Policy</li> <li>Account Sponsorship and Retention Policy</li> <li>User Authentication Policy</li> </ul>"},{"location":"policies/facility-policies/#alcf-acknowledgement-policy","title":"ALCF Acknowledgement Policy","text":"<p>As a U.S. Department of Energy Office of Science User Facility dedicated to the advancement of scientific discovery, the ALCF requests that its users acknowledge and promote the work of others and the resources with which this work was accomplished.</p> <ul> <li>ALCF Acknowledgement Policy</li> </ul>"},{"location":"policies/facility-policies/#data-and-allocation","title":"Data and Allocation","text":"<p>These policies detail data and software usage, as well as pullback and refunds of computing hours.</p> <ul> <li>Data Policy</li> <li>Pullback Policy</li> <li>Refund Policy</li> <li>Software Policy</li> </ul>"},{"location":"policies/facility-policies/#quarterly-reports","title":"Quarterly Reports","text":"<p>The ALCF is required to report the progress and accomplishments of its allocation projects. Policies are detailed by award type.</p> <ul> <li>Quarterly Report Policy</li> </ul>"},{"location":"policies/facility-policies/#queue-and-scheduling-policies","title":"Queue and Scheduling Policies","text":"<ul> <li>General Policies</li> </ul>"},{"location":"policies/accounts/account-sponsorship-retention-policy/","title":"Account Sponsorship &amp; Retention Policy","text":"<p>This page is designed to help you understand the different types of accounts that you will encounter at the ALCF. The policy outlined reviews the responsibilities of an account holder, an account sponsor, and those of a foreign national.</p>"},{"location":"policies/accounts/account-sponsorship-retention-policy/#alcf-account-types","title":"ALCF Account Types","text":"<p>Annual: This account applies to users who are not ALCF Regular Employees. The default renewal date (account deactivation date) for the account is a year from the day the account was requested. These accounts are renewed annually and must be approved by an ALCF Staff member or a Project PI (also known as the \u201capprover\u201d). Users are required to update their account information and agree to the Terms of Use each year. Users need to be a part of an active project for their account to be renewed.</p> <p>Permanent: This account applies to individuals who are Regular Employees within the ALCF and CPS Divisions. If you hold this type of account, periodic renewal is not necessary.</p> <p>Note: Foreign Nationals have a second date (apart from their account deactivation date) that controls their account access. Accounts held by foreign nationals require paperwork referred to as an ANL-593 (or just 593 for shorthand). This paperwork is also required for any on-site access, and also applies to computer accounts. DOE requirements state that the ALCF is to disable any account with expired 593 paperwork. </p> <p>A notification system has been established that issues a warning notice to users when expiration approaches and requests action to ensure that accounts are not needlessly turned off. An approval from the project PI is required to renew ANL 593 for project members that are foreign nationals.</p>"},{"location":"policies/accounts/account-sponsorship-retention-policy/#your-responsibilities-as-an-account-approver","title":"Your responsibilities as an account approver","text":"<p>If you approve any accounts, please take note of the following roles and responsibilities:</p> <p>By approving someone for an account at the ALCF, you are accepting responsibility for the account applicant and confirming that this individual is who they claim to be and is thus entitled to work on our computers. Do not simply \"rubber stamp\" any account application that claims you as an account approver/project PI.</p> <p>You are also responsible for approving account renewal requests. When an account is about to expire, we send a warning notification to the account holder. Among other things, the account holder is asked to contact the approver (the PI of any of the active projects the account holder is associated with) if they wish to renew their account. We cannot and will not extend someone's account without an approval. An important aspect of this process to note is that inaction will result in the account becoming deactivated on the expiration date.</p> <p>You are also responsible for approving ANL 593 renewals requests. When an account\u2019s 593 is about to expire, we send a warning notification to the account holder. Among other things, the account holder is asked to contact the approver (the PI of any of the active projects the account holder is associated with) if they wish to renew their 593. We cannot and will not extend someone's 593 without an approval. An important aspect of this process to note is that inaction will result in the account becoming deactivated at the expiration date.</p>"},{"location":"policies/accounts/account-sponsorship-retention-policy/#account-retention-policy","title":"Account Retention Policy","text":"<p>Accounts can exist in one of three states:</p> <ul> <li>Active: The active state is normal for an account.</li> <li>Inactive: The inactive state occurs when an account expires, and the ability to use ALCF resources is removed by changing the active status of the account to inactive. All files continue to exist in the user's home directory. An account will remain in the inactive state for at least 90 days before moving to the next state.</li> <li>Deleted: After 90 days, an inactive account will be deleted. This removes all references to the account from the system (except the accounts database), including any files and home directories.</li> </ul> <p>Users with inactive or deleted accounts can request a reactivation visiting https://accounts.alcf.anl.gov and clicking on the \u201cReactivate An Account\u201d link.</p>"},{"location":"policies/accounts/accounts-policy/","title":"Accounts Policy","text":"<p>All holders of user accounts must abide by all appropriate Argonne Leadership Computing Facility and Argonne National Laboratory computing usage policies.  The policy details are outlined in the following documents:</p> <ul> <li>ANL's Information Technology Access Agreement</li> <li>Addendum to ANL's Information Technology Access Agreement</li> </ul> <p>These are described at the time of the account request and include requirements such as using a sufficiently strong password, appropriate use of the system, and so on. Any user not following these requirements will have their account disabled.</p> <p>Furthermore, ALCF resources are intended to be used as a computing resource for specific computational science or engineering work, not as a general-purpose computing system. </p> <p>If someone is using the system extensively but not carrying out any computational activities, their account could be disabled.</p>"},{"location":"policies/accounts/user-authentication-policy/","title":"User Authentication Policy","text":"<p>Users of the ALCF systems are required to use a SafeNet token (physical or mobile) one time password, multifactor authentication system.</p> <p>This document explains the policies users must follow regarding SafeNet tokens for accessing the ALCF systems.</p>"},{"location":"policies/accounts/user-authentication-policy/#multifactor-authentication","title":"MultiFactor Authentication","text":"<p>\"Authentication systems are frequently described by the authentication factors that they incorporate. The three factors often considered as the cornerstone of authentication are: Something you know (for example, a password); Something you have (for example, an ID badge or a cryptographic key); and Something you are (for example, a voice print or other biometric measurement).\" -- NIST iTL Bulletin, Aug 2004</p> <p>By the NIST guidelines for identification and authentication (NIST 800-53, Revision 3, Control IA-2), ALCF aims for a Moderate level of security controls. All production systems in ALCF require multifactor authentication for users with network and local (privileged and non-privileged accounts) using the SafeNet tokens.</p>"},{"location":"policies/accounts/user-authentication-policy/#mobile-and-physical-tokens","title":"Mobile and Physical Tokens","text":"<p>ALCF provides every user of the production resources a physical or mobile token called a SafeNet Token. This is named after the company that developed the key fob and mobile software (the organization is now called SafeNet). \"Both tokens use AES-256 bit encryption to generate OTPs [One Time Passwords] comprised of digits, digits and letters or digits, letters and special characters...\"</p> <p>When you receive your physical token, it will be initialized, but it will have no access privileges until you have contacted us to verify your identity.</p> <p>At the end of your account or project lifecycle, please return the token to the ALCF help desk:</p> <p>ALCF Service Desk Argonne National Laboratory 9700 South Cass Avenue Building 240 Argonne, IL 60439</p>"},{"location":"policies/accounts/user-authentication-policy/#protect-your-passcode-token","title":"Protect Your Passcode token","text":"<p>Your passcode token should be protected by you as carefully as your credit cards or house keys. If your token is lost, stolen, or damaged, please contact us immediately so that we can deactivate the token and prevent unauthorized access. Sharing of tokens is strictly forbidden. Please do not mark on the token or alter it in any way.</p>"},{"location":"policies/accounts/user-authentication-policy/#more-information","title":"More information","text":"<p>[New User Guide] (http://www.alcf.anl.gov/user-guides/new-user-guide)</p> <p>Using Passcode Tokens</p>"},{"location":"policies/accounts/user-authentication-policy/#references","title":"References","text":"<ul> <li>http://www.itl.nist.gov/lab/bulletns/bltnaug04.htm</li> <li>http://csrc.nist.gov/publications/nistpubs/800-53-Rev3/sp800-53-rev3-final_updated-errata_05-01-2010.pdf</li> <li>http://csrc.nist.gov/publications/nistpubs/800-63-1/SP-800-63-1.pdf</li> <li>https://safenet.gemalto.com/multi-factor-authentication/authenticators/one-time-password-otp/</li> </ul>"},{"location":"policies/data-and-software-policies/data-policy/","title":"Data Policy","text":""},{"location":"policies/data-and-software-policies/data-policy/#alcf-data-confidentiality","title":"ALCF Data Confidentiality","text":"<p>The Argonne Leadership Computing Facility (ALCF) network is an open-research network. Because our resources and networks are open to many users and cannot be protected at a partitioned level, we cannot guarantee complete security for any data that resides here. It is up to users to provide the security they need.</p> <p>Data is not encrypted at rest. Data transferred via SSH (i.e., scp) is encrypted in transmission using SSH\u2019s mechanisms (e.g., AES256, etc.). Data transferred via Globus ( GridFTP) isn't normally fully encrypted.  The GridFTP control channel is encrypted, but the data channel by default is not (though the authentication processes for both channels are encrypted).  If you need full encryption of the data stream, you need to explicitly select \"encrypt transfer\" in the \"Transfer &amp; Timer Options\" in the Globus UI or use equivalent options in the CLI or transfer API if you're using those. More information here https://docs.globus.org/faq/security</p> <p>The basic level of protection provided is UNIX file level permissions; it is the user's responsibility to ensure that file permissions and umasks are set to match their needs.</p> <p>NOTE: The default permissions and umasks are group and world readable. For help determining or setting file permissions or umasks, or creating a UNIX group, contact support@alcf.anl.gov.</p>"},{"location":"policies/data-and-software-policies/data-policy/#alcf-staff-with-root-privileges","title":"ALCF Staff with Root Privileges","text":"<p>ALCF resource administrators with root privileges are not constrained by the file permissions, and they have the capability to open and/or copy all files on the system. They can also assume a user\u2019s identity on the system. There is no audit trail for access, touching, or moving data; however, ALCF staff does not view or modify project data unless directed by a PI or project member to help debug a problem. Data may be touched or accessed by the filesystem itself if data needs to be repaired or verified for integrity after a filesystem event (e.g., a fsck).</p> <p>The ALCF resources are Federal resources and are the property of the United States Government. Any or all uses of this system and all files on this system may be intercepted, monitored, recorded, copied, audited, inspected, and disclosed to authorized site, Department of Energy, and law enforcement personnel, as well as authorized officials of other agencies, both domestic and foreign.</p> <p>Administrators use elevated privileges for maintenance and system management. Following are instances where ALCF staff might look at your files: - We maintain copies of all .error, .output, and Cobalt log files and may review them to determine if a job failure was due to user error or a system failure. - If you request our assistance via any mechanism (for example, support ticket, direct personal email, in-person, etc.), be aware we may need to view your files using elevated privileges to aid us in resolving your issue.</p>"},{"location":"policies/data-and-software-policies/data-policy/#use-of-proprietarylicensed-software","title":"Use of Proprietary/Licensed Software","text":"<p>All software used on ALCF computers must be appropriately acquired and used according to the appropriate licensing. Possession or use of illegally copied software is prohibited. Likewise, users shall not copy copyrighted software, except as permitted by the owner of the copyright. Currently, the use of export-controlled codes is prohibited.</p>"},{"location":"policies/data-and-software-policies/data-policy/#prohibited-data","title":"Prohibited Data","text":"<p>The ALCF computer systems are operated as research systems and contain only data related to scientific research. Use of ALCF resources to store, manipulate, or remotely access any sensitive or national security information is prohibited unless documented and approved, by the PI and ALCF leadership. </p> <p>This includes, but is not limited to, personally identifiable information (data that falls under the Privacy Act of 1974, 5 U.S.C. 552a), controlled unclassified information (CUI) to include unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), International Traffic in Arms Relations (ITAR), the design or development of nuclear, biological, or chemical weapons, or any weapons of mass destruction. The use of ALCF resources for personal or non-work-related activities is also prohibited.</p>"},{"location":"policies/data-and-software-policies/data-policy/#export-control","title":"Export Control","text":"<p>All principal investigators using ALCF resources and ALCF staff members working with project teams are responsible for knowing whether their project generates any of these prohibited data types or information that falls under Export Control. For questions, contact ALCF Support at support@alcf.anl.gov.</p>"},{"location":"policies/data-and-software-policies/data-policy/#data-storage-systems","title":"Data Storage Systems","text":"<p>Data stored for any length of time on ALCF resources should only be data directly related to work done on any of the ALCF leadership computing systems. Specific policies apply to the three types of data storage systems maintained at ALCF. Read these policies carefully and plan accordingly in terms of space, usage, and data protection.</p>"},{"location":"policies/data-and-software-policies/data-policy/#home-file-system-space","title":"Home File System Space","text":"<p>swift-home</p> <p>The home file system (/home) is intended to hold your executable files, configuration files, etc. It is NOT meant to hold the output from your application runs (use the data/parallel file system for that purpose). The home file system space is generally moderate in size and is the best protected. Because of its size, backups are practical to accomplish.  The system performs tape backups, enabling the recovery of files more than seven days old or recovery from a catastrophic disk failure. Users should email support@alcf.anl.gov if they need assistance. The table below indicates the capabilities and characteristics of each file system.</p> <p>AI Testbed home</p> <p>/home shared across the ALCF AI testbed systems, including the ai testbed's login and compute nodes, is different from mira-home. Default user quota on the ai testbed's home is 1 TB storage and 1,000,000 files. This space is backed up.</p>"},{"location":"policies/data-and-software-policies/data-policy/#team-project-or-campaign-file-system","title":"Team Project or Campaign File System","text":"<p>theta-fs0 and Grand</p> <p>The team project/campaign file system is intended primarily for results output from your computational runs on the ALCF computing systems. This space is accessible to the team members of your project that have an ALCF account. Default storage quota is 1 TB. Consider this space intermediate-term storage. Once any active production and/or analysis is complete and you no longer need regular access to the data, archive it within the ALCF (explained below) or transfer it to your home institution or move it to Eagle to share it with the broader community (explained below). </p> <p>This space has redundancy in the servers and storage but is so large that replication, snapshots, and backups are not practical. Theta-fs0 and Grand are Lustre global parallel file systems. All new projects will be given storage allocations on either Grand or Eagle. Continuing projects (renewals) will have access to theta-fs0. More information on Lustre File Striping Basics: Lustre File Striping Basics  </p> <p>Pullback Policy: Projects that do not use a minimum of 50% of their allocated space after 6 months will be subject to a quota limit reduction.</p> <p>AI Testbed projects file system</p> <p>The team project/campaign file system /projects mounted on AI Testbed's login and compute nodes is intended to facilitate project collaboration and is accessible to the team members of your project that have an ALCF account. /projects on the AI Testbed is different from /projects on Theta, ThetaGPU, and Cooley. Default group storage quota is 2 TB and 2,000,000 files. Please note that this space isn't backed up. Our policy is that data will be purged from disk 6 months after project completion.</p>"},{"location":"policies/data-and-software-policies/data-policy/#shared-community-project-or-campaign-file-system-eagle","title":"Shared Community Project or Campaign File System (Eagle)","text":"<p>The file system Eagle, a Lustre global parallel file system, has community sharing-abilities and is useful for sharing the project/campaign data with the broader research community via Globus. This space does not have redundancy in the servers or storage and is so large that replication, snapshots, and backups are not practical. The table below indicates the capabilities and characteristics of each file system. Default storage quota on Eagle is 1 TB and the default period is 1 year. More information on Lustre File Striping Basics: Lustre File Striping Basics  </p> <p>Eagle Data Pullback Policy:  Projects that do not use a minimum of 50% of their allocated space after 6 months will be subject to a quota limit reduction.</p> <p>Eagle Access Termination Policy:  Project endpoints that have exhibited no activity* for a period of 6 months will be disabled and the storage space will be reclaimed. Notification will be sent to the PI and project members 30 days prior to and the day of the action.</p> <p>Activity is defined as, but not limited to:</p> <ul> <li>Creation of the Globus endpoint</li> <li>Globus transfers to and from the endpoint</li> <li>atime audits of data files indicating access</li> <li>Other factors may include DOIs and citations referring to the project</li> </ul>"},{"location":"policies/data-and-software-policies/data-policy/#archive-space","title":"Archive Space","text":"<p>The archive space is intended for offline storage of results you wish to retain but either have no immediate need to access or no room in your parallel file system space. Archiving capabilities are available via HPSS. The primary HPSS access is via HSI. HTAR is available, but its path length and file size limitations often cause it to fail. Globus Online and GridFTP are clients that can also be used with HPSS.  Due to the possibility of data corruption or loss due to a bad tape, users can request dual writes for particularly critical data. Such requests will be handled on a case-by-case basis.</p>"},{"location":"policies/data-and-software-policies/data-policy/#data-storage-policies","title":"Data Storage Policies","text":""},{"location":"policies/data-and-software-policies/data-policy/#disk-capacity-and-retention-policies","title":"Disk Capacity and Retention Policies","text":"---- /home /lus/theta-fs0  or /projects <sup>*</sup> /lus/grand/projects or /grand lus/eagle/projects or /eagle Default Quota <sup>1</sup> 50 GB 1 TB / 1 million files 1 TB / 1 million files 1 TB / 1 million files Quota Enforcement <sup>2</sup> hard/soft hard/soft hard/soft hard/soft Disk Redundancy <sup>3</sup> dual parity dual parity dual parity dual parity File Server Snapshots <sup>6</sup> (frequency/retained) none none none none File Server Metadata Redundancy yes yes yes yes File Server Metadata Replication <sup>4</sup> yes yes yes yes File Server Data Replication <sup>5</sup> yes yes no no Data Purged from Disk n/a 6 months after project completion <sup>8</sup> 6 months after project completion <sup>8</sup> After 6 months of inactivity (see Eagle Access termination policy listed in the Eagle section above) <sup>8</sup> <p><sup>*</sup> /lus/theta-fs0 does not apply to Polaris</p>"},{"location":"policies/data-and-software-policies/data-policy/#tape-capacity-and-retention-policies","title":"Tape Capacity and Retention Policies","text":"---- /home /lus/theta-fs0  or /projects <sup>*</sup> /lus/grand/projects or /grand lus/eagle/projects or /eagle Automatic Backup to Tape? <sup>7</sup> yes yes no no Archived to Tape Before Deleted from Disk? <sup>9</sup> yes yes no no <ol> <li>While quotas are subject to negotiation on a case-by-case basis, disk space is a finite resource and projects must exercise good data management practices for their own sake and the sake of other users of the facility.  With Lustre, it has become necessary to enforce file quotas as well, which are also negotiable.</li> <li>\u201cHard quota enforcement\u201d means a job will fail when writing output if you exceed the hard quota limit.  \"Soft quota enforcement\" means you may exceed the soft quota limit (but never the higher hard quota value) for up to seven days.  If you do not drop back below the soft quota limit within seven days, writes will begin to fail.</li> <li>Hard drives are in redundancy groups of 10 disks (8 data + 2 parity). In other words, three out of 10 drives would have to fail before data loss occurred.</li> <li>Metadata (i.e., information listing which blocks are part of which files) is written twice to two different storage arrays. Thus, even if an entire array were lost, the metadata would be preserved.</li> <li>Refers to the fact that data (user output) is written twice with each block on two different storage arrays, so that even if an entire array were lost, the data would be preserved.</li> <li>Snapshots are stored in your home directory (see Home File System Space for more info). If you accidentally delete the directory or need a previous version, use the cp command to copy the file back to your home directory.</li> <li>\u201cYes\u201d denotes that ALCF does regular backups without intervention from the user. In case of project data, data is backed up to tape   after a stipulated period (see point 8 below) and is retained for 2 years (subject to change). In all other cases, user is responsible for archiving the data to HPSS or copying it to another facility as desired.</li> <li>The project directory is available on disk for the stipulated period but project quotas are reduced immediately following project end date (except Eagle). Access to the directory will be removed after 90 days. Requests to restore/extend access or reset the quota are reviewed on a case-by-case basis.</li> <li>Users who wish to retain data must archive or transfer their data elsewhere at the end of the project. Users need an active ALCF account to access archived data on HPSS. See Account Retention Policy for more information.</li> </ol>"},{"location":"policies/data-and-software-policies/software-policy/","title":"ALCF Resource Software Use","text":"<p>All software used on ALCF computers must be appropriately acquired and used according to the appropriate licensing. Possession or use of illegally copied software is prohibited. Likewise, users shall not copy copyrighted software, except as permitted by the owner of the copyright. Currently, the use of export-controlled codes is prohibited.</p>"},{"location":"policies/data-and-software-policies/software-policy/#community-software-policy","title":"Community Software Policy","text":"<p>ALCF supports the deployment of community software from active projects on production systems. A project may provide and support a code on ALCF systems for the ALCF user community as described in the [Community Software Service].</p> <p>User deployments are system-specific, and their maintenance is the sole responsibility of the project deploying it. There shall be no expectation of additional support from ALCF, other than for the provisioning of space and integration with the module system. Projects will be provided with an initial module file from a template, with the expectation that they will update and maintain the module, providing paths and instructions so that user communities can access the software.</p>"},{"location":"policies/queue-scheduling/pullback-policy/","title":"Pullback Policy","text":"<p>In an effort to ensure that valuable ALCF computing resources are used judiciously, a pullback policy has been instituted. Projects granted allocations under the INCITE and ALCC programs that have not used a significant amount of their allocation will be evaluated and adjusted during the year following the policies outlined on this page.</p> <p>The figures outlined below represent the maximum amount that will be pulled back from projects after specific dates during the allocation period. The decision to reduce allocations will be made on a case-by-case basis in discussion with the project's primary investigators (PIs).</p>"},{"location":"policies/queue-scheduling/pullback-policy/#incite-pullback-policy","title":"INCITE Pullback Policy","text":"<p>On May 1 of the current INCITE calendar year: - if usage is less than 15% remove up to 15% of the unused balance - if usage is less than 10% remove up to 30% of the unused balance</p> <p>On September 1 of the current INCITE calendar year: - if usage is less than 50% remove up to 33% of the unused balance - if usage is less than 33% remove up to 50% of the unused balance - if usage is less than 10% remove up to 75% of the unused balance</p>"},{"location":"policies/queue-scheduling/pullback-policy/#alcc-pullback-policy","title":"ALCC Pullback Policy","text":"<p>ALCC projects must use 50% of their allocation within the first seven months of the allocation cycle. Any unused time in excess of 50% will be deducted from the project allocation at the end of the seven month period.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/","title":"Queue and Scheduling Policy","text":""},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#general-policy","title":"General Policy","text":"<p>We ask that all users follow good etiquette and be excellent to one another.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#priority","title":"Priority","text":"<p>As with all Argonne Leadership Computing Facility production systems, job priority in the queue is based on several criteria: - positive balance of your project - size (in nodes) of the job, larger jobs receive higher priority - the type of project (e.g. INCITE, ALCC, or discretionary) - job duration - shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#reservations-and-scheduling-policy","title":"Reservations and Scheduling Policy","text":"<p>Some work will require use of Theta that requires deviation from regular policy. On such occasions, normal reservation policy applies. Please send the regular form no fewer than five (5) business days in advance.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#big-run-mondays","title":"Big Run Mondays","text":"<p>As part of our regular maintenance procedures on Mondays, we will promote to the highest priority any jobs in the queued state requesting 802 nodes or more. Promotion is subject to operational discretion.</p> <p>We may also, at our discretion, take the opportunity to promote the priority of capability jobs if the system has been drained of jobs for any other reason.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#monday-maintenance","title":"Monday Maintenance","text":"<p>On Mondays where the ALCF is on a regular business schedule the system may be expected to undergo maintenance from 9:00 am until 5:00 pm US Central Time. The showres command may be used to view pending and active maintenance reservations.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#incitealcc-overburn-policy","title":"INCITE/ALCC Overburn Policy","text":"<p>If an INCITE or ALCC project has exhausted its allocation in the first 11 months of its allocation year, it is eligible for overburn running. At this point, capability jobs submitted by INCITE and ALCC projects will run in the default queue (instead of backfill) for the first 11 months of the allocation year until 125% of the project allocation has been consumed.</p> <p>INCITE and ALCC projects needing additional overburn hours should e-mail support@alcf.anl.gov with a short description of what they plan to do with the additional hours, highlighting specific goals or milestones and the time expected to accomplish them. This will be reviewed by the scheduling committee, allocations committee, and ALCF management. Requests should be submitted 15 days before the start of the next quarter of the allocation year for full consideration. Non-capability jobs from projects that have exhausted their allocation will continue to run in backfill. </p> <p>To be clear, this policy does not constitute a guarantee of extra time, and we reserve the right to prioritize the scheduling of jobs submitted by projects that have not yet used 100% of their allocations, so the earlier that an INCITE or ALCC project exhausts its allocation, the more likely it is to be able to take full advantage of this policy.</p>"},{"location":"policies/queue-scheduling/refund-policy/","title":"Refund Policy","text":"<p>If a system problem affects your run, ALCF will consider a refund of node hours. The ALCF expects all applications to regularly checkpoint, so refunds are typically capped at four hours of runtime for the affected job, unless the problem in question prevented checkpoints. </p> <p>ALCF strongly advises against symlinking between filesystems or hard-coding paths to a different filesytem.</p> <p>To request a refund, send the following information to support@alcf.anl.gov: - Job id - Machine - Reason for refund request</p> <p>For more information, contact support@alcf.anl.gov.</p>"},{"location":"running-jobs/example-job-scripts/","title":"Example Job Scripts","text":"<p>This page contains a small collection of example job scripts users may find useful for submitting their jobs on Polaris. Additional information on PBS and how to submit these job scripts is available here.</p> <p>A simple example using a similar script on Polaris is available in the Getting Started Repo.</p>"},{"location":"running-jobs/example-job-scripts/#cpu-mpi-openmp-examples","title":"CPU MPI-OpenMP Examples","text":"<p>The following <code>submit.sh</code> example submits a 1-node job to Polaris with 16 MPI ranks per node and 2 OpenMP threads per rank. See Queues for details on practical limits to node counts and job times for different sizes of jobs.</p> <p>The <code>hello_affinity</code> program is a compiled C++ code, which is built via <code>make -f Makefile.nvhpc</code> in the linked directory after cloning the Getting Started repository.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:grand\n#PBS -q debug\n#PBS -A Catalyst\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=16\nNDEPTH=2\nNTHREADS=2\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n</code></pre> <p>The following function in the <code>hello_affinity</code> source code is essential for uniquely identifying the CUDA device even when Multi-Instance GPU (MIG) is enabled, as each physical device will be partitioned into multiple virtual devices, each with unique UUIDs differentiated by the last few characters:</p> Identifying physical or virtual GPU by UUID <pre><code>//https://stackoverflow.com/questions/68823023/set-cuda-device-by-uuid\nvoid uuid_print(cudaUUID_t a){\nstd::cout &lt;&lt; \"GPU\";\nstd::vector&lt;std::tuple&lt;int, int&gt; &gt; r = {{0,4}, {4,6}, {6,8}, {8,10}, {10,16}};\nfor (auto t : r){\nstd::cout &lt;&lt; \"-\";\nfor (int i = std::get&lt;0&gt;(t); i &lt; std::get&lt;1&gt;(t); i++)\nstd::cout &lt;&lt; std::hex &lt;&lt; std::setfill('0') &lt;&lt; std::setw(2) &lt;&lt; (unsigned)(unsigned char)a.bytes[i];\n}\nstd::cout &lt;&lt; std::endl;\n}\n</code></pre> <p>NOTE: If you are a <code>zsh</code> user, you will need to ensure ALL submission and shell scripts include the <code>-l</code> flag following <code>#!/bin/bash</code> as seen in the example above to ensure your environment is being instantiated properly. <code>zsh</code> is NOT supported by HPE and support from ALCF will be best effort only.</p> <p>Each Polaris compute node has 1 Milan CPU with a total of 32 physical cores, with each core supporting 2 hardware threads (for a total of 64 logical cores). </p> <p>The process affinity in this example is setup to map each MPI rank to 2 physical cores. Each MPI rank spawns 2 OpenMP threads, so 1 thread per physical core. The OpenMP settings bind each OpenMP thread to a single hardware thread within a core, such that all 32 physical cores are utilized. CPU core IDs <code>32</code> to <code>63</code> are not mapped to any MPI rank, since they correspond to simultaneous multithreaded (SMT) sibling hardware threads that share the execution resources of the core ids <code>0</code> to <code>31</code>, respectively.</p> <ul> <li><code>cd ${PBS_O_WORKDIR}</code> : change into the working directory from where <code>qsub</code> was executed.</li> <li><code>NNODES= `wc -l &lt; $PBS_NODEFILE`</code>: one method for determine the total number of nodes allocated to a job.</li> <li><code>NRANKS_PER_NODE=16</code> : This is a helper variable to set the number of MPI ranks for each node to 16.</li> <li><code>NDEPTH=2</code> : This is a helper variable to space MPI ranks 2 \"slots\" from each other. In this example, individual threads correspond to a slot. This will be used together with the <code>--cpu-bind</code> option from <code>mpiexec</code> and additional binding options are available (e.g. <code>numa</code>, <code>socket</code>, <code>core</code>, etc.).</li> <li><code>NTHREADS=2</code> : This is a helper variable to set the number of OpenMP threads per MPI rank.</li> <li><code>NTOTRANKS=$(( NNODES * NRANKS_PER_NODE))</code> : This is a helper variable calculating the total number of MPI ranks spanning all nodes in the job.</li> </ul> <p>Information on the use of <code>mpiexec</code> is available via <code>man mpiexec</code>. Some notes on the specific options used in the above example follow.</p> <ul> <li><code>-n ${NTOTRANKS}</code> : This is specifying the total number of MPI ranks to start.</li> <li><code>--ppn ${NRANKS_PER_NODE}</code> : This is specifying the number of MPI ranks to start on each node.</li> <li><code>--depth=${NDEPTH}</code> : This is specifying how many cores/threads to space MPI ranks apart on each node.</li> <li><code>--cpu bind depth</code> : This is indicating the number of cores/threads will be bound to MPI ranks based on the <code>depth</code> argument.</li> <li><code>--env OMP_NUM_THREADS=${NTHREADS}</code> : This is setting the environment variable <code>OMP_NUM_THREADS</code> : to determine the number of OpenMP threads per MPI rank.</li> <li><code>--env OMP_PLACES=threads</code> : This is indicating how OpenMP should distribute threads across the resource, in this case across hardware threads.</li> </ul>"},{"location":"running-jobs/example-job-scripts/#hardware-threads","title":"Hardware threads","text":"<p>This example is similar to the previous, but it exhausts all 64 logical cores available on each compute node CPU. We double the number of MPI ranks to 32, one per each physical core. Using <code>--cpu-bind=core</code>, the <code>--depth</code> flag value becomes interpreted by Cray MPICH as spacing in number of physical cores, so <code>NDEPTH=1</code> ensures that rank 0 is bound to CPU core IDs <code>(0,32)</code>, the 2 SMT sibling hardware threads that share the first physical core.</p> <p><pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:grand\n#PBS -q debug\n#PBS -A Catalyst\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=32\nNDEPTH=1\nNTHREADS=2\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind core --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n</code></pre> Many HPC applications do not benefit from utilizing the CPU's SMT2 capabilities, and such software may achieve better performance by using the previous script such that each of the 32 physical cores only runs a single OpenMP thread.</p>"},{"location":"running-jobs/example-job-scripts/#gpu-mpi-examples","title":"GPU MPI Examples","text":"<p>Using the CPU job submission examples above as a baseline, there are not many additional changes needed to enable an application to make use of the 4 NVIDIA A100 GPUs on each Polaris node. In the following 2-node example (because <code>#PBS -l select=2</code> indicates the number of nodes requested), 4 MPI ranks will be started on each node assigning 1 MPI rank to each GPU in a round-robin fashion. A simple example using a similar job submission script on Polaris is available in the Getting Started Repo.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=2:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -j oe\n#PBS -q debug\n#PBS -A Catalyst\n# Enable GPU-MPI (if supported by application)\nexport MPICH_GPU_SUPPORT_ENABLED=1\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=$(nvidia-smi -L | wc -l)\nNDEPTH=8\nNTHREADS=1\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n# For applications that internally handle binding MPI/OpenMP processes to GPUs\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n\n# For applications that need mpiexec to bind MPI ranks to GPUs\n#mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./set_affinity_gpu_polaris.sh ./hello_affinity\n</code></pre> <p>The affinity options <code>NDEPTH=8;</code> and <code>--cpu-bind depth</code> or <code>core</code> are set to ensure that each MPI rank is bound to a separate NUMA node. If OpenMP threading is desired, set <code>NTHREADS=8</code> for each MPI rank to spawn 1 thread per physical core (all in the same NUMA domain that the rank is bound to). The OpenMP-related options are not needed if your application does not use OpenMP. Nothing additional is required on the <code>mpiexec</code> command for applications that internally manage GPU devices and handle the binding of MPI/OpenMP processes to GPUs. A small helper script is available for those with applications that rely on MPI to handle the binding of MPI ranks to GPUs. Some notes on this helper script and other key differences with the early CPU example follow.</p> <p><code>export MPICH_GPU_SUPPORT_ENABLED=1</code></p> <p>For applications that support GPU-enabled MPI (i.e. use MPI to communicate data directly between GPUs), this environment variable is required to enable GPU support in Cray's MPICH. Omitting this will result in a segfault. Support for this also requires that the application was linked against the the GPU Transport Layer library (e.g. -lmpi_gtl_cuda), which is automatically included for users by the <code>craype-accel-nvidia80</code> module in the default environment on Polaris. If this gtl library is not properly linked, then users will see a error message indicating that upon executing the first MPI command that uses a device pointer.</p> <p><code>./set_affinity_gpu_polaris.sh</code></p> <p>This script is useful for those applications that rely on MPI to bind MPI ranks to GPUs on each node. Such a script is not necessary when the application handles process-gpu binding. This script simply sets the environment variable <code>CUDA_VISIBLE_DEVICES</code> to a restricted set of GPUs (e.g. each MPI rank sees only one GPU). Otherwise, users would find that all MPI ranks on a node will target the first GPU likely having a negative impact on performance. An example for this script is available in the Getting Started repo and copied below.   </p>"},{"location":"running-jobs/example-job-scripts/#hardware-threads_1","title":"Hardware threads","text":"<pre><code>#!/bin/bash -l\n#PBS -l select=2:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -q debug\n#PBS -A Catalyst\n# Enable GPU-MPI (if supported by application)\nexport MPICH_GPU_SUPPORT_ENABLED=1\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=$(nvidia-smi -L | wc -l)\nNDEPTH=16\nNTHREADS=16\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n# For applications that internally handle binding MPI/OpenMP processes to GPUs\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind numa --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n\n# For applications that need mpiexec to bind MPI ranks to GPUs\n#mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind numa --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./set_affinity_gpu_polaris.sh ./hello_affinity\n</code></pre> <p>As in the previous hardware threads example, the MPI ranks are spaced apart assuming the user wants to utilize all 64 logical cores (achieved by setting <code>NTHREADS=$NDEPTH=16</code> and <code>--cpu-bind numa</code> here).</p> <p>In this script, we have added <code>-j oe</code> to the list of PBS options; <code>-j oe</code> combines stdout and stderr to the same file and uses the stdout filename provided (if provided). <code>-j eo</code> would do the same but use the stderr filename provided. Without these options, separate files containing stdout and stderr of the job are produced.</p> <p>Here we compare two bare-bones PBS submission scripts for a CUDA example with and without MPI:</p> No MPIWith MPI <pre><code>#!/bin/bash\n#PBS -l select=1\n#PBS -l walltime=00:10:00\n#PBS -q debug\n#PBS -l filesystems=home\n#PBS -A &lt;project-name&gt;\n#PBS -o logs/\n#PBS -e logs/\nmodule load cudatoolkit-standalone/11.8.0\n\n$HOME/ALCFBeginnersGuide/polaris/examples/01_example_cu\n</code></pre> <pre><code>#!/bin/bash\n#PBS -l select=2\n#PBS -l walltime=00:10:00\n#PBS -q debug\n#PBS -l filesystems=home\n#PBS -A &lt;project-name&gt;\n#PBS -o logs/\n#PBS -e logs/\nmodule load cudatoolkit-standalone/11.8.0\n\n# Count number of nodes assigned\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n# set 1 MPI rank per GPU\nNRANKS_PER_NODE=4\n# calculate total ranks\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE}\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} $HOME/ALCFBeginnersGuide/polaris/examples/01_example_mpi\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#setting-gpu-affinity-for-each-mpi-rank","title":"Setting GPU affinity for each MPI rank","text":"<p>The <code>CUDA_VISIBLE_DEVICES</code> environment variable is provided for users to set which GPUs on a node are accessible to an application or MPI ranks started on a node.</p> <p>A copy of the small helper script provided in the Getting Started repo is provided below for reference:</p> GPU affinity script <pre><code>#!/bin/bash -l\nnum_gpus=4\n# need to assign GPUs in reverse order due to topology\n# See Polaris Device Affinity Information https://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho \u201cRANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}\u201d\nexec \"$@\"\n</code></pre> <p>Note</p> <p>The <code>echo</code> command prints a helpful message for the user to confirm the desired mapping is achieved. Users are encouraged to edit this file as necessary for their particular needs.</p> <p>Warning</p> <p>If planning large-scale runs with many thousands of MPI ranks, it is advised to comment out the <code>echo</code> command above so as not to have thousands of lines of output written to <code>stdout</code>.</p>"},{"location":"running-jobs/example-job-scripts/#using-mps-on-the-gpus","title":"Using MPS on the GPUs","text":"<p>Documentation for the NVIDIA Multi-Process Service (MPS) can be found here</p> <p>In the script below, note that if you are going to run this as a multi-node job you will need to do this on every compute node, and you will need to ensure that the paths you specify for <code>CUDA_MPS_PIPE_DIRECTORY</code> and <code>CUDA_MPS_LOG_DIRECTORY</code> do not \"collide\" and end up with all the nodes writing to the same place.</p> <p>An example is available in the Getting Started Repo and discussed below. The local SSDs or <code>/dev/shm</code> or incorporation of the node name into the path would all be possible ways of dealing with that issue.</p> <pre><code>#!/bin/bash -l\nexport CUDA_MPS_PIPE_DIRECTORY=&lt;/path/writeable/by/you&gt;\nexport CUDA_MPS_LOG_DIRECTORY=&lt;/path/writeable/by/you&gt;\nCUDA_VISIBLE_DEVICES=0,1,2,3 nvidia-cuda-mps-control -d\necho \"start_server -uid $( id -u )\" | nvidia-cuda-mps-control\n</code></pre> <p>to verify the control service is running:</p> <pre><code>$ nvidia-smi | grep -B1 -A15 Processes\n</code></pre> <p>and the output should look similar to this:</p> <pre><code>+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n|    1   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n|    2   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n|    3   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>to shut down the service:</p> <p><code>echo \"quit\" | nvidia-cuda-mps-control</code></p> <p>to verify the service shut down properly:</p> <p><code>nvidia-smi | grep -B1 -A15 Processes</code></p> <p>and the output should look like this:</p> <pre><code>+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#using-mps-in-multi-node-jobs","title":"Using MPS in Multi-node Jobs","text":"<p>As stated earlier, it is important to start the MPS control service on each node in a job that requires it.  An example is available in the Getting Started Repo. The helper script <code>enable_mps_polaris.sh</code> can be used to start the MPS on a node.</p> <p><pre><code>#!/bin/bash -l\nexport CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log\nCUDA_VISIBLE_DEVICES=0,1,2,3 nvidia-cuda-mps-control -d\necho \"start_server -uid $( id -u )\" | nvidia-cuda-mps-control\n</code></pre> The helper script <code>disable_mps_polaris.sh</code> can be used to disable MPS at appropriate points during a job script, if needed.</p> <p><pre><code>#!/bin/bash -l\necho quit | nvidia-cuda-mps-control\n</code></pre> In the example job script <code>submit.sh</code> below, MPS is first enabled on all nodes in the job using <code>mpiexec -n ${NNODES} --ppn 1</code> to launch the enablement script using a single MPI rank on each compute node. The application is then run as normally. If desired, a similar one-rank-per-node <code>mpiexec</code> command can be used to disable MPS on all the nodes in a job.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A Catalyst\n#PBS -l filesystems=home:grand:eagle\ncd ${PBS_O_WORKDIR}\n# MPI example w/ 8 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=8\nNDEPTH=8\nNTHREADS=1\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n# Enable MPS on each node allocated to job\nmpiexec -n ${NNODES} --ppn 1 ./enable_mps_polaris.sh\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n\n# Disable MPS on each node allocated to job\nmpiexec -n ${NNODES} --ppn 1 ./disable_mps_polaris.sh\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#single-node-ensemble-calculations-example","title":"Single-node Ensemble Calculations Example","text":"<p>In the script below, a set of four applications are launched simultaneously on a single node. Each application runs on 8 MPI ranks and targets a specific GPU using the <code>CUDA_VISIBLE_DEVICES</code> environment variable. In the first instance, MPI ranks 0-7 will spawn on CPUs 24-31, and GPU 0 is used. This pairing of CPUs and GPU is based on output of the <code>nvidia-smi topo-m</code> command showing which CPUs share a NUMA domain with each GPU. It is important to background processes using <code>&amp;</code> and to <code>wait</code> for all runs to complete before exiting the script or continuing on with additional work. Note, multiple applications can run on the same set of CPU resources, but it may not be optimal depending on the workload. An example is available in the Getting Started Repo.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A Catalyst\n#PBS -l filesystems=home:grand:eagle\n#cd ${PBS_O_WORKDIR}\n# MPI example w/ 8 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=8\nNTHREADS=1\nnvidia-smi topo -m\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\nexport CUDA_VISIBLE_DEVICES=0\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:24:25:26:27:28:29:30:31 ./hello_affinity &amp;\nexport CUDA_VISIBLE_DEVICES=1\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:16:17:18:19:20:21:22:23 ./hello_affinity &amp;\nexport CUDA_VISIBLE_DEVICES=2\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:8:9:10:11:12:13:14:15 ./hello_affinity &amp;\nexport CUDA_VISIBLE_DEVICES=3\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:0:1:2:3:4:5:6:7 ./hello_affinity &amp;\nwait\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#multi-node-ensemble-calculations-example","title":"Multi-node Ensemble Calculations Example","text":"<p>To run multiple concurrent applications on distinct sets of nodes, one simply needs to provide appropriate hostfiles to the <code>mpiexec</code> command. The <code>split</code> unix command is one convenient way to create several unique hostfiles, each containing a subset of nodes available to the job. In the 8-node example below, a total of four applications will be launched on separate sets of nodes. The <code>$PBS_NODEFILE</code> file will be split into several hostfiles, each containing two lines (nodes). These smaller hostfiles are then used as the argument to the <code>--hostfile</code> argument of <code>mpiexec</code> to the launch applications. It is important to background processes using <code>&amp;</code> and to <code>wait</code> for applications to finish running before leaving the script or continuing on with additional work. Note, multiple applications can run on the same set of CPU resources, but it may not be optimal depending on the workload. An example is available in the Getting Started Repo.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=8:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug-scaling\n#PBS -A Catalyst\n#PBS -l filesystems=home:grand:eagle\ncd ${PBS_O_WORKDIR}\n# MPI example w/ multiple runs per batch job\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n# Settings for each run: 2 nodes, 4 MPI ranks per node spread evenly across cores\n# User must ensure there are enough nodes in job to support all concurrent runs\nNUM_NODES_PER_MPI=2\nNRANKS_PER_NODE=4\nNDEPTH=8\nNTHREADS=1\nNTOTRANKS=$(( NUM_NODES_PER_MPI * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} NUM_NODES_PER_MPI= ${NUM_NODES_PER_MPI} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n# Increase value of suffix-length if more than 99 jobs\nsplit --lines=${NUM_NODES_PER_MPI} --numeric-suffixes=1 --suffix-length=2 $PBS_NODEFILE local_hostfile.\n\nfor lh in local_hostfile*\ndo\necho \"Launching mpiexec w/ ${lh}\"\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --hostfile ${lh} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity &amp;\nsleep 1s\ndone\nwait\n</code></pre>"},{"location":"running-jobs/job-and-queue-scheduling/","title":"Running Jobs using PBS","text":""},{"location":"running-jobs/job-and-queue-scheduling/#documentation-tools","title":"Documentation / Tools","text":"<ul> <li>The PBS \"BigBook\": This is really excellent.  We highly suggest you download it and search through it when you have questions.  However, it is big at about 2000 pages / 40MB and contains a bunch of stuff you don't really need, so you can also download the guides separately here:<ul> <li>The PBS Users Guide: This is the users guide.</li> <li>The PBS Reference Guide: This is the Reference Guide.  It shows every option and gives you details on how to format various elements on the command line.</li> </ul> </li> <li>Cobalt qsub options to PBS qsub options: shows how to map cobalt command line options to PBS command line options.  Can be found at the link above.</li> <li><code>qsub2pbs</code>: Installed on Theta and Cooley.  Pass it a Cobalt command line and it will convert it to a PBS command line.  Add the <code>--directives</code> option, and it will output an executable script.  Note that it outputs <code>-l select=system=None</code>.  You would need to change the <code>None</code> to whatever system you wanted to target (<code>polaris</code>, <code>aurora</code>, etc.).</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#introduction","title":"Introduction","text":"<p>At a high level, getting computational tasks run on an HPC system is a two-step process:</p> <ol> <li> <p>You request and get allocated resources (we allocate at the node level, but some facilities you request number of cores and RAM, etc.) on one or more of the systems.    This is accomplished by interacting with the job scheduler / workload manager.  In the ALCF we use PBS Professional.</p> </li> <li> <p>You execute your tasks on those resources.    This is accomplished in your job script by interacting with various system services (MPI, OpenMP, the HPE PALS task launch system, etc.)</p> </li> </ol> <p>Our documentation is organized in two sections aligned with the two steps described above.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Obtaining and managing compute resources at ALCF - General PBS information common to all systems<ul> <li>Definitions and Notes</li> <li>Quick Start</li> <li>qsub - submit a job to run</li> <li>qstat - query the status of jobs/queues</li> <li>qalter - alter a queued job</li> <li>qdel - delete a queued or running job</li> <li>qmove - move a job to a different queue</li> <li>qhold,qrls - place/release a hold on a job in a queue</li> <li>qselect - utility to select jobids that meet criteria</li> <li>qmsg - write a message into a jobs output file</li> <li>qsig - send a signal to a job</li> <li>pbsnodes - Get information about the current state of nodes</li> <li>Using Fakeroot with Singularity</li> </ul> </li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#obtaining-and-managing-compute-resources-at-alcf","title":"Obtaining and managing compute resources at ALCF","text":""},{"location":"running-jobs/job-and-queue-scheduling/#definitions-and-notes","title":"Definitions and Notes","text":"<p><code>chunk</code>: A set of resources allocated as a unit to a job. Specified inside a selection directive. All parts of a chunk come from the same host.  In a typical MPI (Message-Passing Interface) job, there is one chunk per MPI process.</p> <p><code>vnode</code>: A virtual node, or vnode, is an abstract object representing a host or a set of resources which form a usable part of an execution host. This could be an entire host, or a nodeboard or a blade. A single host can be made up of multiple vnodes. Each vnode can be managed and scheduled independently. Each vnode in a complex must have a unique name. Vnodes on a host can share resources, such as node-locked licenses.  PBS operates on vnodes.  A vnode can, and in ALCF often will, represent an entire host, but it doesn't have to.  For instance, there is a mode on Polaris where we could have each physical host look like four vnodes, each with 16 threads, 1/4 of the RAM and one A100.</p> <p><code>ncpus</code>: Number of resources available to execute a program. In ALCF, given the way we configure PBS, this equates to a hardware thread.  For example, a single socket node with a 32 core CPU, each with two hardware threads would report that as ncpus=64.</p> <p><code>ngpus</code>: The number of allocable GPUs on the vnode.  For an NVIDIA A100, this could be one, however, if we enable Multi Instance GPU (MIG) mode and use cgroups it could be as high as 7.</p> <p><code>job</code>: A job equates to a qsub.  A set of resources allocated to you for a period of time.  Your will execute one or more <code>tasks</code> on those resources during your job.</p> <p><code>task</code>: A single execution on the resources of your job, often an <code>mpiexec</code> invocation launched by PALS or PMIx.  You may run one task or many tasks during your job.  You may run tasks sequentially or divide your resources up and run several tasks concurrently.  Also sometimes referred to as job steps.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#quick-start","title":"Quick Start","text":"<p>If you are an ALCF user and are familiar with Cobalt, you will find the PBS commands very similar though the options to qsub are quite different.  Here are the \"Big Four\" commands you will use:</p> <ol> <li><code>qsub</code>: request resources (generally compute nodes) to run your job and start your script/executable on the head node.  Here is the minimal qsub allowed at the ALCF:<ul> <li><code>qsub -A &lt;project&gt; -l select=&lt;# of nodes&gt;,walltime=HH:MM:SS,filesystems=fs1:fs2 &lt;your job script&gt;</code></li> <li>The <code>-A</code>, <code>walltime</code>, and <code>filesystems</code> are mandatory.  You will receive errors if they are not specified.</li> <li>We automatically add <code>-k doe</code> for you.  This streams your output back rather than spooling it and copying it back at the end of the job.  It probably isn't a bad idea to specify it in your script, but we enforce that option, so if you try and change it, you will get an error.</li> <li>It is highly likely you will also want to add <code>-l place=scatter</code> so that each of your chunks (<code>&lt;# of nodes&gt;</code>) gets its own vnode.</li> <li>If you want to run an executable rather than a script replace <code>&lt;your jobs script&gt;</code> in the example above with <code>-- &lt;your executable&gt;</code> (that is dash dash)</li> <li>PBS Documentation: Users Guide, Chapter 2, page UG-11 and Reference Guide Chapter 2, section 2.57, page RG-216</li> </ul> </li> <li><code>qstat</code>: check on the status of your jobs or queues<ul> <li>Try these variations and see which you like best: <code>qstat</code>, <code>qstat -was</code>, <code>qstat -was1</code>, <code>qstat -wan</code>, <code>qstat -wan1</code>. Add <code>-x</code> to see jobs that have completed.  We keep two weeks of history.</li> <li><code>qstat -Q</code> will list all the queues in case you forget.</li> <li>PBS Documentation: Users Guide Sec. 10.2, page UG-175; Reference Guide Sec. 2.55, page RG-200</li> </ul> </li> <li><code>qalter</code>: update your request for resources<ul> <li>Just like qsub, just add a jobid at the end.  Only works before the job starts;</li> <li>If you want to change the walltime to 30 minutes: <code>qalter -l walltime=30:00:00 &lt;jobid&gt;</code></li> <li>PBS Documentation: Users Guide Sec. 9.2, page UG-168; Reference Guide Sec. 2.40, page RG-130</li> </ul> </li> <li><code>qdel</code>: cancel a job that you don't need. This will also kill a running job<ul> <li><code>qdel &lt;jobid&gt;</code></li> <li>PBS Documentation: Users Guide Sec. 9.3, page UG-170; Reference Guide Sec. 2.41, page RG-143</li> </ul> </li> </ol> <p>Note: The page numbers in the PBS guides are unique.  If you search for the specified page number it will take you directly to the relevant page.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qsub-submit-a-job-to-run","title":"<code>qsub</code>: submit a job to run","text":"<p>Users Guide, Chapter 2, page UG-11 and Reference Guide Chapter 2, section 2.57, page RG-216</p> <p>At the ALCF, your qsub will likely use the following parameters:</p> <p><code>qsub -A &lt;project&gt; -k doe -l select=&lt;#&gt;:system=&lt;name&gt;, walltime=HH:MM:SS, filesystems=fs1:fs2, place=scatter &lt;your job script&gt;</code></p> <p>Where:</p> <ul> <li>project is the project name associated with your allocation.  What you check the balance of with the <code>sbank</code> command.  This is a mandatory option at the ALCF.  If you don't include it you will get <code>qsub: Account_Name is required to be set.</code></li> <li>-k doe is telling pbs to stream your output rather than buffer it on the compute nodes and then scp it at the end of the job.  Note we will automatically add this if you don't specify it.  We enforce this option, so if  you try and specify any other output handling you will get an error.</li> <li># of chunks (typically nodes). Each of our systems has a PBS \"resource\" called <code>system</code> defined and set to the system name (polaris, sunspot, etc)</li> <li><code>walltime=HH:MM:SS</code> specifying a wall time is mandatory at the ALCF.  Valid wall times depend on the queue you are using.  There is a table with the queues for each machine at the end of this section and in the machine specific documentation.</li> <li><code>filesystems=fs1:fs2:...</code> Specifying which filesystems your application uses is mandatory at ALCF.  The reason for this is if a filesystem goes down, we have a way of making PBS aware of that and it won't run jobs that need that filesystem.  If you don't specify filesystems you will receive the following error: <code>qsub: Resource: filesystems is required to be set.</code></li> <li><code>place=scatter</code> is telling PBS you want each of your chunks on a separate vnode.  By default, PBS will pack your chunks to get maximum utilization.  If you requested <code>ncpus=1</code> and <code>chunks=64</code> without <code>place=scatter</code> on a system with <code>ncpus=64</code>, all your chunks would end up on one node.</li> <li>Your job script:  See Example Job Scripts for more information about how to build your job script.  For options that wont change, you do have the option of taking things off the command line and putting them in your job script.  For instance the above command line could be simplified to <code>qsub -l select=&lt;#&gt; &lt;your job script&gt;</code> if you added the following to the top (the PBS directives have to be before any executable line) of your job script:</li> </ul> <pre><code>#PBS -A &lt;project&gt;\n#PBS -k doe\n#PBS -l walltime=HH:MM:SS\n#PBS -l filesystems=fs1:fs2\n</code></pre> <p>Also note that if you want to run an executable directly rather than a script you use two dashes and the executable name in place of your script name like this: <code>-- /usr/bin/sleep 600</code></p>"},{"location":"running-jobs/job-and-queue-scheduling/#more-detail","title":"More detail:","text":"<p>The single biggest difference between Cobalt and PBS is the way you select resources when submitting a job.  In Cobalt, every system had its own Cobalt server and you just specified the number of nodes you wanted (-n).  With PBS, we are planning on running a single \"PBS Complex\" which means there will be a single PBS server for all systems in the ALCF and you need to specify enough constraints to get your job to run on the resources you want/need.  One advantage of this is that getting resources from two different systems or \"co-scheduling\" is trivially possible.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#resource-selection-and-job-placement","title":"Resource Selection and Job Placement","text":"<p>Section 2.57.2.6 RG-219 Requesting Resources and Placing jobs in the Reference Guide.</p> <p>Resources come in two flavors:</p> <ul> <li>Job Wide: Walltime is the most common example of a job wide resource.  You use the <code>-l</code> option to specify job wide resources, i.e. <code>-l walltime=06:00:00</code>.  All the resources in the job have the same walltime.</li> <li><code>-l &lt;resource name&gt;=&lt;value&gt;[,&lt;resource name&gt;=&lt;value&gt; ...]</code></li> <li>Chunks: (see the definition above) This is how you describe what your needs are to run your job.  You do this with the <code>-l select=</code> syntax.  In the ALCF, we do whole node scheduling and every node has a resource called <code>system</code> which is set to the system name it belongs to (Polaris, Aurora, etc).  This means you can typically get away with the very simple <code>-l select=128:system=foo</code> which will give you 128 complete nodes on system foo.</li> <li><code>-l select=[&lt;N&gt;:]&lt;chunk&gt;[+[&lt;N&gt;:]&lt;chunk&gt; ...]</code> where N specifies how many of that chunk and a chunk is of the form:</li> <li><code>&lt;resource name&gt;=&lt;value&gt;[:&lt;resource name&gt;=&lt;value&gt; ...]</code></li> <li>Here is a hypothetical example that would select resources with A100s and other resources with A40 GPUs.  PBS takes care of co-scheduling the nodes on the two systems for you transparently.  Note that in this case since we did not specify <code>system=</code> if there were multiple systems that could satisfy a chunk you wouldn't know ahead of time which system you would get.</li> </ul> <p><code>-l select=128:ncpus=64:ngpus=4:gputype=A100+32:ncpus=64:ngpus=2:gputype=A40</code></p> <p>You also have to tell PBS how you want the chunks distributed across the physical hardware.  You do that via the <code>-l place</code> option:</p> <ul> <li><code>-l place=[&lt;arrangement&gt;][: &lt;sharing&gt; ][: &lt;grouping&gt;]</code> where</li> <li>arrangement is one of <code>free | pack | scatter | vscatter</code><ul> <li>unless you have a specific reason to do otherwise, you probably want to set this to <code>scatter</code>, otherwise you may not get what you expect.  For instance on a host with ncpus=64, if you requested <code>-l select=8:ncpus=8</code> you could end up with all of our chunks on one node.</li> <li><code>free</code> means PBS can distribute them as it sees fit</li> <li><code>pack</code> means all chunks from one host.  Note that this is not the minimum number of hosts, it is one host.  If the chunks can't fit on one host, the qsub will fail.</li> <li><code>scatter</code> means take only one chunk from any given host.</li> <li><code>vscatter</code> means take only one chunk from any given vnode.  If a host has multiple vnodes, you could end up with more than one chunk on the host.</li> </ul> </li> <li>sharing is one of <code>excl | shared | exclhost</code> where<ul> <li>NOTE: Node configuration can override your requested sharing mode.  For instance, in most cases ALCF sets the nodes to <code>force_exclhost</code>, so normally you don't have to specify this.</li> <li><code>excl</code> means this job gets the entire vnode</li> <li><code>shared</code> means the vnode could be shared with another job from another user.</li> <li><code>exclhost</code> means this job gets the entire host, even if it has multiple vnodes.</li> </ul> </li> <li>group=<code>&lt;resource name&gt;</code><ul> <li>As an example, for machines that use a dragonfly network topology, we provide a PBS resource named <code>tier1</code> indicating which dragonfly group a node is in.  If you wanted to ensure that all the chunks came from a single dragonfly group, you could specify <code>place=group=tier1</code> as part of your qsub.  <code>tier0</code> is rack granularity, so <code>group=tier0</code> would ensure your nodes all came from one rack.  Note that if you requested more nodes than were available in a rack your job would never run and you would see something like <code>Not Running: Insufficient amount of resource: tier0</code>.</li> </ul> </li> </ul> <p>We have defined placement sets for the tier0 and tier1 resources.  As a result, if you don't specify a grouping PBS will preferentially group your nodes in a placement set, but it won't drain or delay your job start to do so.  For example, if you request 10 nodes and don't specify a grouping, if 10 nodes are available in the same rack, all your nodes will be in one rack.  If not, but there are 10 nodes in a single dragonfly group, all your nodes will be in one dragonfly group.  If you wish to specify a specific rack or dragonfly group, that is accomplished via the select syntax.  For instance, <code>qsub ... -l select=10:tier1=g0</code> would force your 10 nodes to be in dragonfly group 0.</p> <p>Here is a heavily commented sample PBS submission script that shows some more of the options, but remember that the PBS manuals referenced at the top of this page are the ultimate resource.</p> <pre><code>#!/bin/bash -l\n# UG Section 2.5, page UG-24 Job Submission Options\n# Add another # at the beginning of the line to comment out a line\n# NOTE: adding a switch to the command line will override values in this file.\n# These options are MANDATORY at ALCF; Your qsub will fail if you don't provide them.\n#PBS -A &lt;short project name&gt;\n#PBS -l walltime=HH:MM:SS\n#file systems used by the job\n#PBS -l filesystems=home:eagle\n# Highly recommended\n# The first 15 characters of the job name are displayed in the qstat output:\n#PBS -N &lt;name&gt;\n# If you need a queue other than the default, which is prod (uncomment to use)\n##PBS -q &lt;queue name&gt;\n# Controlling the output of your application\n# UG Sec 3.3 page UG-42 Managing Output and Error Files\n# By default, PBS spools your output on the compute node and then uses scp to move it the\n# destination directory after the job finishes.  Since we have globally mounted file systems\n# it is highly recommended that you use the -k option to write directly to the destination\n# the doe stands for direct, output, error\n#PBS -k doe\n#PBS -o &lt;path for stdout&gt;\n#PBS -e &lt;path for stderr&gt;\n# If you want to merge stdout and stderr, use the -j option\n# oe=merge stdout/stderr to stdout, eo=merge stderr/stdout to stderr, n=don't merge\n#PBS -j n\n# Controlling email notifications\n# UG Sec 2.5.1, page UG-25 Specifying Email Notification\n# When to send email b=job begin, e=job end, a=job abort, j=subjobs (job arrays), n=no mail\n#PBS -m be\n# Be default, mail goes to the submitter, use this option to add others (uncomment to use)\n#PBS -M &lt;email addresses&gt;\n# Setting job dependencies\n# UG Section 6.2, page UG-109 Using Job Dependencies\n# There are many options for how to set up dependencies;  afterok will give behavior similar\n# to Cobalt (uncomment to use)\n##PBS depend=afterok:&lt;jobid&gt;:&lt;jobid&gt;\n# Environment variables (uncomment to use)\n# UG Section 6.12, page UG-126 Using Environment Variables\n# RG Sect 2.57.7, page RG-233 Environment variables PBS puts in the job environment\n##PBS -v &lt;variable list&gt;\n## -v a=10, \"var2='A,B'\", c=20, HOME=/home/zzz\n##PBS -V exports all the environment variables in your environment to the compute node\n# The rest is an example of how an MPI job might be set up\necho Working directory is $PBS_O_WORKDIR\ncd $PBS_O_WORKDIR\necho Jobid: $PBS_JOBID\necho Running on host `hostname`\necho Running on nodes `cat $PBS_NODEFILE`\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=1           # Number of MPI ranks per node\nNDEPTH=1           # Number of hardware threads per rank, spacing between MPI ranks on a node\nNTHREADS=1         # Number of OMP threads per rank, given to OMP_NUM_THREADS\nNTOTRANKS=$(( NNODES * NRANKS ))\necho \"NUM_OF_NODES=${NNODES}  TOTAL_NUM_RANKS=${NTOTRANKS}  RANKS_PER_NODE=${NRANKS}  THREADS_PER_RANK=${NTHREADS}\"\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} -env OMP_NUM_THREADS=${NTHREADS} ./hello_mpi\n</code></pre>"},{"location":"running-jobs/job-and-queue-scheduling/#email-notifications","title":"Email Notifications","text":"<p>Users should add <code>-M &lt;email address&gt;</code> if they want notifications as a best practice.</p> <p>Note: For users with '@alcf.anl.gov' email addressed, PBS will send out an email once the job has ended by default. If you do not want to receive these notifications, you will need to add <code>#PBS -m n</code> to your script."},{"location":"running-jobs/job-and-queue-scheduling/#specifying-filesystems","title":"Specifying Filesystems","text":"<p>Note: The <code>filesystems</code> attribute is mandatory. If you do not specify a filesystem(s) you will receive the following error message upon submission:</p> <p><code>qsub: Resource: filesystems is required to be set.</code></p> <p>Valid filesystems are <code>home</code>, <code>eagle</code>, and <code>grand</code>.  For example, to request the home and eagle filesystems for your job you would add <code>-l filesystems=home:eagle</code> to your qsub command.</p> <p>If a job is submitted while a filesystem it requested is marked down, the job will be queued but will not run, with a message in the comment field of the job as to why it is not running. Run <code>qstat -f &lt;jobid&gt;</code> to see the comment field. For example, if the job requested for eagle and if Eagle is unavailable, the comment field will have <code>Can Never Run: Insufficient amount of server resource: eagle_fs (True != False)</code>).  Once the affected filesystem has been returned to normal operation, and the filesystem is marked as being available, the job will then be scheduled normally. The job cannot run until all filesystems requested by the job are available.</p> <p>If a job requesting a filesystem that is marked down is already in the queue, the job will be not run until all of its requested filesystems are available.</p> <p>An example of a job requesting filesystems:</p> <p><code>qsub -l select=10:ncpus=64,walltime=30:00,filesystems=grand:home -A ProjectX -q prod my_job.sh</code></p> <p>To update the filesystems list for your job, use <code>qalter</code>.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qsub-examples","title":"qsub examples","text":"<ul> <li><code>qsub -A my_allocation -l select=4:system=polaris -l filesystems=home:eagle -l walltime=30:00 -q debug-scaling -- a.out</code><ul> <li>run a.out on 4 chunks on polaris with a walltime of 30 minutes in debug-scaling queue; charge my_allocation;</li> <li>Since we allocate full nodes on Polaris, 4 chunks will be 4 nodes.  If we shared nodes, that would be 4 threads.</li> <li>use the -- (dash dash) syntax when directly running an executable.</li> </ul> </li> <li><code>qsub -A my_allocation -l place=scatter  -l filesystems=home:eagle -l select=32:ncpus=32 -q prod -l walltime=30:00 mpi_mm_64.sh</code><ul> <li>32 chunks on any system that meets the requirements. Each chunk must have 32 HW threads; <code>place=scatter</code> means use a different vnode for each chunk, even if you could fit more than one on a vnode. Use the queue named <code>prod</code>.</li> </ul> </li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qstat-query-the-status-of-jobsqueues","title":"<code>qstat</code>: Query the status of jobs/queues","text":"<p>Users Guide Sec. 10.2, page UG-175; Reference Guide Sec. 2.55, page RG-200</p>"},{"location":"running-jobs/job-and-queue-scheduling/#jobs","title":"Jobs","text":"<p>At it's most basic, you just type <code>qstat</code> and it will list all the jobs currently running, queued, or held on the system.  If you are interested in a specific job or jobs, you can provide a space separated list on the command line: <code>qstat job1 job2...</code>.</p> <pre><code>Job id            Name             User              Time Use S Queue\n----------------  ---------------- ----------------  -------- - -----\n349726.polaris-p* PDE2             user1                    0 Q prod\n336987.polaris-p* inf_clDB         user2                    0 H large\n353205.polaris-p* 3d-2.sub         user3             2044:14* R large\n</code></pre> <p>One of the annoying things about <code>qstat</code> is that the output fields are fixed with and it will truncate the output.  This is indicated by an asterisk as the last character.  You can add <code>-w</code> for wide.  It doesn't prevent truncation, but makes it less likely.  A useful variant is <code>qstat -was1</code>.  It shows the number of nodes, tasks, the requested walltime, and the comment, all on one line.  <code>qstat -wan</code> will give you the node list you ran on, just remember that can be long.  If you want an estimate of when the job will start, add the <code>-T</code> option.  Note that start time is not available for all jobs, just the next N jobs that are expected to run.  If you want to know everything there is to know about the job, add the <code>-f</code> flag.</p> <p><pre><code>                                                            Req'd  Req'd   Elap\nJob ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n353201.polaris* user1    large    3d-1.sub    34449  60 38*    --  24:00 R 08:25    Job run at Tue Nov 15 at 16:44 on (x3006c0s13b1n0:ngpus=4:ncpus=64)+(x...\n353289.polaris* user2    medium   run_mae_l*    --   32 20*    --  12:00 Q   --     Not Running: Job would conflict with reservation or top job\n353411.polaris* user3    large    1310W60       --   64  64    --  06:00 Q   --     Not Running: Not enough free nodes available\n336990.polaris* user4    large    inf_clDB      --  464 29*    --  01:00 H   --     Job held by user4 on Mon Oct  3 20:16:26 2022\n</code></pre> The <code>comment</code> field is your friend.  Wondering why your job isn't running?  Check the comment.  Wondering about the fate of a finished job? Add the <code>-x</code> option to see finished jobs (our history retention is currently set at two weeks) and check the comment. This cannot be stressed enough.  Often, when a user ticket comes in about PBS, we answer it by looking at the comment.</p> <p>If you are familiar with <code>jq</code> or some other command line JSON tool, the <code>-F JSON</code> option can be quite handy. <code>grep</code> is great, but when you grep the <code>-f</code> output for something, you probably want to know which node the found lines belong to.  With the JSON output that is trivial.</p> <pre><code>allcock@polaris-login-02:~/.ssh&gt;  qstat -fF JSON | jq '.Jobs | map_values(select(.job_state == \"R\") | {Job_Name, Account_Name, qtime, stime})'\n{\n  \"349710.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\": {\n    \"Job_Name\": \"P38\",\n    \"Account_Name\": \"CompBioAffin\",\n    \"qtime\": \"Fri Nov  4 11:04:12 2022\",\n    \"stime\": \"Fri Nov 11 07:52:12 2022\"\n  },\n  \"352220.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\": {\n    \"Job_Name\": \"mdsim_10000_run1.pbs\",\n    \"Account_Name\": \"RL-fold\",\n    \"qtime\": \"Thu Nov 10 22:41:55 2022\",\n    \"stime\": \"Fri Nov 11 09:00:12 2022\"\n  },\n</code></pre>"},{"location":"running-jobs/job-and-queue-scheduling/#queues","title":"Queues","text":"<p><code>qstat -Q</code> Will show you the names of all the queues and tell you their status.  If they are enabled (Ena column), you can queue jobs into them.  If they are started (Str column) then the scheduler will try and run jobs from it.  There is a <code>-f</code> (full) option but that is mostly for admins, though you can find the min and max node count <code>(resources_[min|max].nodect)</code> and min and max walltime <code>(resources_[min|max]walltime)</code> from the output.  Those values are also available in this documentation.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qalter-alter-a-queued-job","title":"<code>qalter</code>: Alter a queued job","text":"<p>Users Guide Sec. 9.2, page UG-168; Reference Guide Sec. 2.40, page RG-130</p> <p>Basically takes the same options as <code>qsub</code>;  Say you typoed and set the walltime to 300 minutes instead of 30 minutes.  You could fix it (if the job had not started running) by doing <code>qalter -l walltime=30:00 &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code>  The new value overwrites any previous value.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qdel-delete-a-queued-or-running-job","title":"<code>qdel</code>: Delete a queued or running job:","text":"<p>Users Guide Sec. 9.3, page UG-170; Reference Guide Sec. 2.41, page RG-143</p> <p><code>qdel &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></p>"},{"location":"running-jobs/job-and-queue-scheduling/#qmove-move-a-job-to-a-different-queue","title":"<code>qmove</code>: Move a job to a different queue","text":"<p>Users Guide Sec. 9.7, page UG-173; Reference Guide Sec. 2.46, page RG-175</p> <ul> <li><code>qmove &lt;new queue&gt; &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> <li>Only works before a job starts running</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qholdqrls-place-release-a-user-hold-on-a-job","title":"<code>qhold,qrls</code>: Place / release a user hold on a job","text":"<p>Reference Guide Sec 2.44, page RG-150 and Sec 2.50, page RG-183</p> <ul> <li><code>[qhold | qrls] &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qselect-query-jobids-for-use-in-commands","title":"<code>qselect</code>: Query jobids for use in commands","text":"<p>Users Guide Sec. 10.1, page UG-175; Reference Guide Sec. 2.52, page RG-189</p> <ul> <li><code>qdel `qselect -N test1`</code> will delete all the jobs that had the job name set to <code>test1</code>.</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qmsg-write-a-message-into-a-jobs-output-file","title":"<code>qmsg</code> Write a message into a jobs output file","text":"<p>Users Guide Sec. 9.4, page UG-171; Reference Guide Sec. 2.47, page RG-177</p> <ul> <li><code>qmsg -E -O \"This is the message\" &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> <li><code>-E</code> writes it to standard error, <code>-O</code> writes it to standard out</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qsig-send-a-signal-to-a-job","title":"<code>qsig</code> Send a signal to a job","text":"<p>Users Guide Sec. 9.5, page UG-172; Reference Guide Sec. 2.53, page RG-195</p> <ul> <li><code>qsig -s &lt;signal&gt; &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> <li>If you don't specify a signal, <code>SIGTERM</code> is sent.</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#pbsnodes-get-information-about-the-current-state-of-nodes","title":"<code>pbsnodes</code> Get information about the current state of nodes","text":"<p>Reference Guide Sec 2.7 page RG-36</p> <p>This is more for admins, but it can tell you what nodes are free (state), how many \"CPUs\" which is actually the number of threads (ncpus), how many GPUs (ngpus) which with some GPUs like NVIDIA A100s can change depending on the MIG mode, and if the node is shared or not (sharing).</p> <p><code>pbsnodes &lt;node name&gt;</code>: Everything there is to know about a node</p> <pre><code>&gt; pbsnodes x3002c0s7b1n0\nx3002c0s7b1n0\n     Mom = x3002c0s7b1n0.hsn.cm.polaris.alcf.anl.gov\n     Port = 15002\npbs_version = 2022.1.1.20220926110806\n     ntype = PBS\n     state = free\n     pcpus = 64\nresources_available.arch = linux\n     resources_available.demand = False\n     resources_available.gputype = A100\n     resources_available.host = x3002c0s7b1n0\n     resources_available.mem = 527672492kb\n     resources_available.ncpus = 64\nresources_available.ngpus = 4\nresources_available.system = polaris\n     resources_available.tier0 = x3002-g0\n     resources_available.tier1 = g0\n     resources_available.vnode = x3002c0s7b1n0\n     resources_assigned.accelerator_memory = 0kb\n     resources_assigned.hbmem = 0kb\n     resources_assigned.mem = 0kb\n     resources_assigned.naccelerators = 0\nresources_assigned.ncpus = 0\nresources_assigned.ngpus = 0\nresources_assigned.vmem = 0kb\n     resv_enable = True\n     sharing = force_exclhost\n     license = l\n     last_state_change_time = Tue Nov 15 19:26:39 2022\nlast_used_time = Tue Nov 15 19:26:39 2022\nserver_instance_id = polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov:15001\n```bash\n`pbsnodes -avSj`: A nice table to see what is free and in use\n\n```bash\n&gt; pbsnodes -avSj\n                                                        mem       ncpus   nmics   ngpus\nvnode           state           njobs   run   susp      f/t        f/t     f/t     f/t   jobs\n--------------- --------------- ------ ----- ------ ------------ ------- ------- ------- -------\nx3014c0s19b0n0  job-exclusive        1     1      0  503gb/503gb   63/64     0/0     4/4 353394\nx3014c0s19b1n0  resv-exclusive       0     0      0  503gb/503gb    0/64     0/0     4/4 --\nx3014c0s1b0n0   offline              0     0      0  503gb/503gb   64/64     0/0     4/4 --\n</code></pre> <p><code>pbsnodes -avSj | grep free | wc -l</code>: A quick way to see how many nodes are free</p> <pre><code>[20220217-21:09:30]&gt; pbsnodes -avSj | grep free | wc -l\n38\n</code></pre> <p><code>pbsnodes -avSj | grep free | awk '{print $1}'</code>: Lists the free nodes</p> <pre><code>[20220217-21:09:30]&gt; pbsnodes -avSj | grep free | awk '{print $1}'\nx3201c0s25b0n0\nx3209c0s13b0n0\nx3209c0s19b0n0\nx3209c0s1b1n0\n</code></pre> <p><code>pbsnodes -l</code>: (lowercase  l) see which nodes are down. The comment often indicates why it is down</p> <pre><code>[20220217-21:10:31]&gt; pbsnodes -l\nx3014c0s19b0n0       offline,resv-exclusive Xid 74 -- GPUs need reseat\nx3014c0s25b0n0       offline,resv-exclusive Checking on ConnectX-5 firmware\n</code></pre>"},{"location":"running-jobs/job-and-queue-scheduling/#job-priority","title":"Job Priority","text":"<p>In PBS it is not easy to see a priority order for which jobs will run next.  The best way is to use the <code>-T</code> option on qsub and look at the estimated start times.  ALCF runs a custom scheduler algorithm, but in general, the job priority in the queue is based on several criteria:</p> <ol> <li>positive balance of your project</li> <li>size (in nodes) of the job, larger jobs receive higher priority</li> <li>the type of project (e.g. INCITE, ALCC, or discretionary)</li> <li>job duration: shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible</li> </ol>"},{"location":"running-jobs/job-and-queue-scheduling/#troubleshooting-common-errors","title":"Troubleshooting / Common Errors","text":"<p>If you receive a <code>qsub: Job rejected by all possible destinations</code> error, then check your submission parameters. The issue is most likely that your walltime or node count do not fall within the ranges listed above for the production execution queues. Please see the table above for limits on production queue job sizes.</p> <p>NOTE: For batch submissions, if the parameters within your submission script do not meet the parameters of any of the above queues you might not receive the \"Job submission\" error on the command line at all. This can happen because your job is in waiting in a routing queue and has not yet reached the execution queues. In this case you will receive a jobid back and qsub will exit, however when the proposed job is routed, it will be rejected from the execution queues. In that case, the job will be deleted from the system and will not show up in the job history for that system. If you run a qstat on the jobid, it will return <code>qstat: Unknown Job Id &lt;jobid&gt;</code>.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#using-fakeroot-with-singularity","title":"Using Fakeroot with Singularity","text":"<p>The fakeroot feature (commonly referred as rootless mode) allows an unprivileged user to run a container as a \u201cfake root\u201d user by leveraging user namespace UID/GID mapping.  To request this feature be enabled on your job add the following to your <code>qsub</code> command line:</p> <p><code>-l singularity_fakeroot=true</code></p>"},{"location":"running-jobs/machine-reservations-polaris/","title":"Machine Reservations on Polaris","text":"<p>To get a reservation, you must first demonstrate a need to run outside of the normal queueing policies. Reservations are available only to projects with a positive allocation. Lead time for approval is 5 business days. If approved, scheduling is contingent on machine availability.</p> <p>Disclaimer: Approval for reservation requests are subject to their appropriateness and machine availability. Not all requests will be approved. It is particularly difficult to accommodate reservation requests during busy times of the year, e.g. Supercomputing, end of the ALCC and INCITE allocation cycles.</p> <p>To request a reservation, e-mail support@alcf.anl.gov with the requested information below.</p> <ol> <li>RESERVATION REQUEST FOR ALL SYSTEMS (including vis clusters) AT ALCF Machine name:</li> <li>Project for reservation:</li> <li> <p>ALCF account username(s) (NOT the user's legal name) for reservation:</p> <p>NOTE: We can only gate a reservation on an explicit list of users or a list of groups, we can\u2019t mix them both. So users must specify either a project/unixgroup name or a list of usernames, not both.</p> </li> <li> <p>Length of reservation:</p> </li> <li>Earliest date you could start:</li> <li>Deadline for the run(s),</li> <li>Details on the Run: Can it run anytime, day or night?</li> <li>Your local time zone (e.g., US/Central):</li> <li>Total number of jobs to be run:</li> <li>Total amount of data generated during reservation:</li> <li>For each job, indicate: Node count (Note: not processor count)<ol> <li>Run time whether this job depends on any other jobs to finish before it can start</li> <li>Briefly describe the goals for this run: (Example: We are doing a scaling run of code XXXX to determine YYYY)</li> <li>Please provide a detailed explanation of why this workload cannot be accomplished with the existing queues: (Requests omitting this response will be not be processed)</li> <li>After a reservation is granted, you will receive a reservation name by e-mail. Use the command <code>pbs_rstat</code> to verify the reservation attributes.</li> </ol> </li> </ol> <p>For example:</p> <pre><code>pbs_rstat\nResv ID      Queue     User     State               Start / Duration / End             \n---------------------------------------------------------------------------\nA123456.po   A123456   smith@   CO       Mon Aug 18 09:00 / 43200 / Tue Aug 19 11:00\n\nqsub -q A123456 walltime=60:00 -l select=1024:system=polaris -l filesystems=eagle myprog.exe\n</code></pre> <p>Once the reservation is set up, jobs can be submitted to the reservation queue prior to the reservation start time.</p> <p>For recurring reservations, the <code>reserve_start</code> and <code>reserve_end</code> are always the first instance.  <code>reserve_index</code> and <code>reserve_count</code> tell you where you are in the recurrence.</p> <p>For jobs using 33 percent or more of a system, place your job in the queue at least 12 hours prior to the start of the reservation or your reservation may be canceled. The machine will start to drain for your reservation, and it is important that your job is ready to run.</p> <p>You can also move jobs from the regular queue to the reservation queue at any time using the \u201cqmove\u201d command.  Keep in mind that a job won't start unless enough time is left in the reservation. </p> <p>NOTE: There is NOT a 10-minute pad at the end of the reservation.  When the reservation ends all jobs are terminated, deleted, and the reservation queue is deleted.  If a routing queue is used for the reservation, then jobs may be preserved, but any running job(s) are still terminated.</p> <p>If you have finished running your jobs before your reservation has ended, please reach out to the support team to have to release it for other users. At this time, there is no way for a user to release a reservation early.</p>"},{"location":"running-jobs/pbs-qsub-options-table/","title":"PBS Pro <code>qsub</code> Options","text":"<p>Version 1.2 2021-04-28 </p> <p><code>-l select</code> and similar options use a lower case \"L\", <code>-I</code> for interactive is an upper case \"I\"</p> Cobalt CLI PBS CLI PBS Directive Function and Page Reference <code>-A &lt;account_string&gt;</code> <code>-A &lt;account_string&gt;</code> <code>#PBS Account_Name=&lt;accounting string&gt;</code> \"Specifying Accounting String\u201d UG-29 <code>-n NODES</code><code>--nodecount NODES</code> <code>-l select=NODES:system=&lt;hostname&gt;</code> One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51 <code>-t</code> <code>--walltime</code> <code>-l walltime=H:MM:SS</code> One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51 <code>--attrs</code> <code>filesystems=&lt;resouce&gt;</code> <code>-l filesystems=&lt;resource&gt;</code> One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51 <code>-q</code> <code>-q &lt;destination&gt;</code> <code>#PBS -q &lt;queue name&gt;</code> <code>#PBS -q @&lt;server name&gt;</code> <code>#PBS -q &lt;queue name&gt;@&lt;server name&gt;</code> \"Specifying Server and/or Queue\u201d UG-29 <code>--env</code> <code>-v &lt;variable list&gt;</code> \"Exporting Specific Environment Variables\u201d UG-126 <code>--env</code> <code>-V</code> <code>#PBS -V</code> \"Exporting All Environment Variables\u201d UG-126 <code>--attrs</code> Done via custom resources and select statements \"Setting Job Attributes\u201d UG-16 <code>--dependencies=&lt;list&gt;</code> <code>-W depend=afterok:&lt;list&gt;</code> <code>#PBS depend=...</code> \"Using Job Dependencies\u201d UG-107 <code>-I</code><code>--interactive</code> <code>-I</code> Deprecated for use in a script \"Running Your Job Interactively\u201d UG-121 <code>--jobname</code> <code>-N &lt;name&gt;</code> <code>#PBS -N &lt;job name&gt;</code> <code>#PBS -WJob_Name=&lt;job name&gt;</code> \"Specifying Job Name\u201d UG-27 <code>-e</code><code>--error=</code> <code>-e &lt;path&gt;</code> <code>#PBS -e &lt;path&gt;</code><code>#PBS Error_Path=&lt;path&gt;</code> \"Paths for Output and Error Files\u201d UG-42 <code>-o</code>--output= <code>-o &lt;path&gt;</code> <code>#PBS -o &lt;path&gt;</code><code>#PBS Output_Path=&lt;path&gt;</code> \"Paths for Output and Error Files\u201d UG-42 <code>-M</code>--notify see note #1 <code>-M &lt;user list&gt;</code> <code>-m &lt;mail options&gt;</code> (<code>-m be</code> is suggested) <code>#PBS -M &lt;mail recipients&gt;</code> <code>#PBS -WMail_Users=&lt;mail recipients&gt;</code> <code>#PBS -m &lt;mail points&gt;</code> <code>#PBS -WMail_Points=&lt;mail points&gt;</code> \"Setting Email Recipient List\u201d UG-26 <code>-u</code><code>--umask</code> <code>-W umask=&lt;value&gt;</code> <code>#PBS umask=&lt;value&gt;</code> \"Changing Linux Job umask\u201d UG-45 <code>-h</code> <code>-h</code> <code>#PBS -h</code> \"Holding and Releasing Jobs\u201d UG-115 <code>--proccount</code> See Note #2 <code>-l mpiprocs</code>Not needed to get equivalent Cobalt functionality One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51"},{"location":"running-jobs/pbs-qsub-options-table/#pbs-options-that-provide-functionality-above-and-beyond-cobalt","title":"PBS options that provide functionality above and beyond Cobalt","text":"<p>Depending on policy decisions not all of these options may be available.</p> Cobalt CLI PBS CLI PBS Directive Function and Page Reference N/A <code>-a &lt;date_time&gt;</code> <code>#PBS -a</code> \"Deferring Execution\u201d UG-119 N/A <code>-C &lt;directive prefix&gt;</code> \"Changing the Directive Prefix\u201d UG-16 N/A <code>-c &lt;interval&gt;</code> <code>#PBS -c</code> \"Using Checkpointing\u201d UG-113 N/A <code>-G</code> \"Submitting Interactive GUI Jobs on Windows\u201d UG-125 N/A <code>-J X-Y[:Z]</code> <code>#PBS -J</code> \"Submitting a Job Array\u201d UG-150 N/A <code>-j &lt;join&gt;</code> <code>#PBS Join_Path=&lt;joining option&gt;</code> \"Merging Output and Error Files\u201d UG-43 N/A <code>-k &lt;keep&gt;</code> <code>#PBS Keep_Files=&lt;keep option&gt;</code> \"Keeping Output and Error Files on Execution Host\u201d UG-44 N/A <code>-p &lt;priority&gt;</code> <code>#PBS -p</code> \"Setting Priority for Your Job\u201d UG-120 N/A <code>-P &lt;project&gt;</code> <code>#PBS project=&lt;project name&gt;</code> \"Specifying a Project for a Job\u201d UG-27 N/A <code>-r &lt;value&gt;</code> <code>#PBS -r</code> \"Allowing Your Job to be Re-run\u201d UG-118 N/A <code>-R &lt;remove options&gt;</code> \"Avoiding Creation of stdout and/or stderr\u201d UG-43 N/A <code>-S &lt;path list&gt;</code> \"Specifying the Top Shell for Your Job\u201d UG-19 N/A See Note #3 <code>-u &lt;user list&gt;</code> <code>#PBS User_List=&lt;username list&gt;</code> \"Specifying Job Username\u201d UG-28 N/A <code>-W block=true</code> <code>#PBS block=true</code> \"Making qsub Wait Until Job Ends\u201d UG-120 N/A <code>-W group_list=&lt;list&gt;</code> <code>#PBS group_list=&lt;group list&gt;</code> \"Specifying Job Group ID\u201d UG-28 N/A <code>-W release_nodes_on_stageout=&lt;value&gt;</code> \"Releasing Unneeded Vnodes from Your Job\u201d UG-127 N/A <code>-W run_count=&lt;value&gt;</code> \"Controlling Number of Times Job is Re-run\u201d UG-119 N/A <code>-W sandbox=&lt;value&gt;</code> \"Staging and Execution Directory: User Home vs. Job-specific\u201d UG-31 N/A <code>-W stagein=&lt;list&gt;</code> <code>#PBS -W stagein=&lt;execution path&gt;@&lt;input file storage host&gt;:&lt;input file storage path&gt;[,...]</code> \"Input/Output File Staging\u201d UG-31 N/A <code>-W stageout=&lt;list&gt;</code> <code>#PBS -W stageout=&lt;execution path&gt;@&lt;output file storage host&gt;:&lt;output file storage path&gt;[,...]</code> \"Input/Output File Staging\u201d UG-31 N/A <code>-X</code> \"Receiving X Output from Interactive Linux Jobs\u201d UG-124 N/A <code>-z</code> <code>#PBS -z</code> \"Suppressing Printing Job Identifier to stdout\u201d UG-30"},{"location":"running-jobs/pbs-qsub-options-table/#notes","title":"Notes","text":"<ol> <li>To get the equivalent mail notifications from PBS it requires two parameters: the <code>-M</code> just like Cobalt, but also <code>-m be</code> (the <code>be</code> stands for \"beginning\" and \"end\") to specify when the mails should go out. This will give you the same behavior as Cobalt.</li> <li><code>--proccount</code>, while available, only changed behavior on the Blue Gene machines.  To get equivalent functionality just drop it from the CLI.  In PBS it does influence the <code>PBS_NODES</code> file.  See Section 5.1.3 in the PBS Users Guide page UG-78</li> <li>The following Cobalt options have no equivalent in PBS:<ul> <li><code>--cwd</code>: use a script and <code>cd</code> to the directory you want to run from.</li> <li><code>--user_list</code>: There is no way to do this.  We will work on adding this functionality.</li> <li><code>--debuglog</code>: Are we going to try and generate the equivalent of a <code>.cobalt</code> file?</li> </ul> </li> <li>The following Cobalt options were Blue Gene specific and no longer apply:<ul> <li><code>--kernel</code></li> <li><code>-K KERNELOPTIONS</code></li> <li><code>--ion_kernel</code></li> <li><code>--ion_kerneloption</code></li> <li><code>--mode</code>: see notes on running scripts, Python, and other executables</li> <li><code>--geometry</code></li> <li><code>--disable_preboot</code></li> </ul> </li> </ol>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/","title":"PBS Admin Quick Start Guide","text":"<p>The single most important thing I can tell you is where to get the PBS BigBook.  It is very good and a search will usually get you what you need if it isn't in here.</p> <ul> <li>PBS Admin Quick Start Guide</li> <li>Checking Server Status</li> <li>Checking / Setting Node Status</li> <li>Troubleshooting</li> <li>Starting, stopping, restarting, status of the daemons:</li> <li>Starting, stopping scheduling across the entire complex</li> <li>Starting, stopping queues:</li> <li>\"Boosting\" jobs (running them sooner)</li> <li>Reservations</li> <li>MIG Mode</li> <li>Rack and Dragonfly group mappings</li> <li>Restricting a Reservation to Vnodes With Specific Resources</li> <li>Removing Blocking Resources</li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#checking-server-status","title":"Checking Server Status","text":"<p>You can check overall server status and settings with: <code>qmgr -c \"list server\"</code> or <code>qstat -Bf</code> (add <code>-w</code> to <code>qstat</code> if you want to remove wrapping) This will show current server parameters.  If you have manager/operator permissions you will also see any hidden resources. You may also check parameters of the scheduler with <code>qmgr -c \"list sched\"</code>, and by checking <code>$PBS_HOME/sched_priv/sched_config</code>. Hook information can be checked with <code>qmgr -c \"list hook\"</code> and <code>qmgr -c \"list pbshook\"</code>.  Due to permissions all hook operations require root.</p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#checking-setting-node-status","title":"Checking / Setting Node Status","text":"<p>The <code>pbsnodes</code> command is your friend.</p> <ul> <li>check status</li> <li><code>pbsnodes -av</code> gives you everything; grep will be useful here</li> <li><code>pbsnodes -v &lt;node&gt; &lt;node&gt; ...</code> will give you all information on the listed nodes</li> <li><code>pbsnodes -avSj</code> gives you a nice table summary</li> <li><code>pbsnodes -l</code> lists the nodes that are offline</li> <li>Taking nodes on and offline</li> <li><code>pbsnodes -C &lt;comment&gt; -o &lt;nodelist&gt;</code> will mark a node offline in PBS (unschedulable)<ul> <li>Adding the time and date and why you took it offline in the comment is helpful </li> <li><code>&lt;nodelist&gt;</code> is space separated </li> </ul> </li> <li><code>pbsnodes -r &lt;node list&gt;</code> will attempt to bring a node back online     This will only remove the \"offline\" state from a node, if the node would be down for other reasons, that will not change.         * Use -C \"\" to remove any comment that was set when the node was originally marked offline.  </li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>PBS_EXEC (where all the executables are): <code>/opt/pbs/[bin|sbin]</code></li> <li>PBS_HOME (where all the data is): <code>/var/spool/pbs</code></li> <li>logs: <code>/var/spool/pbs/[server|mom|sched|comm]_logs</code></li> <li>config: <code>/var/spool/pbs/[server|mom|sched]_priv/</code></li> <li><code>/etc/pbs.conf</code> - Reference Guide Section 9.1, page RG-371 </li> <li><code>qstat -[x]f [jobid]</code></li> <li>the -x shows jobs that have already completed.  We are currently holding two weeks history.</li> <li>the <code>comment</code> field is particularly useful.  It will tell you why it failed, got held, couldn't run, etc..</li> <li>The jobid is optional.  Without it you get all jobs.</li> <li><code>tracejob &lt;jobid&gt;</code> </li> <li>This will pull all of the logs related to the jobid on that node.  Run on the pbs.server host to get most of the job information</li> <li>If this is run on a compute node involved in jobid then it will aggregate all logs from the mom on that job from that node.</li> <li>You may pass it the <code>-n #</code> option where # is number of days to look back to tell the command to search more days back in the logs.  This defaults to 1 day.</li> <li>This does a rudimentary aggregation and filter of the logs for you.</li> <li><code>qselect</code> -  Reference Guide Section 2.54 page RG-187.</li> <li>allows you to query and return jobids that meet criteria for instance the command below would delete all the jobs from Yankee Doodle Dandy, username yddandy:</li> <li><code>qdel `qselect -u yddandy`</code></li> <li>Error Code Table (Reference Guide Chapter 14, RG-391)</li> <li>If a CLI command (qmgr, qsub, whatever) spits out an error code at you, go look it up in the table, you may well save yourself a good bit of time.</li> <li>We are going to try and either get the error text to come with the code or write a utility to look it up and have that on all the systems.</li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#starting-stopping-restarting-status-of-the-daemons","title":"Starting, stopping, restarting, status of the daemons:","text":"<ul> <li>Server: on pbs0 run <code>systemctl [start | stop |restart | status] pbs</code></li> <li>MoM:</li> <li>If you only want to restart a single MoM, ssh to the host and issue the same commands as above for ther server.</li> <li>If you want to restart the MoM on every compute node, <code>ssh admin.polaris</code> then do: <code>pdsh -g custom-compute \"systemctl [start | stop |restart | status] pbs\"</code> </li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#starting-stopping-scheduling-across-the-entire-complex","title":"Starting, stopping scheduling across the entire complex","text":"<p><code>qmgr -c \"set server scheduling = [True | False]\"</code></p> <p>IMPORTANT NOTE: If we are running a single PBS complex for all our systems (same server is handling Polaris, Aurora, Cooley2, etc) this will stop scheduling on everything.</p> <p>To check the current status you may do: <code>qmgr -c \"list server scheduling\"</code></p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#starting-stopping-queues","title":"Starting, stopping queues:","text":"<ul> <li>started: Can you queue a job or not</li> <li>enabled: Will the scheduler run jobs that are in the queue</li> </ul> <p>So if a queue is started, but not enabled, users can issue qsubs and the job will get queued, but nothing will run until we renable the queue.  Running jobs are unaffected.</p> <p><code>qmgr -c \"set queue &lt;queue name&gt; started = [True | False]\"</code> <code>qmgr -c \"set queue &lt;queue name&gt; enabled = [True | False]\"</code></p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#boosting-jobs-running-them-sooner","title":"\"Boosting\" jobs (running them sooner)","text":"<p>There are two ways you can run a job sooner:</p> <ol> <li><code>qmove run_next &lt;jobid&gt;</code> <ol> <li>Because of the way policy is set for the acceptance testing period, any job in the <code>run_next</code> queue will run before jobs in the default <code>workq</code> with the exception of jobs that are backfilled.  So by moving the job into the <code>run_next</code> queue, you moved it to the front of the line.  There are no restrictions on this, so please do not abuse it.</li> </ol> </li> <li><code>qorder &lt;jobid&gt; &lt;jobid&gt;</code></li> <li>If you don't necessarily need it to run next, but just want to rearrange the order a bit, you can use <code>qorder</code> which swaps the positions of the specified jobids.  So, if one of them was 10th in line and one was 20th, they would switch positions. </li> <li><code>qalter -l score_boost=NNNNN &lt;jobid&gt; &lt;jobid&gt;</code>     If the <code>job_sort_function</code> is enabled and shows up when querying the server, you can add a numeric boost to the score of a job to push it further ahead in the queue. You have to be a manager or operator to alter this value.</li> </ol>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#reservations","title":"Reservations","text":"<p>Most of the reservation commands are similar to the job commands, but prefixed with <code>pbs_r</code> instead of <code>q</code>: <code>pbs_rsub, pbs_rstat, pbs_ralter, pbs_rdel</code>.  You get the picture.  In general, their behavior is reasonably similar to the equivalent jobs commands.  Note that by default, users can set their own reservations.  We have to use a hook, no_user_rsub, to prevent that.  The hook does allow anyone with manager or operator permissions to set reservations.</p> <ul> <li>There are three types of reservations:</li> <li>Advance and standing reservations - reservations for users;  Note that you typically don't specify the nodes.  You do a resource request like with qsub and PBS will find the nodes for you.</li> <li>job-specific now reservations - we have not used these.  Where they could come in handy is for debugging.  A user gets a job through, we convert it to a job-specific reservation, then if their job dies, they don't have to wait through the queue again, they can keep iterating until the wall time runs out.</li> <li>maintenance reservations. - You can explicitly set which hosts to include in the reservation.</li> <li>Also note that reservations occur in two steps.  The <code>pbs_rsub</code> will return with an ID but will say <code>unconfirmed</code>.  That means it was syntactically correct, but PBS hasn't figured out if the resources are available yet.  Once it has the resources, it will switch to confirmed.  This normally is done as fast as you can run <code>pbs_rstat</code>.  A reservation can only be confirmed if scheduling is enabled on the server.</li> <li>-R (start) -E (end) are in \"datetime\" format: [[[[CC]YY]MM]DD]hhmm[.SS] </li> <li>1315, 171315, 12171315, 2112171315 and 202112171315 would all be Dec 17th, 2021 @ 13:15<ul> <li>If that is in the future they are all equivalent and valid</li> <li>If it were Dec 17th, 2021 @ 1400, then 1315 would default to the next day @ 14:00, the rest would be errors because they are in the past. </li> <li>Be careful or this will bite you.  It will confirm the reservation and you will expect it to start in a few minutes, but it is actually for tomorrow.</li> </ul> </li> <li><code>pbs_rsub -N rsub_test -R 2023 -D 05:00 -l select=4</code></li> <li>probably not what you think: <code>resv_nodes = (edtb-03[0]:ncpus=1)+(edtb-03[0]:ncpus=1)+(edtb-03[0]:ncpus=1)+(edtb-03[0]:ncpus=1)</code>  It gave me 4 cores on the same node.</li> <li><code>pbs_rsub -N rsub_test -R 2023 -D 05:00 -l select=2 -l place=scatter</code></li> <li>Getting closer: <code>resv_nodes = (edtb-01[0]:ncpus=1)+(edtb-02[0]:ncpus=1)</code></li> <li>The <code>-l place=scatter</code> got me two different nodes, but edtb allows sharing, so I got one thread on each node, but there were actually jobs running on those nodes at the time. On Polaris, since the nodes are <code>force_exclhost</code> that wouldn't have been an issue.</li> <li><code>pbs_rsub -N rsub_test -R 2217 -D 05:00 -l select=2:ncpus=64 -l place=scatter:excl</code> This gave me what I wanted:<ul> <li><code>resv_nodes = (edtb-03[0]:ncpus=64)+(edtb-04[0]:ncpus=64)</code></li> <li>Leaving it to default to <code>ncpus=1</code> should work, but asking for them all isn't a bad idea.</li> </ul> </li> <li><code>pbs_rsub -N rsub_test -R 1200 -D 05:00 --hosts x3004c0s1b0n0 x3003c0s25b0n0...</code></li> <li>If you use <code>--hosts</code> it makes it a maintenance reservation.  You can't / don't need to add <code>-l select</code> or <code>-l place</code> on a maintenance reservation.  PBS will set it for you and will make it the entire host and exclusive access.  Nodes don't have to be up.  If jobs are running they will continue to run.  This will override any other reservation.</li> <li><code>pbs_ralter</code> You can use this to change attributes of the reservation (start time, end time, how many nodes, which users can access it, etc).  Works just like <code>qalter</code> for jobs. </li> <li><code>pbs_rdel &lt;reservation id&gt;</code>  This will kill all running jobs, delete the queue, meaning you lose any jobs that were in the queue, and release all the resources.</li> <li>NOTE: once the reservation queue is in place, you use all the normal jobs commands (qsub, qalter, qdel, etc.) to manipulate the jobs in the queue.  On the qsub you have to add <code>-q &lt;reservation queue name&gt;</code></li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#giving-users-access-to-the-reservation","title":"Giving users access to the reservation","text":"<p>By default, only the person submitting the reservation will be able to submit jobs to the reservation queue.  You change this with the <code>-U +username@*,+username@*,...</code>.  You can add this to the initial <code>pbs_rsub</code> or use <code>pbs_ralter</code> after the fact.  The plus is basically ALLOW. We haven't tested it, but you can also theoretically use a minus for DENY.  You may also gate on group membership by setting <code>qmgr -c \"set queue &lt;reservation queue name&gt; acl_group_enable=True\"</code> and then adding groups to <code>acl_groups</code> on the reservation queue, using the same sort of syntax as you use for acl_users.  This is a bit of a hack, but if you want anyone to be able to run you can do <code>qmgr -c \"set queue &lt;reservation queue name&gt; acl_user_enable=False\"</code> </p> <p>WARNING: if you have both acl_users and acl_groups enabled, then the submitting user must be in the group and the user ACL list otherwise the job will be rejected! It is recommended that only one or the other be used on a queue.</p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#mig-mode","title":"MIG mode","text":"<ul> <li>See the Nvidia Multi-Instance GPU User Guide for more details.</li> <li><code>sudo nvidia-smi mig -lgip</code> List GPU Instance Profiles;  This is how you find the magic numbers used to configure it below.</li> <li><code>sudo nvidia-smi mig -lgipp</code> list all the possible placements;  The syntax of the placement is <code>{&lt;index&gt;}:&lt;GPU Slice Count&gt;</code> </li> <li><code>nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader</code> - check the status of all the GPUs on the node;  add <code>-i &lt;GPU number&gt;</code> to check a specific GPU</li> <li><code>systemctl stop nvidia-dcgm.service ; systemctl stop nvsm ; sleep 5 ; /usr/bin/nvidia-smi -mig 1</code> Put the node in MIG mode;  <code>-mig 0</code> will take it out of MIG mode.</li> <li><code>nvidia-smi mig -i 3 -cgi 19,19,19,19,19,19,19 -C</code> configure GPU #3 to have 7 instances.</li> <li><code>nvidia-smi mig --destroy-compute-instance; nvidia-smi mig --destroy-gpu-instance</code> Will free up the resources;  You have to do this before you can change the configuration.</li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#polaris-rack-and-dragonfly-group-mappings","title":"Polaris Rack and Dragonfly group mappings","text":"<ul> <li>Racks contain (7) 6U chassis; Each chassis has 2 nodes for 14 nodes per rack</li> <li>The hostnames are of the form xRRPPc0sUUb[0|10]n0 where:<ul> <li>RR is the row {30, 31, 32}</li> <li>PP is the position in the row {30 goes 01-16, 31 and 32 go 01-12}</li> <li>c is chassis and is always 0 (I wish they would have counted up chasses, oh well)</li> <li>s stands for slot, but in this case is the RU in the rack. Values are {1,7,13,19,25,31,37}</li> <li>b is BMC controller and is 0 or 1 (each node has its own BMC)</li> <li>n is node, but is always 0 since there is only one node per BMC </li> </ul> </li> <li>So, 16+12+12 = 40 racks * 14 nodes per rack = 560 nodes.</li> <li>Note that in production group 9 (the last 4 racks) will be the designated on-demand racks</li> <li>The management racks are x3000 and X3100 and are dragonfly group 10</li> <li>The TDS rack is x3200 and is dragonfly group 11 </li> </ul> Group 0 Group 1 Group 2 Group 3 Group 4 Group 5 Group 6 Group 7 Group 8 Group 9 x3001-g0 x3005-g1 x3009-g2 x3013-g3 x3101-g4 x3105-g5 x3109-g6 x3201-g7 x3205-g8 x3209-g9 x3002-g0 x3006-g1 x3010-g2 x3014-g3 x3102-g4 x3106-g5 x3110-g6 x3202-g7 x3206-g8 x3210-g9 x3003-g0 x3007-g1 x3011-g2 x3015-g3 x3103-g4 x3107-g5 x3111-g6 x3203-g7 x3207-g8 x3211-g9 x3004-g0 x3008-g1 x3012-g2 x3016-g3 x3104-g4 x3108-g5 x3112-g6 x3204-g7 x3208-g8 x3212-g9"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#restricting-a-reservation-to-vnodes-with-specific-resources","title":"Restricting a Reservation to Vnodes With Specific Resources","text":"<p>You can restrict a reservation to particular resources in the select statement just like you can with job placement.  For instance, to restrict replacement to nodes that are not in the on-demand queue you can use <code>-l select=256:demand=False</code> in your select statement for a regular or repeating reservation.</p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#removing-blocking-resources","title":"Removing Blocking Resources","text":"<p>There is a current behavior in PBS where reservations may inherit server defaults as restrictions and may not check other server values.  This may result in jobs running unexpectedly, or may cause a job to not be queued.  </p> <p>To fix jobs not being queued, some resources_max restrictions may have to be removed from the reservation queue, for example, you can clear filesystems and project_priority with the following: <code>gmgr -c \"unset queue &lt;reservation queue name&gt; resources_max.filesystems\"</code> <code>gmgr -c \"unset queue &lt;reservation queue name&gt; resources_max.project_priority\"</code></p> <p>If you need to add an additional restriction, you can likewise set a resource on the queue as a resources_max restrictions, for instance, to forbid eagle_fs from being used you can run: <code>qmgr -c \"set queue &lt;reservation queue name&gt; resources_max.eagle_fs=False\"</code> <code>qmgr -c \"set queue &lt;reservation queue name&gt; resources_mix.eagle_fs=False\"</code></p> <p>You can also set this as a part of the -l flag options at reservation creation.</p>"},{"location":"services/continuous-integration/","title":"Continuous Integration on Theta","text":""},{"location":"services/continuous-integration/#continuous-integration","title":"Continuous Integration","text":"<p>Continuous Integration (CI) in software development is the practice of committing code changes regularly to a version control system and having automated processes perform build, test, package, and deploy activities.</p> <p>The key concepts of CI include high frequency, repeatability, and automation in order to realize increased quality and ease of delivery. The main goal CI aims to achieve is the elimination of build and deployment issues, which in turn improves development cycles, provides a timely feedback loop with developers, and results in higher quality deliverables with reduced development time.</p> <p>CI usually describes the work that is done by a deployment or operations team to build and deploy code throughout an environment and make it available to the different interested teams involved in the SDLC. The steps that make up this process are referred to as a workflow or pipeline, which, when combined with automation, provides the mechanism for Continuous Integration.</p> <p>Today it is a common practice to use a CI tool for defining pipelines and executing the tasks required to take code from a source stored in a version control system to compiled and packaged artifacts executing in production. Two excellent examples of CI tools are Jenkins and GitLab.</p>"},{"location":"services/continuous-integration/#ci-tools-at-alcf","title":"CI Tools at ALCF","text":""},{"location":"services/continuous-integration/#jenkins","title":"Jenkins","text":"<p>Jenkins \"is a self-contained, open-source automation server which can be used to automate all sorts of tasks relating to building, testing, and delivering or deploying software.\"</p>"},{"location":"services/continuous-integration/#gitlab-ci","title":"Gitlab-CI","text":"<p>Gitlab  is an application that offers combined functionality as git repository, issue tracker, and CI/CD platform.  The ALCF implementation of the Gitlab-CI environment leverages upstream gitlab runners combined with the ECP's Jacamar custom executor. As CI/CD is built directly into Gitlab, it can allow for tighter devops processes. Gitlab-CI is meant to provide CI/CD services for projects using Gitlab-CI to store their git repositories. ALCF does not allow users to join their own private runners to our existing gitlab ci environment and provides runners on our supported systems.</p>"},{"location":"services/getting-started/","title":"ALCF Services","text":"<p>Below is a list of some of the services ALCF makes availble for use across our HPC clusters.</p> <ul> <li>JupyterHub: An interactive computing environment for different languages.</li> <li>Continuous Integration: An automated processes to help preform build, test, package, and deploy activities.</li> </ul>"},{"location":"services/gitlab-ci/","title":"Continuous Integration via Gitlab-CI","text":""},{"location":"services/gitlab-ci/#gitlab-ci","title":"Gitlab-CI","text":"<p>Gitlab is an application that offers combined functionality as git repository, issue tracker, and CI/CD platform.  The ALCF implementation of the Gitlab-CI environment leverages upstream gitlab runners combined with the ECP's Jacamar custom executor. As CI/CD is built directly into Gitlab, it can allow for tighter devops processes.</p> <p>Gitlab-CI is meant to provide CI/CD services for projects using Gitlab-CI to store your git repositories and executing code on our HPC clusters. ALCF does not allow users to join your own private runners to our existing Gitlab CI/CD environment and provides dedicated runners for our supported systems.</p> <p>Additional information, technical and user documentation, and community support can be found on the Gitlab's Runner website.</p> <p>ALCF's Gitlab-CI environment can be accessed by logging into the ALCF Gitlab-CI web portal using your ALCF credentials (ALCF username and cryptocard token password).</p>"},{"location":"services/gitlab-ci/#quickstart","title":"Quickstart","text":"<ul> <li>A user Emails ALCF Support requesting access for your ALCF Project for gitlab-ci.alcf.anl.gov .</li> <li>ALCF Support will add the ALCF Project to the appropriate system(s) via the Account and Project management system.</li> <li>ALCF will create a <code>Gitlab Group/SubGroup</code> for the ALCF Project and map it to the appropriate ldap group that maps to the ALCF Project</li> <li>ALCF Support will reply back to the user and inform them that the project is created.</li> <li>User(s) will need to login to gitlab-ci.alcf.anl.gov and configure your initial Gitlab profile.  Adding a SSH key so you can pull/push code to the gitlab server.</li> <li>User will then need to create a <code>Gitlab Project</code> in your assigned <code>Gitlab Group/SubGroup</code>.</li> <li>When ready to run CI/CD jobs, add a <code>.gitlab-ci.yml</code> file to your git repositories.</li> <li>You will need to set any ALCF specific variable(s).</li> </ul> <p>Example: A <code>.gitlab-ci.yml</code> file for a Theta project <pre><code>variables:\n  ANL_THETA_SCHEDULER_PARAMETERS: \"-A ProjectName -n 1  -t 10 -q ThetaQueueName --attrs filesystems=home\"\nstages:\n  - stage1\n  - stage2\n  - stage3\nshell_test1:\n  stage: stage1\n  tags:\n    - ecp-theta\n    - shell\n  script:\n    - echo \"Shell Job 1\"\nbatch_test:\n  stage: stage2\n  tags:\n    - ecp-theta\n    - batch\n  script:\n    - echo \"Job 2 start\"\n    - aprun -n 1 id\n    - aprun -n 1 hostname\n    - aprun -n 1 echo \"Running on theta with setuid batch runner\"\n    - echo \"Job end\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#glossary","title":"Glossary","text":"<ul> <li>Group - A collection of projects.  Certain settings can be applied at the <code>Group</code> level and apply down to all child <code>SubGroups</code> and/or <code>Projects</code>.  When a ALCF Project is allocated resources on the Gitlab-CI environment we will create a Gitlab <code>Group</code> that will map to your ALCF Project allocation.</li> <li>Jacamar-CI - A Custom Executor we use that runs jobs as a given user on the shell and is capable of submitting jobs to schedulers like Cobalt and PBS.</li> <li>Job - An individual set of commands that are ran.  This is the lowest unit of Gitlab-CI abstraction.</li> <li>Pipeline - Gitlab organizes your jobs for each run into a <code>pipeline</code>.</li> <li>Project - Gitlab Projects can be thought of as an individual  git repository plus all services and features Gitlab layers on top.  This term is unrelated to the ALCF Project concept.  That often maps to ldap groups and/or quotas and allocations.</li> <li>Stage - A collection of jobs in a pipeline.  Jobs in the next stage will not start till the jobs in the current stage complete.  If a job fails, the pipeline will not run the following stages by default.</li> <li>Triggering User - The user whose actions causes a CI/CD job to run and who the Jacamar-CI executor will run the jobs as. Examples include pushing commits up to the server, creating a merge request, and/or merging one branch into another branch.</li> </ul>"},{"location":"services/gitlab-ci/#projects-using-cicd","title":"Projects Using CI/CD","text":"<p>Any project with a git repository on the gitlab-ci environment has access to the CI/CD environment by default.  In order to launch a shell job on a system you must already have access to that system.</p>"},{"location":"services/gitlab-ci/#on-boarding-with-cicd","title":"On-Boarding with CI/CD","text":"<p>To gain access to the Gitlab-CI environment, send an email to support@alcf.anl.gov requesting access for your project(s). Include with the request :</p> <ul> <li>That you are requesting access to the Gitlab-CI environment at https://gitlab-ci.alcf.anl.gov</li> <li>The ALCF Project shortname</li> <li>The PI\u2019s name </li> </ul> <p>Gitlab-ci jobs run as the triggering user on relevant systems. The triggering user's home directory will be used by Jacamar-CI to copy the git repository and cache files into <code>~/.jacamar-ci</code>. This job will run out of their home directory and consume filesystem quota.  If you need more space you should try to reference files in any ALCF Project allocations you have on shared filesystems.  Unfortunately the initial git clone must run out of <code>~/.jacamar-ci</code> in your home directory.</p> <p>The triggering user is defined as the user account who caused the CI/CD pipeline to execute.  Via scheduling a re-occurring job, pushing commits up to the server, creating a merge request, and/or merging a branch.  When the CI/CD jobs run they will run as that user on the relevant systems. For a job to succeed the <code>triggering user</code> must have appropriate permissions and access to all relevant systems and files.</p>"},{"location":"services/gitlab-ci/#initial-login-and-profile-setup-of-gitlab-ci","title":"Initial Login and Profile setup of Gitlab-CI","text":"<ul> <li>Login to gitlab-ci.alcf.anl.gov using your username and Cryptocard token.</li> <li>Once logged in, add your public key you already have or created earlier so that it can be associated with your account.</li> <li>Click Profile icon on the upper right hand corner, then click \"Edit Profile\"      Gitlab Profile Dropdown screenshot </li> <li>Click \"SSH Keys\" on the left hand menu.      Gitlab Profile Add SSH Key screenshot </li> <li>Copy/Paste in your  SSH public key into the large text box under the word Key<ul> <li>On Linux, Unix, and OSX based systems using OpenSSH your SSH public key is commonly found at <code>~/.ssh/id_rsa.pub</code>. If using windows you will need to consult your applications documentation on the location of your public key.</li> <li>Give it a descriptive title such as the where the key resides, by default it will extract the name from the end of the public key if possible.</li> </ul> </li> <li>Click the <code>Add Key</code> button.  It will be greyed out until you paste a key</li> </ul>"},{"location":"services/gitlab-ci/#gitlab-projects-repositories","title":"Gitlab Projects (repositories)","text":"<p>Gitlab takes a git repository, layers additional functionality on top of them, and then calls it a <code>Gitlab Project</code>.  This is the most common level you will be interacting with Gitlab at.  Please do not confuse ALCF Projects with <code>Gitlab Projects</code> as they are two separate things.  ALCF Projects more closely map to the <code>Gitlab Group/SubGroup</code> concept; which we explain in the next section.   Once you are assigned access to a <code>Gitlab Group/SubGroup</code> you will be able to create arbitrary <code>Gitlab Projects</code> underneath.  Configuring CI/CD jobs for each independently.</p> <p>To create a new <code>Gitlab Project</code>:</p> <ul> <li>In the left pane, click \"Groups\", and then click \"Explore groups\" link on the right.</li> </ul> <p> </p> Gitlab Your Groups Page screenshot <ul> <li>From the list in the \"Explore groups\" page, click the group you were informed corresponds to your <code>ALCF Project</code></li> </ul> <p> </p> Gitlab Explore Groups Page screenshot <ul> <li>Click the <code>New project</code> button near the upper right.  If this is the first project you are creating you will have two large square buttons near the middle of the screen to create <code>Gitlab SubGroups</code> or <code>Gitlab Projects</code></li> </ul> <p> </p> Gitlab Empty Group Page screenshot <ul> <li>On the <code>Create new project</code> page, click <code>Create blank project</code></li> </ul> <p> </p> Gitlab Create New Project screenshot <ul> <li>Fill in the <code>Project Name</code> field. The <code>Project slug</code> field will auto populate based on the <code>Project Name</code>, do not change it.  If you are pushing an existing repository, you MUST uncheck the default <code>Initialize repository with a README</code> option. Failure to uncheck this option will result in a merge conflict that you will need to resolve manually between your existing \"local\" git repository and the one you just created on the server.</li> </ul> <p> </p> Gitlab Create New Project screenshot <ul> <li>Click <code>Create project</code> button near the bottom</li> </ul>"},{"location":"services/gitlab-ci/#gitlab-groupssubgroups-folders","title":"Gitlab Groups/SubGroups (Folders)","text":"<p>Gitlab organizes <code>Gitlab Projects</code> into \"folders\" called <code>Groups</code> or <code>SubGroups</code>.  When an ALCF Project is granted access to gitlab-ci a Gitlab <code>Group</code> will be created with access for all members of that ALCF Project.  Users will then be able to create arbitrary Gitlab <code>Projects</code>. </p> <p>Each ALCF Project will have a top-level <code>Group</code> or <code>SubGroup</code> created with the ALCF Project\u2019s name.  It is used for organization in the multi-project environment and is required for implementing the needed level of security. The <code>Group</code> folder is where all of the your <code>Gitlab Projects</code> are to be stored, you can additionally create new <code>SubGroups</code>, <code>Projects</code>, group variables, etc within your designated <code>Group</code>, <code>SubGroups</code>, and/or <code>Projects</code>.</p> <p>To create a new <code>Gitlab SubGroup</code>:</p> <ul> <li>In the left pane, click \"Groups\", and then click \"Explore groups\" link on the right.</li> </ul> <p> </p> Gitlab Your Groups Page screenshot <ul> <li>From the list in the \"Explore groups\" page, click the group you were informed corresponds to your <code>ALCF Project</code></li> </ul> <p> </p> Gitlab Explore Groups Page screenshot <ul> <li>Click the <code>New subgroup</code> button near the upper right.  If this is the first project you are creating you will have two large square buttons near the middle of the screen to create <code>Gitlab SubGroups</code> or <code>Gitlab Projects</code></li> </ul> <p> </p> Gitlab Empty Group Page screenshot <ul> <li>On the <code>Create subgroup</code> page, enter the <code>Subgroup name</code>.  <code>Subgroup slug</code> will auto populate, do not change it.</li> </ul> <p> </p> Gitlab Create New SubGroup screenshot <ul> <li>Click <code>Create subgroup</code> button near the bottom</li> </ul>"},{"location":"services/gitlab-ci/#gitlab-runner-nodes","title":"Gitlab Runner Nodes","text":"<p>Each system is assigned one or more Gitlab runner node(s) that are shared by all users in gitlab-ci.  Each runner is only capable of running one users pipeline at a time.  While multiple jobs in that pipeline may run in parallel.</p> <p>Each node will have two runners available, <code>shell</code> and <code>batch</code>.  <code>shell</code> will run shell jobs directly on the runner node as the user.  <code>batch</code> will submit the job to the HPC cluster's scheduler that is paired to that node.  You will need to select the appropriate runner in your <code>.gitlab-ci.yml</code> file for the job to be executed properly.  For more details on the <code>.gitlab-ci.yml</code> file, please see upstream docs.</p>"},{"location":"services/gitlab-ci/#gitlab-ciyml-configuration-sections","title":".gitlab-ci.yml Configuration Sections","text":"<p>Gitlab uses a per repository <code>.gitlab-ci.yml</code> file.  On any commit, merge request, or merge gitlab will attempt to trigger a CI/CD pipeline based on the contents of this file. Within the <code>.gitlab-ci.yml</code> file you can limit jobs to only run under certain conditions.  A common workflow is to have linting and validation to happen on every commit to a non-master/non-main branch.  And then preforming larger more complex tasks when that branch is merged back into master/main.  All jobs launched on a given event are organized into a <code>Pipeline</code> and you can watch the progress of your pipeline via the CI/CD pipeline page for your <code>Project</code>.</p> <p> </p> Gitlab Group and Projects screenshot <p> </p> Gitlab Group and Projects screenshot"},{"location":"services/gitlab-ci/#tags","title":"tags","text":"<p>Tags are used to select which runner a job will be sent to. Improper tags can prevent your job from running and result in a failed job.</p>"},{"location":"services/gitlab-ci/#alcf-specific-tags","title":"ALCF Specific tags","text":"<p>Currently we have two sets of tags necessary to run on our systems. One tag will select which cluster the jobs are sent to.  The other tag will determine if the job is ran locally on the gitlab runner, or submitted to a job scheduler on a HPC cluster.</p> <p>Cluster Tag(s)</p> Cluster tag Description Theta ecp-theta This tag will send jobs to the Theta HPC runners ThetaGPU thetagpu This tag will send jobs to the ThetaGPU HPC runners Polaris polaris This tag will send jobs to the Polaris HPC runners <p>Job Type Tag(s)</p> tag Description shell This tag will execute the job locally on the gitlab-runner node. batch This tag will submit the job to the HPC clusters job scheduler."},{"location":"services/gitlab-ci/#variables","title":"variables","text":"<p>Variables can be stored two ways, inline in the <code>.gitlab-ci.yml</code> file or as a setting in the gitlab <code>Group</code> or <code>Project</code> itself.  Variables are exported as environment variables by the gitlab-runner for each job and can be used inside the <code>.gitlab-ci.yml</code> file.</p> <p>To set a variable directly in the <code>.gitlab-ci.yml</code> file declare a <code>variables:</code> section with each <code>VariableName: \"VariableValue\"</code> being on its own line.  <code>variables:</code> can be declared globally or in individual jobs.</p> <p>Example: Declaring variables <pre><code>variables:\n  GlobalVariable1: \"Global Value 1\"\n  GlobalVariable2: \"Global Value 2\"\n\njob:\n  variables:\n    LocalVariable: 'This is a local variable'\n  script:\n    - 'echo $LocalVariable'\n</code></pre></p> <p>To store it in the <code>Group</code> or <code>Project</code> settings.  On the left side menu, click <code>Settings&gt;CI/CD</code>. Expand the Variables option on the right side frame.  You can add variables by clicking <code>Add variable</code>.</p> <p>For for more details please set please see upstream docs</p> <p> </p> Gitlab Group and Projects screenshot <p> </p> Gitlab Group and Projects screenshot"},{"location":"services/gitlab-ci/#alcf-specific-variables","title":"ALCF Specific Variables","text":"<p>If you are planning to submit jobs to a scheduler then you will need to specify a per system variable <code>ANL_${CLUSTER}_SCHEDULER_PARAMETERS</code>; where <code>${CLUSTER}</code> is the name of the cluster.  This variable will contain any command line flags you would need to submit jobs as if you were on the command line / scripting.  Please consult the below table for more info.</p> Cluster Scheduler Variable Name Support docs Theta Cobalt ANL_THETA_SCHEDULER_PARAMETERS Theta Job Queue and Scheduling ThetaGPU Cobalt ANL_THETAGPU_SCHEDULER_PARAMETERS ThetaGPU Job Queue and Scheduling Polaris PBS ANL_POLARIS_SCHEDULER_PARAMETERS Polaris Getting Started <p>Example: Running a batch job on Theta HPC <pre><code>variables:\n ANL_THETA_SCHEDULER_PARAMETERS: \"-A ProjectName -n 1  -t 10 -q myQueue --attrs filesystems=home\"\n\nbatch_test:\n  tags:\n    - ecp-theta\n    - batch\n  script:\n    - echo \"Job start\"\n    - aprun -n 1 id\n    - aprun -n 1 hostname\n    - aprun -n 1 echo \"Running on theta with setuid batch runner\"\n    - echo \"Job end\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#stages","title":"Stages","text":"<p>Jobs can be organized into <code>stages</code>.  Jobs in the next stage wont start til any dependencies in the previous stage have completed.  This is often used if you need to build and test your code before attempting to run or package it.  These stages are assembled in a directed graph called a <code>Pipeline</code>.  By default gitlab has the following default stages executed in order : <pre><code>.pre\nbuild\ntest\ndeploy\n.post\n</code></pre></p> <p>You can declare your own stages by declaring a <code>stages:</code>  array near the top of your <code>.gitlab-ci.yml</code> file.  Stages will be processed in order.</p> <p>Example: Declaring Stages <pre><code>stages:\n  - stage1\n  - stage2\n  - stage3\n</code></pre></p> <p>Example: Theta pipeline with custom stages <pre><code>variables:\n  ANL_THETA_PROJECT_SERVICE_USER: \"ecpcisvc\"\n  ANL_THETA_SCHEDULER_PARAMETERS: \"-A Operations -n 1  -t 10 -q build --attrs filesystems=home\"\n\nstages:\n  - stage1\n  - stage2\n\ntest1:\n  stage: stage1\n  tags:\n    - ecp-theta\n    - shell\n  script:\n    - export\n    - id\n    - hostname\n    - echo \"Running on theta with setuid shell runner\" \n    - echo test &gt; test.txt\ntest2:\n  stage: stage2\n  tags:\n    - ecp-theta\n    - batch\n  script:\n    - echo \"Job 2 start\"\n    - aprun -n 1 id\n    - aprun -n 1 hostname\n    - aprun -n 1 echo \"Running on theta with setuid batch runner\"\n    - echo \"Job 2 end\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#rules","title":"Rules","text":"<p>Gitlab allows CI/CD jobs to only be launched if certain conditions are met.  Gitlab sets a series of variables in addition to any the user explicitly sets when a job launches.  A job can check these variables and choose to run or not based on the results.  Often times this is used to ensure certain jobs only run on commits, merge requests, and/or merges.  By default if any rule matches it will run.  You can override this behavior with commands like <code>when: never</code> when a conditional matches.</p> <p>For more details please set please see upstream docs</p> <p>Rules can use the following conditional checks: <pre><code>if\nchanges\nexists\nallow_failure\nvariables\nwhen\n</code></pre></p> <p>Example: Gitlab job designed to only run on merge requests <pre><code>test1:\n  rules:\n    - if: $CI_COMMIT_TAG                    # Do not execute jobs for tag context\n      when: never\n    - if: $CI_COMMIT_BRANCH == \"master\"     # Do not run on master, since will run on the merge request just prior\n      when: never\n    - if: $CI_MERGE_REQUEST_IID             # CI_MERGE_REQUEST_IID exists, so run job\n  stage: stage1\n  tags:\n    - ecp-theta\n    - shell\n  script:\n    - echo \"Run test 1\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#template-jobs","title":"Template Jobs","text":"<p>Gitlab allows you to create <code>template jobs</code> which can then be applied to later jobs.  <code>Template jobs</code> won't themselves run.  Each <code>template job</code> name must begin with a period (.) and follow the same syntax as normal jobs.  To instantiate a job based on the <code>template job</code> use the keyword <code>extends</code>.  If your specific job declares a key/value already in the template, the specific job will overwrite it.</p> <p>Example: Uses a job template so two tests will only run on merge requests <pre><code>.MR_rules:\n  rules:\n    - if: $CI_COMMIT_TAG                    # Do not execute jobs for tag context\n      when: never\n    - if: $CI_COMMIT_BRANCH == \"master\"     # Do not run on master, otherwise runs everything from scratch on merge\n      when: never\n    - if: $CI_MERGE_REQUEST_IID\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'    \n\ntest1:\n  extends: .MR_rules\n  stage: stage1\n  tags:\n    - ecp-theta\n    - shell\n  script:\n    - echo \"Run test 1\"\ntest2:\n  extends: .MR_rules\n  stage: stage2\n  tags:\n    - ecp-theta\n    - shell\n  script:\n    - echo \"Run test 2\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#console-output","title":"Console Output","text":"<p>To see the output of a job click on it in the GUI and it will show the STDOUT and STDERR from the job run.  If the job can not launch successfully it will have error messages from the gitlab-runner and/or executor Jacamar-ci.   Please be aware of any sensitive data you do not want exported or saved to the output console, such as passwords.   Please do not output large amounts of data from your jobs to the stdout.  If your CI/CD job outputs lots of text to STDOUT or STDERR please consider redirecting it into a job log.</p> <p> </p> Gitlab Group Job Console"},{"location":"services/gitlab-ci/#storage-use-and-policy","title":"Storage Use and Policy","text":""},{"location":"services/gitlab-ci/#gitlab-project-quota","title":"Gitlab Project Quota","text":"<p>Each repository will be have a default quota of 1GB in Gitlab.  Quota increases may be requested by emailing Support.  This quota is separate from the storage quotas allocated to ALCF Projects and ALCF Users on the HPC clusters and shared filesystems.</p>"},{"location":"services/gitlab-ci/#cicd-filesystem-usage","title":"CI/CD Filesystem usage","text":"<p>CI/CD jobs will run out of your home directory by default.  Each job will begin by cloning the repository into a path under <code>~/.jacamar-ci</code> and will continue to write there unless you reference other destinations in your CI/CD job.  You will need to ensure that you have the minimum amount of space for this runner operation.  If you do not, the job will fail to run.  Each gitlab runner will create a new sub directory under <code>~/.jacamar-ci</code> for itself, however it will reuse space for subsequent pipelines launched for that project on that runner.</p> <p>It is recommended that if you need more space then your home directory can provide, that you leverage any ALCF Project space you may have been allocated on a shared filesystem.</p>"},{"location":"services/gitlab-ci/#gitlab-ci-access-termination-policy","title":"Gitlab-CI Access Termination Policy","text":"<p>Projects that have exhibited a period of inactivity for at least 6 months will have their access disabled and their repositories deleted.  Notification will be sent to the PI 30 days prior to the day of the action.  </p> <p>Inactivity is defined as, but not limited to:</p> <ul> <li>No new projects created</li> <li>No new commits to an existing project</li> <li>Prolonged period of continuously failing CI/CD jobs (In the case of re-occurring scheduled jobs)</li> </ul>"},{"location":"services/jenkins/","title":"Jenkins on Theta","text":""},{"location":"services/jenkins/#jenkins-to-be-decommissioned","title":"Jenkins to be decommissioned","text":"<p>New projects should request access to use our GitLab-CI-based service.  You can learn how to request access in our documentation found here. </p> <p>Existing projects can continue to use Jenkins.  We will notify projects when we have the date it will be retired.  Projects will have ample notice to migrate their work to our GitLab-CI service.</p>"},{"location":"services/jenkins/#jenkins-at-alcf","title":"Jenkins at ALCF","text":"<p>The ALCF provides a tool for implementing CI processes named Jenkins. Using the Jenkins tool, ALCF projects can make use of CI functionality. The Jenkins CI tool enables projects to auto-compile their custom software code, automate testing cycles, provide a feedback loop, and submit jobs to HPC resources. The custom pipelines needed for each project can be defined in Jenkins by project users, and execution can be controlled through triggers.</p> <p>Additional information, technical and user documentation, and community support can be found on the Jenkin's project website.</p>"},{"location":"services/jenkins/#projects-using-jenkins","title":"Projects Using Jenkins","text":"<p>Enabling a project to use Jenkins requires some additional steps and configuration to get started. Once enabled for a project, users can access the Jenkins CI environment and configure jobs or pipelines for building and testing their project code.</p>"},{"location":"services/jenkins/#on-boarding-with-jenkins","title":"On-Boarding with Jenkins","text":"<p>To enable Jenkins for your project, send an email to support@alcf.anl.gov requesting Jenkins access for your project and include the ALCF project shortname and the PI\u2019s name with the request.</p> <p>The project\u2019s PI will get an email with details and a new Jenkins account associated with the project. This is a service account that the Jenkins CI tool will use when executing tasks associated with your project. The CI account will be listed as a project member and added to the project\u2019s group for access controls.</p>"},{"location":"services/jenkins/#alcf-jenkins","title":"ALCF Jenkins","text":"<p>Log in to the ALCF Jenkins web portal using your ALCF credentials (ALCF username and cryptocard token password).</p>"},{"location":"services/jenkins/#folders","title":"Folders","text":"<p>Each Jenkins project will have a top-level \"folder\" created with the project\u2019s name. Please do not delete the project folder: it is used for organization in the multi-project environment and is required for implementing the needed level of security. The project folder is where all of the project objects are stored, you can additionally create any subfolders, jobs, pipelines, etc. within your project folder to meet your CI needs.</p> <p>In the example below, we have a project named \"TestFromJanet2\" with an associated folder.</p> <p> </p> CI folders screenshot"},{"location":"services/jenkins/#nodes","title":"Nodes","text":"<p>Each Jenkins project will have an assigned node for execution. Nodes execute jobs defined within a project, typically on the target system\u2019s login node. Currently there are Jenkins nodes configured for HPC systems Theta and Cooley, as well as non-HPC nodes with 32 cores (Intel Xeon Processor E5-2683 v4) and 128 GB RAM for generic x86 processing with access to the Mira shared filesystems.</p> <p>In the example below, the node for this project is named \u2018TestFromJanet2-Theta. Jobs and pipeline steps triggered from Jenkins will execute on the TestFromJanet2-Theta node which has been configured to use host: thetalogin1 and will use the project\u2019s Jenkin's user ID (provided during on-boarding) to execute scripts or code just as if the end user had logged into the thetalogin1 node and executed the same set of actions manually from the command line.</p> <p> </p> CI slaves screenshot"},{"location":"services/jenkins/#job-configuration","title":"Job Configuration","text":"<p>When configuring any new job within a project there are some guidelines to follow for setting permissions and nodes. Project data is kept secure by setting up permissions at the project level and node selection controls where the job will execute.</p> <p>When creating jobs, enable project-based security, set the inheritance strategy, and add your project\u2019s group name to the permission matrix table. The example below has enabled project-based security, set the inheritance strategy to Do not inherit permission grants from other ACLs, and added the project\u2019s group name \"TestFromJanet2\" to the permission matrix granting all rights to the group.</p> <p> </p> CI permissions screenshot <p>To assign the node that the project will use to execute jobs, select the option Restrict where this project can be run and enter the project\u2019s assigned node. The example below has assigned the jobs to node: TestFromJanet2-Theta so that any time the job is executed, it runs on host: thetalogin1.</p> <p> </p> Execute screenshot"},{"location":"services/jenkins/#common-jenkins-features","title":"Common Jenkins Features","text":""},{"location":"services/jenkins/#version-control-features","title":"Version Control Features","text":"<p>Jenkins can connect to most common version control systems (VCS), including git/svn. The ALCF Jenkins instance can connect with local VCS hosted at at ANL as well as with external VCS, such as that hosted at Github.</p> <p>On the job configuration page, look for the section Source Code Management (SCM). If it is there already, add it to the job. The required fields for SCM are Repository URL and Credentials. The example below shows a connection to the ALCF internal Gitlab VCS and uses previously setup credentials.</p> <p> </p> Repository access <p>To use the new connection to the Git repository interactively, configure the job to be parameterized and add a Git Parameter to the job. The example below shows the configuration to select a branch at build time.</p> <p> </p> Git parameter <p>On the build screen, select from the drop-down menu the branch to be referenced during this job execution. The example below shows the list of available branches from the configured repository. It is automatically populated during the Git connector configuration of the preceding steps. If a new branch is added to the Git repository, it will display in the populated list of available branches when the job runs in Jenkins.</p> <p> </p> Select branch"},{"location":"services/jenkins/#build-steps","title":"Build Steps","text":"<p>Build steps are where users define executable tasks and jobs do something interesting within an environment. A core component of Jenkins, build steps can take a few different forms and are most commonly configured to call remote scripts for code building and deployment. A build step can even contain the shell script contents to execute on the remote machine.</p> <p> </p> Add build step <p>The example below uses the Execute Shell build step type and codes the shell logic within the Jenkins portal.</p> <p> </p> Execute shell"},{"location":"services/jenkins/#pipelines","title":"Pipelines","text":"<p>Pipelines in Jenkins allow for more advanced execution logic and are written in Groovy. A pipeline can be added directly to your project as an object using the New Item link. More commonly, they are defined in a \"Jenkinsfile\" and stored in VCS along with the project code. The Jenkinsfile can be created and edited outside of the Jenkins system using any text editor.</p> <p>To add a pipeline manually, select Pipeline from the new New Item dialog box.</p> <p> </p> New Item dialog box <p>The pipeline can then be configured and edited from the project folder in the same way as jobs, as shown in the example below.</p> <p> </p> Pipeline configuration <p>To add a pipeline using a Jenkinsfile in SCM, add the pipeline object as shown below. On the pipeline configuration page, select Pipeline script from SCM and provide the SCM connection details along with the Script Path. The Script Path is the path-to and filename where the Jenkinsfile is located within the SCM repository. The example below uses a Jenkinsfile stored in the project source code from the ALCF Git repository, and the Jenkinsfile containing the Groovy code pipeline definition is located at scripts/Jenkinsfile from the repository root.</p> <p> </p> Pipeline script path"},{"location":"services/jenkins/#triggers","title":"Triggers","text":"<p>Triggers are events that initiate tasks in Jenkins. Triggers can be called a few different ways, including directly by a user via the Build Now action (a time-based trigger similar to a Cron system), or based on commits made to source control.</p> <p>The example below shows a time-based configuration to run the job on a regular schedule. Details on the scheduling syntax can be found by clicking the blue question mark to the right of the Schedule field.</p> <p> </p> Build triggers"},{"location":"services/jenkins/#console-output","title":"Console Output","text":"<p>Jenkins provides console output and saves this history for each job run. During job execution you can view the live output from the tasks in a display similar to what would be seen if the commands were run directly in an interactive console.</p> <p> </p> Console output"},{"location":"services/jenkins/#credentials","title":"Credentials","text":"<p>Credentials are stored in Jenkins and used when connecting to remote resources that require authentication in a non-interactive manner. Once defined, credentials can be used throughout the Jenkins system when configuring jobs, SCM connections, SSH connections, etc.</p> <p>To add a set of credentials, click on Credentials from the available options on the left-hand navigation menu. Then select System and click on the link for Global credentials.</p> <p> </p> Credentials <p>Click Add Credentials from the left-hand navigation menu and provide the required information. The example below configures a new credential set of type \"SSH Username with private key.\" Make sure Scope is set to \"Global.\" Provide the username, private key (copy and paste), and key passphrase, and then give a pertinent ID and detailed description to help identify and organize stored credentials in the system.</p> <p> </p> Add credentials"},{"location":"services/jenkins/#faqs","title":"FAQS","text":"<p>Why does my project's execution node say it is offline? Node services for executing project tasks are initiated when there is demand for the node. The process of starting the node services can take up to one minute; the status change is displayed in the Jenkins web portal. When there is no longer demand for the node, the services will stop again after one minute of idle time.</p> <p>Why is my shell environment different when executing tasks on a Jenkins node? Since Jenkins uses SSH with no tty, any shell scripts need to have this at the top so that login scripts are run against the session:</p> <p>#!/bin/bash -1</p>"},{"location":"services/jenkins/#glossary","title":"Glossary","text":"<p>Continuous Integration (CI) - The process of automating the build and testing of code every time developers commit changes to version control.</p> <p>Pipeline - A CI pipeline is a list of tasks or jobs that are defined and executed as a procedure within a project. Pipeline is analogous to workflow.</p> <p>Source Control Management (SCM) - A term used in Jenkins to describe objects related to version control.</p> <p>Version Control System (VCS) - Software that manages access, storage, and revision history for a code repository.</p>"},{"location":"services/jenkins/#appendix","title":"Appendix","text":""},{"location":"services/jenkins/#abbreviated-setup","title":"Abbreviated Setup","text":"<ul> <li>Request Jenkins access for your project by emailing the ALCF Service Desk.</li> <li>Add jobs and pipelines to the project folder space to handle code compiling and testing.</li> <li>Configure jobs with credentials, SCM integrations, and trigger components depending on the intended behavior for your project.</li> <li>Execute jobs and pipelines by invoking the configured triggers.</li> </ul>"},{"location":"services/jupyter-hub/","title":"Jupyter Hub","text":""},{"location":"services/jupyter-hub/#using-jupyter-hub","title":"Using Jupyter Hub","text":"<p>JupyterHub is an open-source application to allow multiple users to launch Jupyter Notebooks from a central location. At ALCF users can use the JupyterHub instances at https://jupyter.alcf.anl.gov to run notebooks on servers connect to the compute resource or on the compute resource itself.</p> <p>The JupyterHub instance assigned to Cooley start notebooks on a login node.  The JupyterHub instance assigned to theta run notebooks a server external to Theta.  This instance does not have access to HPE/Cray programing environment and tools located in <code>/opt/cray, opt/intel, etc.</code>.  These instances provide users access to their home (/home/$USER) and project folders on theta-lustre, grand and eagle file systems (/home/$USER, /project, /grand, /eagle) using symbolic links in the home directory.</p> <p>The JupyterHub instances assigned to ThetaGPU allows users to start notebooks on a compute node through the job schedule system (Cobalt).  The instance assigned to Polaris will start notebooks on compute nodes through the job scheduler (PBS Pro).  These instances follow the scheduling and accounting policies run the notebook against the project allocation.</p>"},{"location":"services/jupyter-hub/#customize-environment","title":"Customize Environment","text":"<p>ALCF provides a simple Python environment to start.  User can customize their environment to meet their needs by creating virtual python environment and defining new kernels.  Below is an example of setting up a simple environment <code>projectA</code> with module mpi from within a notebook</p> <p>For more information on how to manage conda environments, refer to this page.</p> <p>From a terminal: <pre><code># Load a conda module\nmodule load conda\nconda activate base\n\n# set shell proxy variables to access external URL\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=$http_proxy\n\n# create an environment name projectA\nconda create -y -n projectA\n\n# Activate conda environment\nconda activate projectA\n\n# Install required packages\nconda install -y jupyter nb_conda ipykernel mpi\n\n# Add environment to available kernel list\npython -m ipykernel install --user --name projectA\n\n# deactivate conda environment\nconda deactivate\n</code></pre></p> <p>Once the base environment is setup the user must add an <code>env</code> section to the <code>kernel.json</code> file, located in directory <code>${USER}/.local/share/jupyter/kernels/projecta</code>, defining the <code>CONDA_PREFIX</code> and <code>PATH</code> variables.  Currently, Polaris compute nodes access the internet through a proxy.  To configure the kernel to use the proxy add variables <code>http_proxy</code> and <code>https_proxy</code> to the <code>env</code> section.  This will allow users to install packages form within the notebook using <code>!conda</code> magic commands.  The following is a sample configuration:</p> <pre><code>{\n \"argv\": [\n  \"/home/&lt;user&gt;/.conda/envs/projectA/bin/python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"projectA\",\n \"language\": \"python\",\n \"env\": {\n    \"CONDA_PREFIX\":\"/home/&lt;user&gt;/.conda/envs/projecta\",\n    \"PATH\":\"/home/&lt;user&gt;/.conda/envs/projecta/bin:${PATH}\",\n    \"http_proxy\":\"http://proxy.alcf.anl.gov:3128\",\n    \"https_proxy\":\"http://proxy.alcf.anl.gov:3128\"\n },\n \"metadata\": {\n  \"debugger\": true\n }\n}\n</code></pre> <p>After completing these steps, you will see projectA kernel when you click new on the Jupyter Hub home page or when you use Kernel menu on a Jupyter notebook.</p>"},{"location":"services/jupyter-hub/#accessing-project-folders","title":"Accessing Project Folders:","text":"<p>Jupyterhub file browser limits the user view files and directories within their home directory. To access project directory located outside of the user home directory a symbolic link to the directory mist be created within the user home directory. An example of this is:</p> <p>If a user is connected to the Cooley instances and wants to access Theta project ABC, the user should execute the following commands to create the links:</p> <pre><code>From terminal:\nln -s /project/ABC ABC_project\nln -s /lus/theta-fs0/projects/EFG EFG_project\n\nFrom notebook:\n!ln -s /project/ABC ABC_project\n!ln -s /lus/theta-fs0/projects/EFG EFG_project\n</code></pre>"},{"location":"services/jupyter-hub/#running-notebook-on-a-compute-node","title":"Running Notebook on a compute node","text":"<p>The ThetaGPU and Polaris instance of JupyterHub allow users to start Jupyter Notebooks on compute nodes through the given job scheduler.  The job will be executed according to ALCF\u2019s  queue and scheduling policy (Note:  If the queued Job does not start within 2 minutes JupyterHub will timeout and the job will be removed from queue)</p>"},{"location":"services/jupyter-hub/#thetagpu","title":"ThetaGPU","text":"<p>After authenticating to the ThetaGPU  JupyterHub instances the user will be presented with a drop down selection for \"Select a job profile\", with the options  \u201cLocal Host Process\u201d and \u201cThetaGPU Compute Node\u201d.</p> <p> </p> Select a job profile <p>Local Host Process\u201d will start the Jupyter Notebook on the JupyterHub server (external to the compute resource).</p> <p>\"ThetaGPU Compute Node\" will allow a user to start a Jupyter Notebook instance on an available compute node by requesting a node via the job scheduler, Cobalt.  When a user selects this option additional options will appear and must be selected.</p> <ul> <li>ThetaGPU Queue (MinTime/MaxTime): This field provide a list of available queues on the system.  In most cases a user should use \u201csingle-gpu\u201d or \u201cfull-node\u201d depending on their job requirements.</li> <li>Project List:  This field displays the active projects associated with the user on the given system (ThetaGPU).</li> <li>Runtime (minutes):  This field allows the user to set the runtime of the job in minutes.  The user should refer to the ThetaGPU Queue (MinTime/MaxTime) for the minimum and maximum runtime allowed for a selected queue.</li> </ul> <p> </p> ThetaGPU Job options <p>Once the appropriate information is provided the user will click the \u201cStart\u201d button and wait for the job to spawn.  In cases where the job queue is long the interface will time out and the job will be removed from the queue.</p> <p> </p> Job queued"},{"location":"services/jupyter-hub/#polaris","title":"Polaris","text":"<p>The Polaris JupyterHub instance does not have a \u201cLocal Host Process\u201d option.  All jupyter notebooks are run on a compute node through the job scheduler.  When the user authenticates the user will be presented with a \u201cStart My Server\u201d button and after clicking the button the user will be presented the available job options needed to start the notebook.</p> <ul> <li>Select a job profile:  This field list the current available Profiles \u201cPolaris Compute Node\u201d</li> <li>Queue Name: This field provide a list of available queues on the system.</li> <li>Project List: This field displays the active projects associated with the user on the given system (ThetaGPU).</li> <li>Number Chunks: This field allows the user to select the number of compute nodes to allocated to the job starts.</li> <li>Runtime (minutes:seconds) : This field allows the user to set the runtime of the job in minutes and seconds. The user should refer to the Polaris queue scheduling policy for minimum and maximum runtime allowed for a selected queue.</li> <li>File Systems: This field allow the user to select which file systems are required.   By default al file systems are checked.</li> </ul> <p> </p> Polaris Job options <p>Once the appropriate information is provided the user will click the \u201cStart\u201d button and wait for the job to spawn.  In cases where the job queue is long the interface will time out and the job will be removed from the queue.</p>"},{"location":"services/jupyter-hub/#end-a-jupyter-notebook-running-on-a-compute-node","title":"End a Jupyter Notebook running on a compute node","text":"<p>Failing to correctly end a running Jupyter Notebook will continue to consume the selected Project's allocation on the resource in question. When a user has completed their task in Jupyter the user should stop the Jupyter instance running on the compute node before logging out.  To stop the Notebook, click the \u201cControl Panel\u201d button in the top right, then click \u201cStop My Server\u201d.</p> <p> </p> Stop panel <p> </p> Stop server"},{"location":"services/jupyter-hub/#references","title":"References","text":"<ul> <li>ThetaGPU Queue Policy</li> <li>Polaris Queue Policy</li> </ul>"},{"location":"theta/theta-decommissioning/","title":"Theta and Theta-fs0 Decommissioning","text":"<p>Theta and Theta-fs0 will be retired at the end of calendar year 2023.  ThetaGPU will go offline for a short time, but then will come back online as an independent system running the PBS scheduler to be in line with the current standard for the ALCF.  Here are additional details:</p> <ul> <li>Jobs on Theta and ThetaGPU will stop running at 23:59:59 on 12/31/2023.</li> <li>There is currently no ETA for the ThetaGPU return to service, but we will provide a timeline in Q4CY23.</li> <li>Information on how to get allocations on the new ThetaGPU will be provided in Q4CY23.</li> </ul>"},{"location":"theta/theta-decommissioning/#for-theta-fs0","title":"For Theta-fs0:","text":"<p>Step 1: Update your scripts/workflows to switch all uses of theta-fs0 to the Grand filesystem as soon as possible. Data allocations on Grand will be provided. </p> <p>NOTE: theta-fs0 will be mounted read-only on 10/30/2023 and will no longer be available starting 01/01/2024. Any jobs attempting to write to theta-fs0 on 10/30/2023 or later will fail.</p> <p>Step 2: Migrate existing data off theta-fs0 as needed:</p> <ul> <li>If there is data you do not need on theta-fs0, just leave it in place. No need to move it or delete it.</li> <li>To move your data off of theta-fs0 use Globus Online. Please see using Globus for instructions on how to do so. This will work inside ALCF as we have Globus endpoints on all the relevant file systems.  If you need to transfer data out of ALCF and there isn\u2019t a Globus endpoint there are directions on the page linked above about how to use Globus Connect Personal to install an endpoint for yourself.</li> </ul>"},{"location":"theta/applications-and-libraries/applications/elpa/","title":"ELPA","text":""},{"location":"theta/applications-and-libraries/applications/elpa/#what-is-elpa","title":"What is ELPA?","text":"<p>ELPA is a Fortran/C/MPI library to solve dense hermitian (real or complex) matrices. ELPA is conceived to compute the eigenvectors and eigenvalues of large matrices in petascale computers. ELPA uses BLACS framework and some SCALAPACK functions to distribute and solve the eigen problem. Computationally intensive kernels in ELPA are optimized using intrinsic code for SSE, AVX, AVX2, AVX512, and QPX architectures. This code is popular in electronic structure codes, but it can be used for machine learning and other approaches that require full or partial spectrum solution of a matrix eigen problem.  ELPA scales efficiently in Theta solving matrices of 1 million by 1 million in less than 3 hours in 3000 KNL nodes.</p>"},{"location":"theta/applications-and-libraries/applications/elpa/#using-elpa-at-alcf","title":"Using ELPA at ALCF","text":"<p>ALCF  provides assistance with compiling the library. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta/applications-and-libraries/applications/elpa/#how-to-obtain-the-code","title":"How to obtain the code","text":"<p>ELPA can be downloaded free of charge from https://elpa.mpcdf.mpg.de/software. </p>"},{"location":"theta/applications-and-libraries/applications/elpa/#building-elpa-on-theta","title":"Building ELPA on Theta","text":"<p>ELPA must be compiled with AVX512 support in Theta. ELPA has OpenMP support, but it has shown lower performance for large number of MPI ranks. Because the interface of ELPA subroutines may change among versions 2016, 2017 and 2018, we strongly suggest that users visit https://elpa.mpcdf.mpg.de/software for further information.</p> <p>This is an example for the compilation of ELPA in Theta.</p> <pre><code>&gt; cat build_elpa_theta.sh\ngit clone https://gitlab.mpcdf.mpg.de/elpa/elpa.git\ncd elpa\naclocal\nautoreconf\n./configure --prefix=/soft/applications/elpa/elpa2017 \\\n     --host=x86_64-suse-linux-gnu \\\n     --disable-shared --enable-avx512 \\\n     FC=ftn CC=cc \\\n     SCALAPACK_LDFLAGS=\"\" \\\n     SCALAPACK_FCFLAGS=\"\" \\\n     FCFLAGS=\u201d-I/opt/intel/compilers_and_libraries_2018.0.128/linux/mkl/include/intel64/lp64\u201d\nmake &amp;&amp; make install\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/gromacs/","title":"Gromacs on Theta","text":""},{"location":"theta/applications-and-libraries/applications/gromacs/#what-is-gromacs","title":"What is Gromacs?","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p>"},{"location":"theta/applications-and-libraries/applications/gromacs/#using-gromacs-at-alcf","title":"Using GROMACS at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for GROMACS. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta/applications-and-libraries/applications/gromacs/#building-gromacs","title":"Building Gromacs","text":"<ol> <li>Download latest source code: http://manual.gromacs.org/documentation/2022.1/download.html</li> <li>tar -xzf gromacs-2022.1.tar.gz</li> <li>cd gromacs-2022.1</li> <li>mkdir build</li> <li>module load cmake</li> <li>module swap PrgEnv-intel PrgEnv-gnu/6.0.10</li> <li><pre><code>cmake -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC \\\n      -DBUILD_SHARED_LIBS=OFF -DGMX_BUILD_OWN_FFTW=ON \\\n      -DCMAKE_INSTALL_PREFIX=/path-to/gromacs-2022.1/build \\\n      -DGMX_MPI=ON -DGMX_OPENMP=ON -DGMX_CYCLE_SUBCOUNTERS=ON -DGMX_GPU=OFF \\\n      -DGMX_BUILD_HELP=OFF -DGMX_HWLOC=OFF -DGMX_SIMD=AVX_512_KNL \\\n      -DGMX_OPENMP_MAX_THREADS=256\n</code></pre></li> <li>make \u2013j 16</li> <li>make install</li> <li>The installed binary is <code>build/bin/gmx_mpi</code>.</li> </ol>"},{"location":"theta/applications-and-libraries/applications/gromacs/#running-gromacs-on-theta","title":"Running Gromacs on Theta","text":"<p>Prebuilt Gromacs binaries can be found in the directory <code>/soft/applications/gromacs/gromacs_theta</code>.</p> <p>A sample qsub script follows.</p> <pre><code>#!/bin/bash\n#COBALT -n 1\n#COBALT -t 30 \n#COBALT -q debug-cache-quad \n#COBALT -project catalyst \n#COBALT --attrs mcdram=cache:numa=quad\n#COBALT --attrs filesystems=home,theta-fs0\n\nexport GMX_MAXBACKUP=-1 \n\naprun -n64 -N64 --env OMP_NUM_THREADS=2 --cc depth -d 2 -j 2 \\\n      /soft/applications/gromacs/gromacs_theta/gmx_mpi.2022.1 mdrun \\\n      -dlb yes -resethway -pin on -v deffnm step5_1 -g test.log\n</code></pre> <p>We strongly suggest that users try combinations of different numbers of nodes, MPI ranks per node, and OMP threads per rank to find the optimal throughput for their particular workload.</p> <p>The following is a representative benchmark for a system with 30,000 atoms generated on a single Theta node with above aprun command.</p> Core time(sec) Wall time(sec) (%) Time 9016.068 70.441 12799.4 ns/day hour/ns Performance 61.330 0.391"},{"location":"theta/applications-and-libraries/applications/lammps/","title":"LAMMPS on Theta","text":""},{"location":"theta/applications-and-libraries/applications/lammps/#overview","title":"Overview","text":"<p>LAMMPS is a general-purpose molecular dynamics software package for massively parallel computers. It is written in an exceptionally clean style that makes it one of the most popular codes for users to extend and it currently has dozens of user-developed extensions.</p> <p>For details about the code and its usage, see the LAMMPS home page. This page is dedicated to information pertaining to Theta/ThetaGPU at the ALCF.</p>"},{"location":"theta/applications-and-libraries/applications/lammps/#using-lammps-at-alcf","title":"Using LAMMPS at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta/applications-and-libraries/applications/lammps/#how-to-obtain-the-code","title":"How to Obtain the Code","text":"<p>LAMMPS is an open-source code, which can be downloaded at http://lammps.sandia.gov/download.html.</p>"},{"location":"theta/applications-and-libraries/applications/lammps/#building-on-theta","title":"Building on Theta","text":"<p>After LAMMPS has been downloaded and unpacked, you should see a directory whose name is of the form lammps-. One should see lammps-/src/MAKE/MACHINES/Makefile.theta in recent versions that can be used for compilation on Theta. The top portion of that Makefile is provided below with suggested compiler settings. For older versions of LAMMPS, you will need to take an existing Makefile (e.g. Makefile.mpi) for the specific version used and edit the top portion appropriately to create a Makefile.theta. <p><pre><code># theta = Flags for Knights Landing Xeon Phi Processor, Intel compiler, Cray MPI, MKL FFT\n# module unload libsci\n# make theta -j 8\n\nSHELL = /bin/sh\n\n# ---------------------------------------------------------------------\n# compiler/linker settings\n# specify flags and libraries needed for your compiler\n\nKOKKOS_DEVICES = OpenMP\nKOKKOS_ARCH = KNL\n\nCC       = CC -mkl\nOPTFLAGS = -xMIC-AVX512 -O3 -fp-model fast=2 -no-prec-div -qoverride-limits\nCCFLAGS  = -g -qopenmp -qno-offload -ansi-alias -restrict \nCCFLAGS += -DLMP_INTEL_USELRT -DLMP_USE_MKL_RNG $(OPTFLAGS)\nCCFLAGS += -std=c++11\nCCFLAGS += -DLAMMPS_MEMALIGN=64\nSHFLAGS  = -fPIC\nDEPFLAGS = -M\n\nLINK      = $(CC)\nLINKFLAGS = -g -qopenmp $(OPTFLAGS) -dynamic\n#LIB       = -ltbbmalloc\nLIB       = -L$(TBBROOT)/lib/intel64/gcc4.8 -ltbbmalloc -Wl,-rpath=$(TBBROOT)/lib/intel64/gcc4.8\nSIZE      = size\n\nARCHIVE    = ar\nARFLAGS    = -rc\nSHLIBFLAGS = -shared\n\n# ---------------------------------------------------------------------\n# LAMMPS-specific settings, all OPTIONAL\n# specify settings for LAMMPS features you will use\n# if you change any -D setting, do full re-compile after \"make clean\"\n\n# LAMMPS ifdef settings\n# see possible settings in Section 2.2 (step 4) of manual\n\nLMP_INC =\n\n# MPI library\n# see discussion in Section 2.2 (step 5) of manual\n# MPI wrapper compiler/linker can provide this info\n# can point to dummy MPI library in src/STUBS as in Makefile.serial\n# use -D MPICH and OMPI settings in INC to avoid C++ lib conflicts\n# INC = path for mpi.h, MPI compiler settings\n# PATH = path for MPI library\n# LIB = name of MPI library\n\nMPI_INC  = -DMPICH_SKIP_MPICXX -DOMPI_SKIP_MPICXX=1\nMPI_PATH =\nMPI_LIB  =\n\n# FFT library\n# see discussion in Section 2.2 (step 6) of manaul\n# can be left blank to use provided KISS FFT library\n# INC = -DFFT setting, e.g. -DFFT_FFTW, FFT compiler settings\n# PATH = path for FFT library\n# LIB = name of FFT library\n\nFFT_INC  = -DFFT_MKL -DFFT_SINGLE\nFFT_PATH =\nFFT_LIB  = -L$(MKLROOT)/lib/intel64 -Wl,--start-group -lmkl_intel_lp64 \\\n           -lmkl_core -lmkl_intel_thread -Wl,--end-group\n\n...\n</code></pre> As newer versions of LAMMPS are distributed and changes made to the Makefile, the example Makefile above can be used to generate an updated Makefile using one of the Intel examples packaged with LAMMPS. With the Makefile in place, LAMMPS can be compiled from the lammps-/src directory using the following command. <pre><code>cd lammps-&lt;version&gt;/src\nmake theta -j 8\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/lammps/#running-lammps-jobs-on-theta","title":"Running LAMMPS Jobs on Theta","text":"<p>Following is an example executable script \u201crun_lammps.csh\u201d to run LAMMPS on two nodes of Theta with 64 MPI ranks per node. The job can be submitted with command \u201cqsub run_lammps.csh\u201d, where  is replaced with an active project allocation. <pre><code>#!/bin/csh\n#COBALT -n 2 -t 10 -q debug-cache-quad -A &lt;project_name&gt; -O LAMMPS\n\naprun -n 128 -N 64 -d 1 --cc depth -e OMP_NUM_THREADS=1 -j 1 ./lmp_theta -in lmp.in\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/lammps/#performance-notes","title":"Performance Notes","text":"<p>When possible, users will want to build LAMMPS executables with the USER-OMP and USER-INTEL packages for best performance on Theta. Following is an example script \u201crun_lammps_intel.csh\u201d to run LAMMPS on two nodes of Theta with 64 MPI ranks per node and two OpenMP threads per rank with the USER-INTEL and USER-OMP packages. The job can be submitted with command \u201cqsub run_lammps_intel.csh.\u201d <pre><code>#!/bin/csh\n#COBALT -n 2 -t 10 -q debug-cache-quad -A &lt;project_name&gt; -O LAMMPS\n\naprun -n 128 -N 64 -d 2 --cc depth -e OMP_NUM_THREADS=2 -j 2 ./lmp_theta -in lmp.in -sf hybrid intel omp\n</code></pre> Not all available forcefields in LAMMPS are supported in one or both of these packages. For the latest information, please check LAMMPS website and documentation.</p>"},{"location":"theta/applications-and-libraries/applications/lammps/#building-on-thetagpu","title":"Building on ThetaGPU","text":"<p>There are two key packages available in LAMMPS for running on the GPUs available in ThetaGPU: GPU and KOKKOS. Example Makefiles based on recent version of LAMMPS are available for download from ALCF GitHub.</p> <p>LAMMPS can be built on the ThetaGPU compute nodes with the default software environment and support for the GPU package using the following commands once the Makefiles at the above link are placed appropriately based on instructions in the README. <pre><code>cp Makefile.gpu_thetagpu lammps-&lt;version&gt;/lib/gpu\ncp Makefile.thetagpu lammps-&lt;version&gt;/src/MAKE/MACHINES\n\ncd lammps-&lt;version&gt;/lib/gpu\nmake -f Makefile.gpu_thetagpu -j 8\ncd ../../src\nmake yes-GPU\nmake thetagpu -j 8\n</code></pre></p> <p>LAMMPS can be built with the default software environment and support for the KOKKOS package using the following commands and the Makefile at the above link.</p> <pre><code>cp Makefile.thetagpu_kokkos lammps-&lt;version&gt;/src/MAKE/MACHINES\n\ncd lammps-&lt;version&gt;/src\nmake yes-KOKKOS\nmake thetagpu_kokkos -j 8\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/lammps/#running-lammps-jobs-on-thetagpu","title":"Running LAMMPS jobs on ThetaGPU","text":"<p>Following is an example executable script submit_full-node.sh to run LAMMPS a ThetaGPU node using all GPUs for both the GPU and KOKKOS packages. This example is based on the Rhodoposin benchmark using lammps-/bench/in.rhodo. <p>After the appropriate command is uncommented, the job can be submitted with \u201cqsub ./submit_full-node.sh\u201d, where \"-A Catalyst\" in the script is replaced with an appropriate active project allocation.</p> <p>Note: The preceding 'qsub' command should be executed 1) on the ThetaGPU login nodes or 2) from the Theta login node after executing 'module load cobalt/cobalt-gpu'.</p> <p><pre><code>#!/bin/sh\n#COBALT -n 1 -t 15 -q full-node -A Catalyst\n\n# submit job to run on 8 GPUs w/ 8 MPI ranks per GPU and 2 OpenMP threads per rank\n#env OMP_NUM_THREADS=2 mpirun -np 64 ~/bin/lammps/lammps-git/src/lmp_thetagpu -in in.rhodo -pk gpu 8 -pk omp 0 -sf hybrid gpu omp\n\n# KOKKOS package: submit job to run on 1 GPU w/ 1 MPI ranks per GPU and 2 OpenMP threads per rank\n\n#!/bin/sh\n#COBALT -n 1 -t 15 -q full-node -A Catalyst\n\n# submit job to run on 8 GPUs w/ 8 MPI ranks per GPU and 2 OpenMP threads per rank\n#env OMP_NUM_THREADS=2 mpirun -np 64 ~/bin/lammps/lammps-git/src/lmp_thetagpu -in in.rhodo -pk gpu 8 -pk omp 0 -sf hybrid gpu omp\n\n# KOKKOS package: submit job to run on 1 GPU w/ 1 MPI ranks per GPU and 2 OpenMP threads per rank\nmpirun -np 8 ~/bin/lammps/lammps-git/src/lmp_thetagpu -in in.rhodo -k on g 8 -sf kk -pk kokkos neigh half\n</code></pre> Additional details on the specific usage of the GPU and KOKKOS packages and how best to use multiple GPUs per node is available in the LAMMPS documentation.</p>"},{"location":"theta/applications-and-libraries/applications/namd/","title":"NAMD","text":""},{"location":"theta/applications-and-libraries/applications/namd/#what-is-namd","title":"What Is NAMD?","text":"<p>NAMD, recipient of a 2002 Gordon Bell Award and a 2012 Sidney Fernbach Award, is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 1,000,000 cores for the largest simulations. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR. NAMD is distributed free of charge with source code. You can build NAMD yourself or download binaries for a wide variety of platforms. </p> <p>NAMD has been well optimized by Intel in collaboration with Beckman institute of UIUC. The nonbond kernel is carefully tuned, including minimized random memory access, loop unrolling, compiler directives and AOS vs SOA, to fully leverage Intel\u2019s vectorizing compiler. Generic version charm++ on Aries interconnect is developed to make best use of asynchronous communication feature of charm++ targeting the large number of small-size messages in NAMD instance.  </p>"},{"location":"theta/applications-and-libraries/applications/namd/#using-namd-at-alcf","title":"Using NAMD at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta/applications-and-libraries/applications/namd/#building-charm","title":"Building Charm++","text":"<ul> <li>git clone https://charm.cs.illinois.edu/</li> <li>cd charm</li> <li>module swap intel intel/16.0.3.210</li> <li>module load gcc/6.3.0</li> <li>module load cray-fftw</li> <li>module load rca</li> <li>module load craype-hugepages8M</li> <li>module unload darshan</li> <li>module list</li> <li>./build charm++ gni-crayxc-persistent-smp --no-build-shared --with-production -xMIC-AVX512 -j8 Recommended: a user can also build the charm++ tarball file within a NAMD source code downloaded via link http://www.ks.uiuc.edu/Development/Download/download.cgi?PackageName=NAMD</li> </ul>"},{"location":"theta/applications-and-libraries/applications/namd/#building-namd","title":"Building NAMD","text":"<p>wget http://www.ks.uiuc.edu/Research/namd/libraries/tcl8.5.9-crayxe-threaded.tar.gz</p> <pre><code>tar xzf tcl8.5.9-crayxe-threaded.tar.gz (be sure tcl8.5.9-crayxe-threaded is in $HOME)\nmodule load craype-hugepages8M\nmodule load fftw\ncd namd2\nln -s /path/to/charm \n./config CRAY-XC-KNL-intel --with-fftw3 --charm-arch gni-crayxc-persistent-smp\n(or just \"./config CRAY-XC-KNL-intel\" as fftw3 and gni-crayxc-persistent-smp are defaults)\n\nAdd --with-memopt to the config line to build memory-optimized binary (you can add .suffixto the directory name)\ncd CRAY-XC-KNL-intel\nmake -j8\n\nBe sure that $HOME/tcl8.5.9-crayxe-threaded Tcl library is included and linked!\n\n\"make -j8 release\" will give you a distribution directory and tar file\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/namd/#running-namd","title":"Running NAMD","text":"<p>A prebuilt NAMD binary can be found in directory /soft/applications/namd/</p> <p>The following should be included in the job script:</p> <p><pre><code>module load craype-hugepages8M\nexport HUGETLB_DEFAULT_PAGE_SIZE=8M\nexport HUGETLB_MORECORE=no\nexport ATP_ENABLED=1\n</code></pre> There does not seem to be a reason to use other than cache-quad mode. Since all NAMD data will likely fit in MCDRAM using flat-quad with numactl -m 1 is reasonable, but no performance advantage over cache-quad was observed.</p> <p>Best performance is generally with two hyperthreads per core, although at the scaling limit one hyperthread per core may be faster.</p> <p>Careful attention must be paid to core mapping. The rules are:</p> <ol> <li> <p>Reserve cpu 255 for the OS with -r 1 and then avoid also cpus 63, 127, and 191 that share the core.</p> </li> <li> <p>Give each communication thread (one per process) a full core to itself.</p> </li> <li> <p>Do not mix worker threads from different processes on the same core pair (that share L2 cache).</p> </li> </ol> <p>Given these rules the following options seem reasonable (invoked as, e.g., $APRUN7 /path/to/namd2 $CARGS72 myinput.namd):</p> <ul> <li>APRUN1=\"aprun -n $((1*$COBALT_JOBSIZE)) -N 1 -d 125 -j 2 -r 1\"</li> <li>CARGS11=\"+ppn 62 +commap 62 +pemap 0-61 --useCkLoop 6\"</li> <li>CARGS12=\"+ppn 124 +commap 62 +pemap 0-61+64 --useCkLoop 6\"</li> </ul> <p>or</p> <ul> <li>APRUN3=\"aprun -n $((3*$COBALT_JOBSIZE)) -N 3 -d 41 -j 2 -r 1\"</li> <li>CARGS31=\"+ppn 20 +commap 60-62 +pemap 0-59 --useCkLoop 6\"</li> <li>CARGS32=\"+ppn 40 +commap 60-62 +pemap 0-59+64 --useCkLoop 6\"</li> </ul> <p>or</p> <ul> <li>APRUN4=\"aprun -n $((4*$COBALT_JOBSIZE)) -N 4 -d 29 -j 2 -r 1\"</li> <li>CARGS41=\"+ppn 14 +commap 14-62:16 +pemap 0-63:16.14 --useCkLoop 6\"</li> <li>CARGS42=\"+ppn 28 +commap 14-62:16 +pemap 0-63:16.14+64 --useCkLoop 6\"</li> </ul> <p>or</p> <ul> <li>APRUN7=\"aprun -n $((7*$COBALT_JOBSIZE)) -N 7 -d 17 -j 2 -r 1\"</li> <li>CARGS71=\"+ppn 8 +commap 56-62 +pemap 0-55 --useCkLoop 6\"</li> <li>CARGS72=\"+ppn 16 +commap 56-62 +pemap 0-55+64 --useCkLoop 6\"</li> </ul> <p>The first case, one process per node, is untested and would only make sense for cases with very little communication, such as a multi-copy simulation with one copy per node.</p> <p>Three processes per node would also not scale well, and since four processes per node has the same number of worker threads per node as seven processes per node, the final case (APRUN7, CARGS72) generally performs best.</p> <p>The \"--useCkLoop 6\" flag is a NAMD option that enables optional PME shared-memory parallelization; it may not make sense for smaller simulations.</p> <p>Note: These settings are only for 64-core processors, especially the +64 in the pemap, which assumes hyperthreads on the same core are numbered with a stride of 64. </p>"},{"location":"theta/applications-and-libraries/applications/namd/#scaling-and-performance","title":"Scaling and Performance","text":"<p>Scaling tests on Theta were run up to 3072 nodes using benchmarks of 210M atoms STMV system (from Phillips et al., SC14).</p> <p>Compared per node, Theta is slower than GPU-accelerated Titan, faster than Edison, and a factor of ten faster than Mira. Edison scales better due to faster cores that reduce the impact of serial bottlenecks.</p> <p>Compared per core to other CPU-only machines, Theta is slightly faster than Blue Waters (treated as 32 cores).</p> <p> </p> Performance on various systems <p> </p> Performance on various systems"},{"location":"theta/applications-and-libraries/applications/nwchem/","title":"NWChem","text":"<p>NWChem is a parallel quantum chemistry mainly written in Fortran77 and that uses MPI and OpenMP for distributed and multicore computing. NWChem was designed to solve large-scale  electronic structure calculations with Hartree-Fock, Density Functional Theory, and other wavefunction correlated methods. See full features at http://www.nwchem-sw.org.</p>"},{"location":"theta/applications-and-libraries/applications/nwchem/#using-nwchem-at-alcf","title":"Using NWChem at ALCF","text":"<p>ALCF provides binaries and compiling instructions for NWChem. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta/applications-and-libraries/applications/nwchem/#how-to-obtain-the-code","title":"How to Obtain the Code","text":"<p>NWChem is an open-source code the official web site is http://www.nwchem-sw.org. Change log and release announcements could be found here http://www.nwchem-sw.org/index.php/Download.</p> <p>Official NWChem source could be downloaded from the project\u2019s github.com repository https://github.com/nwchemgit/nwchem/releases. For detailed compiling options, please visit the wiki page of NWchem, https://github.com/nwchemgit/nwchem/wiki/Compiling-NWChem. The follow instructions were tested in for the version 6.8.1.</p> <pre><code>cat nwchem_envs_681.sh\n\nexport NWCHEM_TOP=/projects/nwchem/nwchem-6.8.1\n\nexport NWCHEM_TARGET=LINUX64\n\nexport USE_MPI=y\n\nexport USE_CPPRESERVE=y\n\nexport NWCHEM_MODULES=\"all\"\n\nexport USE_MPI=y\n\nexport USE_MPIF=y\n\nexport USE_MPIF4=y\n\nexport USE_OPENMP=1\n\nexport USE_KNL=1\n\nexport USE_OPENMP=y\n\nexport  INTEL_64ALIGN=1\n\nexport USE_NOIO=1\n\nexport USE_GAGITHUB=1\n\nexport CRAYPE_LINK_TYPE=dynamic\n\nexport export ARMCI_NETWORK=MPI_TS\n\nexport BLAS_SIZE=4\n\nexport BLASOPT=\"  -lmkl_intel_lp64 -lmkl_core -lmkl_intel_thread -lpthread -lm -ldl\"\n\nexport SCALAPACK=\" -lmkl_scalapack_lp64 -lmkl_intel_lp64 -lmkl_core -lmkl_intel_thread -lmkl_blacs_intelmpi_lp64 -lpthread -liomp5  -ldl\"\n\nexport BLAS_LIB=$BLASOPT\n\nexport LAPACK_LIB=$BLASOPT\n\nexport SCALAPACK_SIZE=4\n\nexport USE_KNL=1\n\nexport USE_64TO32=1\n\ncd  $NWCHEM_TOP/src\n\nmake nwchem_config\n\nmake 64_to_32\n\nmake\n</code></pre> <p>Alternatively, binaries can be found in the folders under /soft/applications/nwchem</p>"},{"location":"theta/applications-and-libraries/applications/nwchem/#running-jobs-on-theta","title":"Running Jobs on Theta","text":"<p>The following script \u201csubmit.sh\u201d is an example to run Nwchem in Theta a job of 8 nodes with 64 MPI ranks per node. The job can be submitted with command \"qsub submit.sh\"</p> <pre><code>cat submit.sh\n\n#!/bin/bash\n\n#COBALT -n 8\n\n#COBALT -t 30\n\n#COBALT -A myproject\n\nmodule load atp\n\nbin=/soft/applications/nwchem/6.8/bin/nwchem\necho \"Running Cobalt Job $COBALT_JOBID.\"\n\nrpn=64\n\ndf -k\n\nexport MPICH_GNI_MAX_EAGER_MSG_SIZE=16384 \n\nexport MPICH_GNI_MAX_VSHORT_MSG_SIZE=10000 \n\nexport MPICH_GNI_MAX_EAGER_MSG_SIZE=131072 \n\nexport MPICH_GNI_NUM_BUFS=300 \n\nexport MPICH_GNI_NDREG_MAXSIZE=16777216 \n\nexport MPICH_GNI_MBOX_PLACEMENT=nic \n\nexport MPICH_GNI_LMT_PATH=disabled \n\nexport COMEX_MAX_NB_OUTSTANDING=6\n\nexport LD_LIBRARY_PATH=/opt/intel/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64_lin\n\naprun  -n $((COBALT_JOBSIZE*rpn)) -cc depth -d1 -j1  $bin test.nw\n\nqsub submit.sh\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/qbox/","title":"Qbox on Theta","text":""},{"location":"theta/applications-and-libraries/applications/qbox/#what-is-qbox","title":"What is Qbox?","text":"<p>Qbox is a C++/MPI scalable parallel implementation of first-principles molecular dynamics based on the plane-wave, pseudopotential formalism. As described on the Qbox website http://qboxcode.org/index.htm, Qbox is designed for operation on large parallel computers.</p>"},{"location":"theta/applications-and-libraries/applications/qbox/#using-qbox-at-alcf","title":"Using Qbox at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. Some provided executables on Theta are available here: /soft/applications/qbox. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta/applications-and-libraries/applications/qbox/#how-to-obtain-the-code","title":"How to Obtain the Code","text":"<p>Qbox is an open-source code that can be downloaded at http://qboxcode.org/. Check the Qbox website for current information and latest releases.</p>"},{"location":"theta/applications-and-libraries/applications/qbox/#building-on-theta","title":"Building on Theta","text":"<p>Qbox requires the standard math libraries plus the Xerces-C library, which can be downloaded at http://xerces.apache.org/xerces-c.</p>"},{"location":"theta/applications-and-libraries/applications/qbox/#xerces-c-312","title":"Xerces-C 3.1.2","text":"<p>In the xerces directory</p> <pre><code>./configure --host=x86_64-build-linux-gnu --build=x86_64-target-linux-gnu CC=cc CXX=CC CFLAGS=-O2 CXXFLAGS=-O2 --prefix=${HOME}/xerces-c-3 --disable-shared  --disable-pretty-make --disable-threads --enable-transcoder-iconv --disable-netaccessor-curl\nmake\nmake install\n</code></pre> <p>The libraries and headers will be installed in the following paths:</p> <p>Libraries: ${HOME}/xerces-c-3/lib</p> <p>Headers: ${HOME}/xerces-c-3/include</p>"},{"location":"theta/applications-and-libraries/applications/qbox/#building-on-theta_1","title":"Building on Theta","text":"<p>After Qbox has been downloaded and unpacked, you should see a directory whose name is of the form qbox-. Go to the directory qbox-/build and create a new theta.mk arch file as described below.  <pre><code>#----------------------------------------------------------------------------\n#\n# theta.mk\n#\n#----------------------------------------------------------------------------\n#\n PLT=x86_64\n#----------------------------------------------------------------------------\n MPIDIR=$(I_MPI_ROOT)/intel64\n XERCESCDIR=$(HOME)/xerces-c-3\n PLTOBJECTS = readTSC.o\n\n CXX=CC -mkl\n LD=CC -mkl\n\n PLTFLAGS += -DIA32 -D_LARGEFILE_SOURCE \\\n             -D_FILE_OFFSET_BITS=64 -DUSE_MPI -DSCALAPACK -DADD_ \\\n             -DAPP_NO_THREADS -DXML_USE_NO_THREADS -DUSE_XERCES\n\n# FFT must be FFTW2, FFTW3, ESSL or NOLIB\n  FFT=FFTW3\n\nifeq ($(FFT),FFTW3)\n PLTFLAGS += -DUSE_FFTW3 -DUSE_FFTW3_THREADS\n PLTFLAGS += -DFFTWMEASURE\n PLTFLAGS += -DFFTW3_2D\n FFTWINCLUDEDIR=$(MKLROOT)/include/fftw\n INCLUDE += -I$(FFTWINCLUDEDIR)\nendif\n\nifeq ($(FFT),NOLIB)\n PLTFLAGS += -DFFT_NOLIB\nendif\n\nINCLUDE += -I$(MPIDIR)/include -I$(XERCESCDIR)/include\n\nCXXFLAGS=  -g -qopenmp -O3 -xMIC-AVX512 -fp-model fast=2 -no-prec-div -qoverride-limits -restrict -D$(PLT) \\\n            $(INCLUDE) $(PLTFLAGS) $(DFLAGS)\n\nLIBPATH += -L$(MPIDIR)/lib64 \\\n           -L$(MKLROOT)/lib/intel64 \\\n           -L$(XERCESCDIR)/lib\n\nLIBS += $(PLIBS) \\\n         -Wl,--start-group -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64 -lmkl_intel_lp64 -lmkl_core -lmkl_intel_thread -Wl,--end-group \\\n         -lxerces-c -lpthread -liomp5 -lm -ldl\n\n# Parallel libraries\n PLIBS =\n\n LDFLAGS = -g $(LIBPATH) $(LIBS)\n#----------------------------------------------------------------------------\n</code></pre> <p>As newer versions of Qbox are distributed and changes made to the Makefile, the example arch file above can be used to generate an arch file appropriate for the specific version of Qbox. With the arch file in place, Qbox can be compiled from the qbox-/src directory using the following command. <p><code>make TARGET=../build/theta -j 16</code></p>"},{"location":"theta/applications-and-libraries/applications/qbox/#running-qbox-jobs-on-theta","title":"Running Qbox Jobs on Theta","text":"<p>The following is an example executable script \u201crun_qbox.csh\u201d to run Qbox on two nodes of Theta with 64 MPI ranks per node. The job can be submitted with command \u201cqsub run_qbox.csh\u201d where  is replaced with an active project allocation. <pre><code>#!/bin/csh\n#COBALT -n 2 -t 10 -q debug-cache-quad -A &lt;project_name&gt; -O QBOX\n\naprun -n 128 -N 64 -d 1 --cc depth -e OMP_NUM_THREADS=1 -j 1 ./qb test.i\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/qbox/#performance-notes","title":"Performance Notes","text":"<p>The text below is taken from a discussion on the Qbox user forum regarding NROWMAX.</p> <p>The nrowmax variable is used to determine the shape of the rectangular process grid used by Qbox. This process grid is the one used by the Scalapack library. When Qbox starts, the ntasks MPI tasks are assigned to processes arranged in a rectangular array of dimensions nprow * npcol. The default value of nrowmax is 32. The plane-wave basis is divided among nprow blocks, and the electronic states are divided among npcol blocks.</p> <p>The following algorithm is used by Qbox to determine the values of nprow and npcol:</p> <p>The number of rows nprow is first set to nrowmax. The value of nprow is then decremented until ntasks%nprow==0, i.e. nprow divides the total number of tqasks. The value of npcol is then given by ntasks/nprow. While this looks cryptic, what this algorithm tries to achieve is actually quite simple: define a process grid of dimensions nrowmax*npcol, where npcol=ntasks/nrowmax. This is not always possible, in particular if ntasks%nrowmax != 0. This is why the second part of the algorithm decrements nprow until ntasks%nprow==0.</p> <p>Note: The value of nprow is never larger than nrowmax (hence the name).</p> <p>This algorithm is implemented in Wavefunction::create_contexts() in file Wavefunction.C</p> <p>Examples: <pre><code>ntasks=128, nrowmax=32 (default) =&gt; process grid 32 x 4\nntasks=48, nrowmax=32 (default) =&gt; process grid 24 x 2\nntasks=256, nrowmax=64 =&gt; process grid 64 x 4\n</code></pre> The shape of the process grid affects performance. In general, it is advantageous to have nprow as large as possible, but not larger than the size of the (fine) FFT grid in the z direction. For example, if the fine FFT grid (printed as np0v,np1v,np2v on output) is 110 x 110 x 110, the value of nrowmax should be 110. Note that other values of nrowmax also work, but performance is usually inferior. For example, one could use nrowmax=128 even if the grid is 110 x 110 x 110, but some of the processes will not be used optimally during FFTs.</p> <p>Choosing the value of nrowmax is usually a trial-and-error process. Before running long simulations, it is advisable to run a few test jobs with different values of nrowmax and choose the value that gives best performance.</p>"},{"location":"theta/applications-and-libraries/applications/qmcpack/","title":"QMCPACK on Theta","text":""},{"location":"theta/applications-and-libraries/applications/qmcpack/#overview","title":"Overview","text":"<p>QMCPACK is a modern, high-performance, open-source Quantum Monte Carlo (QMC) simulation code. Its main applications are electronic structure calculations of molecular, quasi-2D, and solid-state systems.</p>"},{"location":"theta/applications-and-libraries/applications/qmcpack/#how-to-access-qmcpack","title":"How to access QMCPACK","text":"<p>Prebuilt QMCPACK binaries are provided on Theta under /soft/applications/qmcpack in folder latest-release and current-develop. latest-release uses the latest release of QMCPACK and is recommended for production runs. current-develop uses the development branch which contains new features and bug fixes before the next release. While latest-release and current-develop are updated, all the old binaries are still accessible to users under the folder of year (2017,2018,2019). All the binaries are dynamically linked and require certain versions of libraries to function properly. Please read the README file and load necessary modules. QMCPACK is heavily optimized for Xeon Phi processors by using the Structure-of-Array data layout and new algorithms. Please use the SoA binary whenever possible.</p>"},{"location":"theta/applications-and-libraries/applications/qmcpack/#using-qmcpack-at-alcf","title":"Using QMCPACK at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. For questions, contact us at support@alcf.anl.gov.</p> <p>If building your own QMCPACK is necessary, follow the instruction described below.</p>"},{"location":"theta/applications-and-libraries/applications/qmcpack/#building-on-theta","title":"Building on Theta","text":"<p>This recipe was verified on February 20, 2019, with QMCPACK v3.6.0.</p> <pre><code>export CRAYPE_LINK_TYPE=dynamic\n# Do not use cmake 3.9.1, it causes trouble with parallel HDF5.\nmodule load cmake/3.11.4\nmodule unload cray-libsci\nmodule load cray-hdf5-parallel\nmodule load gcc   # Make C++ 14 standard library available to the Intel compiler\nexport BOOST_ROOT=/soft/libraries/boost/1.64.0/intel\ncmake -DENABLE_SOA=1 ..\nmake -j 24\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/qmcpack/#running-qmcpack-jobs-on-theta","title":"Running QMCPACK jobs on Theta","text":"<p>Below is an example of a submission script of running the binary qmcpack on Theta:</p> <pre><code>#!/bin/bash\n#COBALT -q default\n#COBALT -A YOUR_PROJECT\n#COBALT -n 128\n#COBALT -t 30\n#COBALT -O dmc\n#COBALT --attrs mcdram=cache:numa=quad\n\nfile_prefix=NiO-fcc-S8-dmc\nexe=/soft/applications/qmcpack/latest-release/build_KNL_Intel_real_SoA/bin/qmcpack\n\nNCORES=64\nHT=1\nNTHREADS=$((NCORES * HT))\n\naprun -n $COBALT_PARTSIZE -N 1 -cc depth -d $NTHREADS -j $HT $exe $file_prefix.xml &gt; $file_prefix.out\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/qmcpack/#references","title":"References","text":"<ul> <li>https://qmcpack.org</li> <li>https://qmcpack.readthedocs.io/en/develop/</li> </ul>"},{"location":"theta/applications-and-libraries/applications/quantum-espresso/","title":"Quantum ESPRESSO on Theta","text":""},{"location":"theta/applications-and-libraries/applications/quantum-espresso/#what-is-quantum-espresso","title":"What Is Quantum ESPRESSO?","text":"<p>Quantum ESPRESSO (QE) is a suite of codes for electronic structure calculations and materials research. This code uses Density Functional Theory calculations with periodic boundary conditions to estimate energies, forces, and other properties of atomic scale systems. QE runs in parallel (MPI and OpenMP) and is based on plane waves basis functions and pseudopotentials. QE is an open-source project at http://www.quantum-espresso.org/.</p>"},{"location":"theta/applications-and-libraries/applications/quantum-espresso/#how-to-access-quantum-espresso","title":"How to access Quantum ESPRESSO","text":"<p>Prebuilt QE binaries are provided on Theta under /soft/applications/quantum_espresso. The binaries for v5.3.0 and v6.2.1 are dynamically linked and require certain version of libraries to function. Please read the README file and load necessary modules. The binaries for v6.3 and beyond are statically linked and there is no dependance on loaded modules.</p>"},{"location":"theta/applications-and-libraries/applications/quantum-espresso/#using-quantum-espresso-at-alcf","title":"Using Quantum ESPRESSO at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. For questions, contact us at support@alcf.anl.gov.</p> <p>If building your own QE is necessary, follow the instructions below.</p>"},{"location":"theta/applications-and-libraries/applications/quantum-espresso/#building-on-theta","title":"Building on Theta","text":"<p>This recipe is verified on Oct 12th 2018 with QE-6.3.</p> <p>QE uses autotools to create a make.inc file required to compile the code. First, obtain a copy of source code on the Theta login nodes.</p> <p>We suggest compiling the code with Intel Fortran compiler (under Cray wrapper) and using Intel MKL for both linear algebra and FFT. Due to the conflict of MKL and libsci, libsci needs to be unloaded. Enabling wavefunction IO via hdf5 library is optional.</p> <p><pre><code>module unload cray-libsci\nmodule load cray-hdf5-parallel \nexport CRAYPE_LINK_TYPE=dynamic\n</code></pre> Inside the source code directory, QE can be configured with the follow command in a shell terminal: <pre><code>./configure \\\n --prefix=/home/myhome/mypath \\\n MPIF90=ftn CC=cc --enable-openmp --with-scalapack=intel --with-hdf5=/opt/cray/pe/hdf5-parallel/1.10.1.1/INTEL/16.0 \\\n --with-elpa-include=/soft/applications/elpa/elpa-2017.11.001/include/elpa-2017.11.001/modules \\\n --with-elpa-lib=/soft/applications/elpa/elpa-2017.11.001/lib/libelpa.a\n</code></pre>  Now proceed to build the code. Use \"make\" and install it with \"make install\".</p> <p>If statically linked binaries are needed, do the following before hitting \"make\":</p> <p><pre><code>export CRAYPE_LINK_TYPE=static\n</code></pre> Edit make.inc by replacing <pre><code>-lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64\n</code></pre> in SCALAPACK_LIBS with <pre><code>${MKLROOT}/lib/intel64/libmkl_scalapack_lp64.a -Wl,--start-group ${MKLROOT}/lib/intel64/libmkl_intel_lp64.a ${MKLROOT}/lib/intel64/libmkl_intel_thread.a ${MKLROOT}/lib/intel64/libmkl_core.a ${MKLROOT}/lib/intel64/libmkl_blacs_intelmpi_lp64.a -Wl,--end-group\n</code></pre></p>"},{"location":"theta/applications-and-libraries/applications/quantum-espresso/#running-qe-jobs-on-theta","title":"Running QE Jobs on Theta","text":"<p>Below is an example of a submission script of running the binary pw.x of QE in Theta:</p> <pre><code>#!/bin/bash\n#COBALT -n 8\n#COBALT -t 10\n#COBALT -q default\n#COBALT -A my_project\n#COBALT -O my_test\n\nPROCS_PERNODE=16\nHT=1\nPROCS=$((COBALT_PARTSIZE * PROCS_PERNODE))\nNTHREADS=$((64 * HT / PROCS_PERNODE))\n\npw=/soft/applications/quantum_espresso/6.3/bin/pw.x\n\necho \"Running Cobalt Job $COBALT_JOBID.\"\n\naprun -n $PROCS -N $PROCS_PERNODE -cc depth -d $NTHREADS -j $HT $pw -in ./test.in &amp;&gt; ./out\n</code></pre> <p>This script file can be submitted as \u2018qsub script.sh\u2019, assuming you have a \u2018test.in\u2019 file in place.</p>"},{"location":"theta/applications-and-libraries/applications/quantum-espresso/#references","title":"References","text":"<p>QE User Manual</p>"},{"location":"theta/applications-and-libraries/applications/quantum-package/","title":"Quantum Package","text":""},{"location":"theta/applications-and-libraries/applications/quantum-package/#what-is-quantum-package","title":"What Is Quantum Package?","text":"<p>Quantum Package (QP) is a Fortran/MPI scalable parallel implementation of selected Configuration Interaction (sCI), approaching the full CI answer when fully converged,. Quantum Package is currently interfaced to Pyscf and QMCPACK and allows the generation of high accuracy trial wavefunctions for small to medium size molecules and solids.  A more complete description of the methods implemented in the code can be found on the QP website https://quantumpackage.github.io/qp and in the publication https://doi.org/10.26434/chemrxiv.7749485.v2. While the code is highly scalable, the method underneath sCI scales O(N!) with number of orbitals N. Therefore it\u2019s usage applies to systems smaller than 800 orbitals.</p>"},{"location":"theta/applications-and-libraries/applications/quantum-package/#using-quantum-package-at-alcf","title":"Using Quantum Package at ALCF","text":"<p>ALCF does not officially support QP software, but provides a binary located and maintained in /soft/applications/quantum_package and guidance to compile your own. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta/applications-and-libraries/applications/quantum-package/#how-to-obtain-the-code","title":"How to Obtain the Code","text":"<p>Quantum Package is an open source code downloadable at https://github.com/QuantumPackage/qp2. For more information refer to the code manual https://quantumpackage.github.io/qp2.</p>"},{"location":"theta/applications-and-libraries/applications/quantum-package/#building-on-theta","title":"Building on Theta","text":"<p>While it is recommended to use the version of the code available on /soft/applications/quantum_package , you can build your own version on Theta as follow.</p> <pre><code>git clone https://github.com/QuantumPackage/qp2 cd qp2 sed s/\u201dmpiifort\u201d/\u201dftn\u201d/g &gt; config/ifort_avx_mpi.cfg ./configure -c  config/ifort_avx_mpi.cfg source quantum_package.rc ./configure \u2013i all source quantum_package.rc ./configure -c config/ifort_avx_mpi.cfg ninja\n</code></pre> <p>Executables are installed and updated on Theta as often as the new updates are released.</p>"},{"location":"theta/applications-and-libraries/applications/quantum-package/#running-jobs-on-theta","title":"Running Jobs on Theta","text":"<p>The following is an example script \u201crun_fci.sh\u201d to run an fci job on Theta on 128 nodes with 1MPI task per node and 128 threads.  The job can be submitted with command \u201cqsub run_fci.sh.\u201d</p> <pre><code>cat run_fci.sh #!/bin/bash #COBALT -q default #COBALT -A PRJECT_NAME    #COBALT -n 128 #COBALT -t 180 #COBALT -O MyOutputName #COBALT --attrs mcdram=cache:numa=quad exportATP_ENABLED=1 file_prefix=File  exe=/soft/applications/quantum_package source ${exe}/quantum_package.rc  NCORES=64 HT=2 NTHREADS=$((NCORES * HT)) let SLAVE_NODE=${COBALT_PARTSIZE}-1 aprun -n1-N1-ccdepth -d$NTHREADS-j$HTqp_run fci ${file_prefix}.ezfio&gt;&gt;${file_prefix}-fci.out &amp; sleep 360 aprun -n${SLAVE_NODE}-N1-ccdepth -d$NTHREADS-j$HTqp_run \u2013s fci ${file_prefix}.ezfio&gt;&gt;${file_prefix}-fci-slave.out &amp; wait qsub run_fci.sh\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/vasp/","title":"VASP","text":""},{"location":"theta/applications-and-libraries/applications/vasp/#what-is-vasp","title":"What is VASP?","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a software package for performing electronic structure calculations with periodic boundary conditions. It is most commonly used that to perform density functional theory (DFT) calculations in a planewave basis using the projector augemented wave (PAW) method. A more complete description of VASP can be found here: https://www.vasp.at</p>"},{"location":"theta/applications-and-libraries/applications/vasp/#using-vasp-at-alcf","title":"Using VASP at ALCF","text":"<p>VASP is commercial software. Access to binaries compiled by ALCF can only be accessed after the user requesting access has been verified to be on the VASP license by an official VASP license distributor. </p> <p>To access the VASP binary at ALCF, please email the details listed directly below to support@alcf.anl.gov. It can take up to 5 - 10 business days to verify a VASP license. These waiting times are longer than usual due to the impact of COVID-19.</p> <p>Information to provide: - User\u2019s Full Name: - User\u2019s ALCF username: - Name of Organization that purchased the VASP license: - Principal Investigator who is the POC for the VASP license: - VASP license number: - Version of VASP requested (VASP5, VASP6):  - ALCF resource which you plan to run VASP on: [Theta, ThetaGPU, Polaris other]</p>"},{"location":"theta/applications-and-libraries/applications/vasp/#vasp-support-policy","title":"VASP Support Policy","text":"<p>ALCF compiles the latest release of VASP on a per request basis. We do not offer support for compiling customized versions of VASP with plugins. We are able to provide Makefiles and step-by-step build instructions to users with a verified VASP license. Support for scientific runs that encounter performance or numerical issues should be directed to the official VASP support mailing list or the VASP user forum. Limited support is available for fatal errors encountered at run time. </p>"},{"location":"theta/applications-and-libraries/applications/vasp/#how-to-obtain-the-code","title":"How to Obtain the Code","text":"<p>The VASP souce can only be obtained by an official license reseller of VASP. This is either the University of Vienna or Material Designs, Inc.</p>"},{"location":"theta/applications-and-libraries/applications/vasp/#running-jobs-on-theta","title":"Running Jobs on Theta","text":"<p>We have two versions of VASP available: a) 5.4.4. with April 2017 patch b) 6.3.2. Please note that we are no longer providing access to VASP 6-dev (pre-release) per instructions from VASP headquarters.</p> <p>The binaries are available here: <pre><code>/soft/applications/vasp/vasp.5.4.4.18Apr17/bin/\n/soft/applications/vasp/vasp.6.3.2/bin/\n</code></pre> The top-level directory (/soft/applications/vasp) contains example scripts for running VASP as well as step-by-step instructions.</p>"},{"location":"theta/applications-and-libraries/applications/vasp/#references","title":"References","text":"<p>Please encourage your group to do some tests to determine which binary better suits their needs. Here are some presentations and papers that may be useful in making a decision: - Using VASP at NERSC - Performance of Hybrid MPI/OpenMP VASP on Cray XC40 Based on KNL</p>"},{"location":"theta/applications-and-libraries/applications/west/","title":"WEST on Theta","text":""},{"location":"theta/applications-and-libraries/applications/west/#what-is-west","title":"What Is WEST?","text":"<p>WEST is a Fortran/MPI scalable parallel implementation of large-scale electronic structure calculations within many-body perturbation theory. WEST is currently interfaced with Quantum Espresso planewave DFT software. As described on the WEST website http://west-code.org, WEST is highly scalable and is used for calculations of solids, liquids, nanostructures, molecules, and interfaces, including samples with ~2000 electrons.</p>"},{"location":"theta/applications-and-libraries/applications/west/#using-west-at-alcf","title":"Using WEST at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. Some provided executables on Theta are available here: /soft/applications/qe_west/qe_v6.1.0-west_3.1.0. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta/applications-and-libraries/applications/west/#how-to-obtain-the-code","title":"How to Obtain the Code","text":"<p>WEST is an open-source code that can be downloaded at http://west-code.org. Similarly, the Quantum Espresso code can be downloaded at https://www.quantum-espresso.org. Check the WEST website for current information on supported algorithms.</p>"},{"location":"theta/applications-and-libraries/applications/west/#building-on-theta","title":"Building on Theta","text":"<p>WEST currently requires a working PW executable from Quantum Espresso. Current information on installation details for both WEST and PW can be found at http://west-code.org/documentation.php. After Quantum Espresso and WEST codes have been downloaded and unpacked, the PW and WEST executables can be compiled using the following script in the qe- directory. <pre><code>cat build_theta.sh\n#!/bin/bash\n\nexport BLAS_LIBS=\"-L$MKLROOT/intel64/lib -Wl,--start-group -lmkl_intel_lp64 -lmkl_core -lmkl_intel_thread -Wl,--end-group\"\nexport SCALAPACK_LIBS=\"-L$MKLROOT/intel64/lib -Wl,--start-group -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64 -lmkl_intel_lp64 -lmkl_core -lmkl_intel_thread -Wl,--end-group\"\nexport FFT_LIBS=\"\"\nexport MPIF90=\"ftn -g -mkl\"\nexport CC=\"cc -g -mkl\"\nexport F77=\"ftn -g -mkl\"\nexport FFLAGS=\"-xMIC-AVX512 -align array64byte -fp-model fast=2 -no-prec-div -assume byterecl\"\n\n./install/configure --host=x86_64-build-linux-gnu --build=x86_64-target-linux-gnu --enable-parallel --with-scalapack --enable-openmp\n\nmake pw -j 16\n\ncd West\nmake\n</code></pre> <p>As newer versions of Quantum Espresso and WEST are released, check the corresponding websites for current information.</p> <pre><code>&gt; cat run_west.sh\n#!/bin/bash\n#COBALT -n 2 -t 10 -q debug-cache-quad -A &lt;project_name&gt; -O WEST\n\naprun -n 128 -N 64 -d 1 --cc depth -e OMP_NUM_THREADS=1 -j 1 ./wstat wstat.i\n&gt; qsub run_west.sh\n</code></pre>"},{"location":"theta/applications-and-libraries/applications/west/#running-west-jobs-on-theta","title":"Running WEST Jobs on Theta","text":"<p>The following is an example executable script \u201crun_west.sh\u201d to run the wstat WEST executable on two nodes of Theta with 64 MPI ranks per node. The job can be submitted with command \u201cqsub run_west.sh\u201d where  is replaced with an active project allocation."},{"location":"theta/applications-and-libraries/libraries/boost/","title":"Boost on Theta","text":"<p>Boost, a collection of modern, peer-reviewed C++ libraries, is installed on Theta and can be accessed by loading the module corresponding to the appropriate C++ compiler: <pre><code>boost/intel/&lt;version&gt; - Boost compiled with Intel's C++ compiler. \nboost/gnu/&lt;version&gt; - Boost compiled with the GNU C++ compiler. \nboost/cray/&lt;version&gt; - Boost compiled with Cray's C++ compiler. \nboost/llvm/&lt;version&gt; - Boost compiled with the Clang C++ compiler.\nboost/llvm-libc++/&lt;version&gt; - Boost compiled with the Clang C++ compiler using libc++ as the C++ standard library (for compatibility with the -stdlib=libc++ option). \n</code></pre></p> <p>The modules will adjust to include linker paths so that the Boost header files and libraries can be found by the compiler and linker. The modules will also set the \u201cBOOST_ROOT\u201d environmental variable. The modules, however, will not cause any Boost library to be automatically linked to your application. If you use a Boost library that requires linking to a pre-compiled library, you are responsible for adding the necessary linking flags (e.g.,  -lboost_program_options-mt). The name of each library has the following form:</p> <p><pre><code>libboost_&lt;name&gt;-&lt;variant&gt;.{so,a}\n</code></pre> where   is the variant tag as explained here: http://www.boost.org/doc/libs/1_64_0/more/getting_started/unix-variants.html"},{"location":"theta/applications-and-libraries/libraries/spack/","title":"Spack","text":"<p>Spack is a package manager developed for HPC which supports combinatorial versioning, i.e. it allows for multiple versions of packages to be built. These builds can vary with canonical version number, build options, compilers, and processor architectures. Each of a package\u2019s dependencies can be similarly versioned, and a built version is fully specified by a concretized spack spec, and referenced by a hash generated from the specified options.</p> <p>The learning curve for spack can be steep and new users are encouraged to experiment with their own installation of spack (easily available for cloning at https://github.com/spack/spack) and to look at configuration settings used in the ALCF installation for hints as to what settings may be appropriate and useful. </p> <p>Example settings for Theta are available at /soft/spack/alcf/theta. Not all of these settings will be useful for all builds and some may foil spack\u2019s concretization process, and it is not recommended to adopt these wholesale as global settings. The recommended method is to include these settings ad hoc in a spack environment to more tightly control what information spack\u2019s concretizer uses for its builds.</p> <p>There are a growing number of packages installed to the ALCF spack instance at /soft/spack/root, and this instance can be used as a spack upstream resource as described here:</p> <p>https://spack.readthedocs.io/en/latest/chain.html#using-multiple-upstream-spack-instances. Pointing to the ALCF spack instance as an upstream repository is trivial. Simply create an \u2018upstreams.yaml\u2019 file in any of the configuration scopes (e.g. $SPACK_ROOT/etc/spack/upstreams.yaml) will allow your instance to leverage any builds in the upstream, and will appear when running \u2018spack find\u2019.</p> <p>An example upstreams.yaml: <pre><code>upstreams: \n    alcf-spack: \n        install_tree: /soft/spack/root/opt/spack\n</code></pre> Support requests and feedback for ALCF-specific issues should be directed to support@alcf.anl.gov. For general spack questions, users are encouraged to consult the following resources:</p> <ul> <li>Spack development website</li> <li>Spack documentation</li> <li>Spack tutorial</li> <li>Spack Slack channel</li> </ul>"},{"location":"theta/applications-and-libraries/visualization/remote-vis/","title":"Remote Visualization on Theta Using VNC","text":"<p>For visualization and analysis applications that do not support a client/server mode, VNC can be used for remotely accessing such applications running on Theta.</p>"},{"location":"theta/applications-and-libraries/visualization/remote-vis/#setup-on-theta","title":"Setup on Theta","text":"<p>On cooley.alcf.anl.gov, if you do not have a ~/.vnc/xstartup file, create one like the following: <pre><code>#!/bin/sh\n   xterm &amp;\n   icewm\n</code></pre> Be sure to make it executable:</p> <p><code>chmod u+x ~/.vnc/xstartup</code> Also, create a VNC password, which you will need to provide each time you connect a remote VNC client to a VNC server running on Theta:</p> <p><code>vncpasswd</code></p> <p>This will store an obfuscated version in ~/.vnc/passwd</p>"},{"location":"theta/applications-and-libraries/visualization/remote-vis/#start-a-vnc-server-on-theta","title":"Start a VNC server on Theta","text":"<p>Since we want the VNC server to run on a backend node, in order to avoid increasing the load to login and mom nodes, we need to submit an interactive job:</p> <p><code>qsub -I -n 1 -t &lt;time&gt; -q debug-cache-quad -A &lt;projectID&gt;</code></p> <p>Once your job starts, you will be logged into a mom node, where you can launch a VNC server:</p> <p>Note: Make a note of your node number <pre><code>vncserver --NeverShared=1 -geometry 1920x1080\nx0vncserver --display=:0.0 --NeverShared=1 --geometry=2400x1500+0+0 --PasswordFile=/home/&lt;username&gt;/.vnc/passwd --MaxProcessorUsage=100\n</code></pre></p> <p>Notes:</p> <ul> <li>Take note of the hostname where your job is running (in the form cc###).  You will need this in the next steps.</li> <li>We use x0vncserver so that we can leverage the existing X server running on the node, which uses the GPU.  </li> <li>We specify --display=:0.0 to tell it which display to use.</li> <li>Because the existing display has a resolution of 4096x4096, we use the --geometry flag to specify a region of that display to use.  This should be set this to a size appropriate for displaying on your local display.  You may also wish to adjust the +0+0 to adjust the portion of the display that is visible.</li> <li>Replace  with your login name in the path to your VNC PasswordFile. <li>Since we will have exclusive use of the node, we set the --MaxProcessorUsage=100 (otherwise the default is 35).</li>"},{"location":"theta/applications-and-libraries/visualization/remote-vis/#on-your-local-resource","title":"On Your Local Resource","text":"<p>From a shell on your local resource, establish an ssh tunnel through the Cooley login node to the backend node where you started the VNC server (the cc### noted above.) This will require the use of your OTP token.</p> <p><code>ssh -L 5900:cc###:5900 &lt;username&gt;@cooley.alcf.anl.gov</code></p> <p>Once the ssh connection is established, from this shell launch the xstartup script on your visualization node.  </p> <p>If your default shell is bash, use the following command (this will block, and not return you to a command prompt): <code>ssh cc### \u201cexport DISPLAY=:0.0; ~/.vnc/xstartup\u201d</code></p> <p>If your default shell is csh/tcsh, use the following command (this will block, and not return you to a command prompt):</p> <p><code>ssh cc### \u201csetenv DISPLAY :0.0; ~/.vnc/xstartup\u201d</code></p> <p>Now in a start a vnc viewer on your local resource, for example:</p> <p><code>xvncviewer localhost::5900</code></p> <p>Notes: - Since we are tunneling, set the host to localhost. - Syntax for VNC clients may vary.  Check the documentation for your specific client to determine appropriate syntax for specifying the host and port.</p> <p>This should open a vnc viewer with an xterm running in it, where you can launch graphics applications running on the Cooley backend node, and taking advantage of the GPU.</p> <p>Additional Note: - Because you are likely not using the full 4096x4096 area of the display, it is possible that some applications that automatically place their windows may place them outside of the region that you are viewing.  Some application may provide a mechanism for placing the window at a specific location.  Otherwise, you may need to adjust the +0+0 portion of the --geometry flag when running the x0vncserver executable to adjust the portion of the display that is visible.</p>"},{"location":"theta/applications-and-libraries/visualization/remote-vis/#cleaning-up","title":"Cleaning Up","text":"<p>When you are all done, be sure to clean up: - Exit the VNC viewer - Kill the VNC server (cntrl C), and exit the cc### shell - You may need to cntrl C to exit the ssh command in the shell used to create the tunnel  - Then exit that shell to close the tunnel</p>"},{"location":"theta/compiling-and-linking/clang-compiler-theta/","title":"Clang Compiler on Theta","text":""},{"location":"theta/compiling-and-linking/clang-compiler-theta/#using-clang-compiler","title":"Using Clang Compiler","text":"<p>The Clang compiler, which is part of the LLVM compiler infrastructure, is available as a programming environment on Theta. To use Clang, switch your current programming environment module with the <code>PrgEnv-llvm</code> module. </p> <p>For example: <code>module switch PrgEnv-intel PrgEnv-llvm</code></p>"},{"location":"theta/compiling-and-linking/clang-compiler-theta/#mailing-list-and-support","title":"Mailing List and Support","text":"<p>For help with Clang-related questions, ALCF users can e-mail support@alcf.anl.gov</p>"},{"location":"theta/compiling-and-linking/clang-compiler-theta/#general-usage","title":"General Usage","text":"<p>Clang\u2019s command-line argument handling is designed to be similar to gcc\u2019s command-line argument handling, and where possible, Clang tries to support the same flags. For more information on Clang, see Clang: a C language family frontend for LLVM.</p>"},{"location":"theta/compiling-and-linking/clang-compiler-theta/#fast-math-optimizations","title":"Fast-Math Optimizations","text":"<p>Clang supports a number of fast-math optimizations, enabled by passing -ffast-math, which increase performance but violate the relevant IEEE specification on floating-point computation.</p>"},{"location":"theta/compiling-and-linking/clang-compiler-theta/#link-time-optimization-lto","title":"Link-Time Optimization (LTO)","text":"<p>LTO is a powerful feature of Clang and its associated toolchain, which enables the compiler to perform additional global optimizations, such as function inlining as part of the final linking process. This can be expensive in terms of compile time, but can yield significant runtime performance gains.</p> <p>To use LTO, you must pass the -flto flag to Clang, both when compiling and when linking.</p>"},{"location":"theta/compiling-and-linking/clang-compiler-theta/#faq","title":"FAQ","text":"<p>Why do I receive linking errors complaining about multiple definitions of inline functions?</p> <p>The source code you're compiling probably assumes the GNU semantics for the inline keyword, and not those defined by the C99 standard. Compile your code with the -fgnu89-inline flag to force Clang to use the non-standard GNU semantics.</p> <p>Linking code compiled with clang++ together with code compiled with clang++ -stdlib=libc++ does not work, why?</p> <p>Code compiled using clang++ uses the same libstdc++ standard template library (STL) implementation as the GNU g++ compiler. This provides compatibility with C++ libraries, including some system libraries, compiled with the GNU programming environment. </p> <p>When using <code>-stdlib=libc++</code>, however, LLVM's libc++ is used as the STL implementation. This STL implementation is incompatible with libstdc++, and so linking errors will result for functions that use STL objects as part of their signatures (i.e., parameter or return types).</p> <p>Is there a corresponding Fortran compiler available?</p> <p>The Flang Fortran frontend will be available soon.</p>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/","title":"Compiling and Linking Overview on Theta","text":"<p>Note: ThetaGPU have very different programming environment. See webpages dedicated to ThetaGPU.</p>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#compiling-and-linking-on-theta","title":"Compiling and Linking on Theta","text":"<p>Due to hardware differences between login (non-KNL) and compute nodes (Intel Phi 2nd Generation KNL), compilation of application codes on Theta systems normally involves cross-compiling. The module craype-mic-knl loaded by default defines the target architecture along with the appropriate compilation options that tune the compiler optimization to KNL hardware. For that reason, any compiler options preexisting in the application Makefile that auto-tune the code to the automatically recognized CPU architecture of the login node where the compilation proceeds have negative impact on performance of the compiled code, and should be removed.</p> <p>The default programming environment on Theta provides Intel compiler suite. Additional available compilers are Cray, GNU, and Clang, each arranged into a respective programming environment.</p>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#wrapper-command-names","title":"Wrapper Command Names","text":"<p>Regardless of the choice of a particular compiler suite, compilation on Theta requires the use of standard Cray compiler wrappers, which automatically invoke the correct base compiler provided by the selected programming environment and include MPI and other dependencies:</p> <ul> <li>cc \u2013 for C compiler</li> <li>CC \u2013 for C++ compiler</li> <li>ftn \u2013 for Fortran compiler</li> </ul> <p>The mpicc, mpiCC, mpic++, mpif77, or mpif90 wrappers used outside Cray systems must be substituted by the corresponding cc, CC, or ftn wrappers in the Makefile in order to generate a working binary code on Theta.</p>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#usage","title":"Usage","text":"<p>Following example illustrates compilation of a hello.c, hello.cpp, and hello.f, which are C, C++, and Fortran program codes, respectively into an executable hello:</p> <pre><code>cc -g -O -o hello hello.c\nCC -g -O -o hello hello.cpp\nftn -g -O -o hello hello.f\n</code></pre> <p>It is highly recommended to include compiler option -g regardless of the used optimization level. It does not impact performance but facilitates reconstructing the call stack by mapping to the source code in case of abnormal termination of the running application.</p> <p>The compiler flags used by the Cray compiler wrappers can be displayed using the -craype-verbose option. Similarly, the include and library information used by the compiler wrappers can be displayed using the --cray-print-opts option.</p> <p>cc -craype-verbose test.c cc --cray-print-opts test.c</p> <p>Note: Application programming using C++14 standard requires the ability to link against recent GNU libraries. The command module load gcc adds the required version of GNU C and C++ libraries to the search path.</p>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#linking","title":"Linking","text":"<p>The standard mechanism of linking on Theta is to use compiler wrappers cc, CC, or ftn. The wrappers are aware of the required system libraries and include those automatically.</p> <p>All programming environments on Theta default to static linking. The linking mode can be controlled via environment variable:</p> <pre><code>export CRAYPE_LINK_TYPE=dynamic\n\nor\n\nexport CRAYPE_LINK_TYPE=static\n</code></pre> <p>Alternatively, compiler wrappers accept options -static or -dynamic to choose the linking mode. It is recommended to avoid using compiler options -Bstatic or -Bdynamic since the Cray wrappers do not analyze those options and directly pass them to the underneath linker that may result in conflicting linking instructions.</p> <p>When opting for dynamic linking it is recommended to link the required dynamic libraries by using the RUNPATH mechanism, i.e. incorporating the path to the libraries used at the linking stage into the application binary code so that the binary code knows where to find the required dynamic libraries at runtime. The Cray wrappers link with RUNPATH by default. This avoids reliance on LD_LIBRARY_PATH, which can sometimes lead to accidental linking against incompatible libraries. The following example illustrates how to check the RUNPATH in dynamically linked object code:</p> <p><pre><code>readelf -d a.out | grep RUNPATH\n</code></pre> where <code>a.out</code> is an application binary or a dynamically linked library.</p> <p>An existing bug in Intel programming environment on Theta leads to some environment variables not being property set, which prevents running dynamically linked codes in a Cobalt interactive session even when the application code is properly linked with RUNPATH. A simple workaround is to refresh the environment by reloading the PrgEnv-intel module after logging into the interactive session:</p> <p><pre><code>module swap PrgEnv-intel PrgEnv-cray; module swap PrgEnv-cray PrgEnv-intel\n</code></pre> which unload and then put back Intel programming environment. The bug affects only interactive sessions and does not apply to regular offline Cobalt batch jobs.</p>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#man-pages","title":"Man Pages","text":"<p>For additional information about Cray wrappers, see man pages:</p> <pre><code>man cc\nman CC\nman ftn\n</code></pre> <p>Likewise, each base compiler comes with the respective man page.</p>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#intel","title":"Intel","text":"<pre><code>man icc (C/C++) \nman ifort\n</code></pre>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#cray","title":"Cray","text":"<pre><code>man craycc\nman crayCC\nman crayftn\n</code></pre>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#gnu","title":"GNU","text":"<pre><code>man gcc \nman g++ \nman gfortran\n</code></pre>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#llvmclang-fortran-is-not-yet-available","title":"LLVM/Clang (Fortran is not yet available)","text":"<pre><code>clang --help\nclang++ --help\n</code></pre>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#changing-a-compiler-suite","title":"Changing a compiler suite","text":"<p>Choosing a non-default compiler on Theta presumes changing the programming environment. Following commands accomplish that operation:</p> <pre><code>module swap PrgEnv-intel PrgEnv-cray\nmodule swap PrgEnv-intel PrgEnv-gnu\nmodule swap PrgEnv-intel PrgEnv-llvm\n</code></pre>"},{"location":"theta/compiling-and-linking/compiling-and-linking-overview/#compiling-for-login-nodes","title":"Compiling for Login Nodes","text":"<p>Login nodes are a shared resource, and are not meant for production computations.</p> <p>It is acceptable to compile tools or other similar utilities that will run on the login node. These cannot be MPI applications though. The following command executed on the login node:</p> <pre><code>module swap craype-mic-knl craype-haswell\n</code></pre> <p>will instruct the programming environment to compile the binary for CPU architecture of the login node.</p>"},{"location":"theta/compiling-and-linking/theta-example-program-makefile/","title":"Example Program and Makefile for Theta","text":"<p>Here is a simple example program and makefile for building on the ALCF's Theta system. When you compile your code for Theta, you are normally cross-compiling for the compute nodes (Intel Xeon Phi 2nd Generation [KNL]), rather than directly compiling for the login nodes (non-KNL). The default, you are using the Intel compiler suite. We also provide the Cray, GNU, and Clang compiler suites via modules.</p>"},{"location":"theta/compiling-and-linking/theta-example-program-makefile/#c-program","title":"C Program","text":"<pre><code>/* File pi.c */\n#include &lt;mpi.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv)\n{\n  int n, myid, numprocs, i;\n  double PI25DT = 3.141592653589793238462643;\n  double mypi, pi, h, sum, x;\n\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myid);\n\n  /* Number of intervals */\n  n = 25;\n\n  MPI_Bcast(&amp;n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  h = 1.0 / (double) n;\n  sum = 0.0;\n  for ( i = myid + 1; i &lt;= n; i += numprocs )\n  {\n       x = h * ((double) i - 0.5);\n       sum += ( 4.0 / (1.0 + x*x) );\n  }\n  mypi = h * sum;\n  MPI_Reduce(&amp;mypi, &amp;pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (myid == 0)\n  {\n      printf(\"pi is approximately %.16f, Error is %.16f\\n\",\n               pi, fabs(pi - PI25DT));\n  }\n\n  MPI_Finalize();\n  return 0;\n}\n</code></pre>"},{"location":"theta/compiling-and-linking/theta-example-program-makefile/#makefile","title":"Makefile","text":"<pre><code># File makefile\nCC = cc\n\nOPTFLAGS = -O3\nCFLAGS = $(OPTFLAGS) -g\n\nall: pi\n\npi: pi.o\n  $(CC) $(CFLAGS) -o pi pi.o\n\npi.o: pi.o\n    $(CC) $(CFLAGS) -c pi.c\n\nclean:\n    rm pi pi.o\n</code></pre>"},{"location":"theta/compiling-and-linking/theta-example-program-makefile/#usage","title":"Usage","text":"<p>Session screenshot shows building and running, using an example job script: <pre><code>thetalogin6(pi)&gt; ls\nmakefile pi.c runpi.bash* \n\nthetalogin6(pi)&gt; make \ncc -O3 -g -c pi.c \ncc -O3 -g -o pi pi.o \n\nthetalogin6(pi)&gt; ls \nmakefile pi* pi.c pi.o runpi.bash* \nthetalogin6(pi)&gt; cat runpi.bash\n\n #!/bin/bash \necho \"Starting Cobalt job script\" \nexport n_nodes=$COBALT_JOBSIZE \nexport n_mpi_ranks_per_node=64 \nexport n_mpi_ranks=$(($n_nodes * $n_mpi_ranks_per_node)) \naprun -n $n_mpi_ranks -N $n_mpi_ranks_per_node -cc depth -d 4 -j 4 ./pi \n\nthetalogin6(pi)&gt; qsub -A AProject -n 1 -t 20 runpi.bash \n\n106040 thetalogin6(pi)&gt; qstat -u user1 \n\nJobID JobName User WallTime QueuedTime RunTime Nodes State attrs Queue Score Project \n====================================================================================\n106040 N/A user1 00:20:00 00:00:05 N/A 1 queued {'numa': 'quad', 'mcdram': 'cache'} default 51.0 AProject \n\nthetalogin6(pi)&gt; qstat -u user1 \n\nJobID JobName User WallTime QueuedTime RunTime Nodes State attrs Queue Score Project \n====================================================================================\n106040 N/A user1 00:20:00 00:00:25 00:00:19 1 starting {'numa': 'quad', 'mcdram': 'cache'} default 51.1 AProject \n\nthetalogin6(pi)&gt; qstat -u user1 \n\nJobID JobName User WallTime QueuedTime RunTime Nodes State attrs Queue Score Project \n====================================================================================\n106040 N/A user1 00:20:00 00:00:32 00:00:59 1 running {'numa': 'quad', 'mcdram': 'cache'} default 51.2 AProject \n\nthetalogin6(pi)&gt; ls -lt \ntotal 18240 \n-rw-r--r-- 1 user1 cobalt 1671 Jun 27 23:08 106040.cobaltlog \n-rw-r--r-- 1 user1 users 185 Jun 27 23:07 106040.output \n-rw-r--r-- 1 user1 users 0 Jun 27 23:07 106040.error \n-rwxr-xr-x 1 user1 users 9300576 Jun 27 23:06 pi* \n-rw-r--r-- 1 user1 users 10256 Jun 27 23:06 pi.o \n-rw-r--r-- 1 user1 users 152 Jun 27 23:05 makefile \n-rwxr-xr-x 1 user1 users 237 Jun 27 23:05 runpi.bash* \n-rw-r--r-- 1 user1 users 816 Jun 27 22:20 pi.c\n\n thetalogin6(pi)&gt; cat 106040.output \n\nStarting Cobalt job script pi is approximately 3.1417259869152536, Error is 0.0001333333254605 \nApplication 3373484 resources: utime ~6s, stime ~6s, Rss ~5036, inblocks ~0, outblocks ~8 \nthetalogin6(pi)&gt;\n</code></pre></p>"},{"location":"theta/compiling-and-linking/theta-library-compiler-tracking-overview/","title":"Library and Compiler Tracking on Theta","text":""},{"location":"theta/compiling-and-linking/theta-library-compiler-tracking-overview/#overview","title":"Overview","text":"<p>The ALCF trackdeps software monitors which compilers and libraries are being used to build executables on our systems.  The information collected is used to inform ALCF and DOE decisions on support and research priorities.</p> <p>Currently, tracking is enabled by default.  It can be disabled by running:</p> <pre><code>module unload trackdeps\n</code></pre>"},{"location":"theta/compiling-and-linking/theta-library-compiler-tracking-overview/#recorded-executable-information","title":"Recorded Executable Information","text":"<p>When executables are built with tracking enabled, the input files used to build the executable are recorded by the system. To view what input files the system has recorded for any particular executable, with the softenv key enabled, run:</p> <pre><code>trackdeps-show /path/to/the/executable\n</code></pre>"},{"location":"theta/data-science-workflows/apache-spark/","title":"Apache Spark","text":"<p>Apache Spark is a fast and general-purpose cluster computing system. </p>"},{"location":"theta/data-science-workflows/apache-spark/#spark-job","title":"Spark Job","text":"<p>Spark Job is a set of scripts that interfaces with Cobalt that automates the job submission process while using Apache Spark. These scripts can be found on Cooley and Theta under /soft/datascience/Spark_Job. If you want to make custom modifications, you can copy or git clone it to your own directory. In the following, we will call the installation directory, <code>/PATH/TO/SPARK_JOB</code>.</p> <p>The guaranteed stable user interface is the file: <code>/PATH/TO/SPARK_JOB/submit-spark.sh</code></p> <p>It is designed to minimize the changes required for deploying Spark applications. For absolute stability, you can use explicit version number for the path, eg: /soft/datascience/Spark_Job_v1.0.2.</p>"},{"location":"theta/data-science-workflows/apache-spark/#usage","title":"Usage","text":"<p><pre><code>submit-spark.sh [options] [JOBFILE [arguments ...]]\nJOBFILE (optional) can be:\n        script.py           pyspark scripts\n        bin.jar             java binaries\n        run-example CLASS   run spark example CLASS\n        scripts             other executable scripts (requires `-s`)\nRequired options:\n        -A PROJECT          Allocation name\n        -t WALLTIME         Max run time in minutes\n        -n NODES            Job node count\n        -q QUEUE            Queue name\nOptional options:\n        -o OUTPUTDIR        Directory for COBALT output files (default: current dir)\n        -s                  Enable script mode\n        -m                  Master uses a separate node\n        -p &lt;2|3&gt;            Python version (default: 3)\n        -I                  Start an interactive ssh session\n        -w WAITTIME         Time to wait for prompt in minutes (default: 30)\n        -h                  Print this help message\n</code></pre> The result output will be under the current directory.</p> <p>Without specifying a JOBFILE<code>, the script will submit a job via Cobalt, start Spark, and launch an ssh session to the master node in Bash, with all the environment properly set up and directory changed to</code>OUTPUTDIR`.  The Cobalt job will be deleted once you exit the ssh session.</p> <p>With a <code>JOBFILE</code> and optionally more arguments, the script will submit a job via Cobalt, start Spark, and pass the JOBFILE to <code>$SPARK_HOME/bin/spark-submit</code> automatically, unless <code>-s</code> is given (see below).</p> <p>The required options, <code>-A</code>, <code>-t</code>, <code>-n</code>, <code>-q</code>, correspond to their counterparts for <code>qsub</code> (part of Cobalt), and will be passed to <code>qsub</code>, see the manual page of <code>qsub</code> for details.</p> <p>The option <code>-o OUTPUTDIR</code> instructs the script to use <code>OUTPUTDIR</code> to save all the files.  By default, Cobalt will save files, <code>$COBALT_JOBID.{cobaltlog,error,output}</code>, under this directory.</p> <p>You can find a list of relevant environment variables in the file, <code>$COBALT_JOBID/env</code>.  In addition, under this <code>OUTPUTDIR</code>, Spark will use <code>$COBALT_JOBID/conf</code> as <code>SPARK_CONF_DIR</code>, <code>$COBALT_JOBID/logs</code> for logs, and <code>$COBALT_JOBID/workers</code> for Spark temporaries.</p> <p>The option <code>-s</code> instructs the script to run the JOBFILE as a generic executable scripts, within which you may call, o launch, YOURFILE, which is a Spark jar file or a PySpark script.</p> <p>The option <code>-m</code> instructs the script to avoid launch Spark executor processes on the master node, such that only the Spark driver runs on the master node.  This means that the actual number of nodes used by the executors is one less than the nodes requested (by <code>-n</code>).</p> <p>The option <code>-p &lt;2|3&gt;</code> sets a default python environment (Intel Python), either version 2 or 3.  If you pass an argument other than 2 or 3, no default python environment will be setup.  In this case, you are responsible to set it up in <code>env_local.sh</code>, if you intend to use PySpark.</p> <p>The option <code>-I</code> always launchs an ssh session to the master node, even if you also pass a JOBFILE.  The JOBFILE will start running, while the ssh session is established, so that you can inspect the running job.  Note that the job will be deleted once you exit the ssh session even if the JOBFILE is still running.</p> <p>The option <code>-w WAITTIME</code> instructs the script, if running in interactive mode (no JOBFILE or with <code>-I</code>), to wait at most WAITTIME minutes before it quits and deletes the job.</p> <p>In addition to the above options, a file <code>env_local.sh</code>, if exists under the <code>OUTPUTDIR</code> (see Optional options above), will be sourced by the script, to setup environment.  The file <code>env_local.sh</code> must be compatible to Bash installed in the host environment (both login nodes and compute nodes).</p>"},{"location":"theta/data-science-workflows/apache-spark/#environment-variables","title":"Environment Variables","text":"<p>The scripts set a few environment variables for informationalpurposes, and for controlling the behavior.</p> <p>As an example, the environment variables below show various information, taken from the command line, the job scheduler, and the system. Please do not modify these variables unless you know what you are doing.</p> <pre><code>SPARKJOB_HOST=\"theta\"\nSPARKJOB_INTERACTIVE=\"1\"\nSPARKJOB_JOBID=\"242842\"\nSPARKJOB_PYVERSION=\"3\"\nSPARKJOB_SCRIPTMODE=\"0\"\nSPARKJOB_SCRIPTS_DIR=\"/lus/theta-fs0/projects/datascience/xyjin/Spark_Job\"\nSPARKJOB_SEPARATE_MASTER=\"0\"\nSPARKJOB_OUTPUT_DIR=\"/lus/theta-fs0/projects/datascience/xyjin/Spark_Job/example\"\nSPARK_MASTER_URI=spark://nid03838:7077\nMASTER_HOST=nid03838\n</code></pre>"},{"location":"theta/data-science-workflows/apache-spark/#user-setup","title":"USER SETUP","text":"<p>The file, <code>env_theta.sh</code> or <code>env_cooley.sh</code>, contains preset configurations for either machine.  To override these presets, create a file, <code>env_local.sh</code>, under the <code>OUTPUTDIR</code>, which by default is where you launch <code>submit-spark.sh</code>.  Note that the file,<code>env_local.sh</code>, will be sourced by bash mutiple times.  You can change the default output directory, where <code>env_local.sh</code> should reside, by using the <code>-o</code> option to submit-spark.sh. Below are some useful customizable variables.</p> <p><pre><code>SPARK_HOME=\"/soft/datascience/apache_spark\"\nSPARK_CONF_DIR=\"/lus/theta-fs0/projects/datascience/xyjin/Spark_Job/example/242842/conf\"\nPYSPARK_PYTHON=\"/opt/intel/python/2017.0.035/intelpython35/bin/python\"\nSPARKJOB_WORKING_DIR=\"/lus/theta-fs0/projects/datascience/xyjin/Spark_Job/example/242842\"\nSPARKJOB_WORKING_ENVS=\"/lus/theta-fs0/projects/datascience/xyjin/Spark_Job/example/242842/envs\"\n</code></pre> The above is the environment set up when running a job under the OUTPUTDIR, <code>/projects/datascience/xyjin/Spark_Job/example</code></p> <p>The variable <code>SPARKJOB_OUTPUT_DIR</code> contains the directory path, and <code>SPARKJOB_WORKING_DIR</code> and <code>SPARKJOB_WORKING_ENVS</code> depends on <code>SPARKJOB_OUTPUT_DIR</code>. You can set customizable variables in <code>env_local.sh</code>.  We provide an example copy of this file under the <code>example</code> directory. TUNING PARAMETERS We use <code>env_local.sh</code> for generating <code>spark-defaults.conf</code> for each individual job. Typically for a scala job on Theta, you con put the following in the <code>env_local.sh</code> file.</p> <pre><code># The created spark-defaults.conf file will only affect spark\n# submitted under the current directory where this file resides.\n# The parameters here may require tuning depending on the machine and workload.\n[[ -s $SPARK_CONF_DIR/spark-defaults.conf ]] ||\n        cat &gt; \"$SPARK_CONF_DIR/spark-defaults.conf\" &lt;&lt;'EOF'\nspark.task.cpus 4\nspark.driver.memory 32g\nspark.executor.memory 128g\nspark.driver.extraJavaOptions -XX:+UseParallelGC -XX:ParallelGCThreads=8\nspark.executor.extraJavaOptions -XX:+UseParallelGC -XX:ParallelGCThreads=8\nEOF \n</code></pre>"},{"location":"theta/data-science-workflows/apache-spark/#tune-these-numbers-for-your-workload","title":"Tune These Numbers for Your Workload","text":"<pre><code>spark.task.cpus 4\nspark.scheduler.maxRegisteredResourcesWaitingTime 4000s\nspark.scheduler.minRegisteredResourcesRatio 1\nspark.scheduler.listenerbus.eventqueue.capacity 100000\nspark.worker.timeout 24000\nspark.executor.heartbeatInterva l4000s\nspark.files.fetchTimeout 12000s\nspark.network.timeout 24000s\nspark.locality.wait 6000s\nspark.driver.memory 16g\nspark.executor.memory 128g\nspark.driver.extraJavaOptions; -XX:+UseParallelGC -XX:ParallelGCThreads=8\nspark.executor.extraJavaOptions -XX:+UseParallelGC -XX:ParallelGCThreads=8\n</code></pre>"},{"location":"theta/data-science-workflows/apache-spark/#other-tunings","title":"Other tunings","text":"<ul> <li>Number of partitions for your RDD</li> <li>Point spark.local.dir to the local SSD</li> <li>Do not use \"Dynamic Allocation\" unless you have a strong reason</li> </ul>"},{"location":"theta/data-science-workflows/apache-spark/#scala-interactive","title":"Scala Interactive","text":"<p>Start an interactive job: <code>/soft/datascience/Spark_Job/submit-spark.sh -A datascience -t 60 -n 2 -q debug-cache-quad</code></p> <p>Launch a scala shell: <code>$SPARK_HOME/bin/spark-shell --master $SPARK_MASTER_URI</code></p> <p>In the spawned scala shell, you can enter scala statements as follows:  <pre><code>sc.getExecutorMemoryStatus\n(java.net.InetAddress.getLocalHost, Runtime.getRuntime.maxMemory)\nsc.parallelize(1 to 10).\nmap((_, java.net.InetAddress.getLocalHost, Runtime.getRuntime.maxMemory)).\ncollect \n</code></pre> Due to the scheduler's behavior and the number of cores available, you may need a much larger number (<code>1 to 10</code> above) than the number of worker nodes for the above statement to actually run on all nodes.</p>"},{"location":"theta/data-science-workflows/apache-spark/#example-submit-commands","title":"Example Submit Commands","text":"<pre><code>$/PATH/TO/SPARK_JOB/submit-spark.sh -A datascience -t 60 -n 2 -q debug-cache-quad run-example SparkPi \n</code></pre> <p>The script will submit a COBALT job using the <code>datascience</code> allocation, for a maximum wall clock time of 60 minutes, request 2 nodes, using the <code>debug-cache-quad</code> queue.  The job will run the scala example<code>SparkPi</code> came with the default install of apache spark.</p> <pre><code>$/PATH/TO/SPARK_JOB/submit-spark.sh -A datascience -t 60 -n 2 -q debug $SPARK_HOME/examples/src/main/python/pi.py 10000 \n</code></pre> <p>The script will submit a COBALT job using the <code>datascience</code> allocation, for a maximum wall clock time of 60 minutes, request 2 nodes, using the <code>debug</code> queue.  The job will run the pyspark example <code>pi.py</code> came with the default install of apache spark. <pre><code>$/PATH/TO/SPARK_JOB/submit-spark.sh -A datascience -t 60 -n 2 -q debug -s SomeExe Args \n</code></pre></p> <p>The script will submit a COBALT job using the <code>datascience</code> allocation, for a maximum wall clock time of 60 minutes, request 2 nodes, using the <code>debug</code> queue.  The job will run the executable <code>SomeExe</code> with arguments <code>Args</code> on the compute node that has the spark master running.  Spark related variables will populate the running environment.</p> <p><pre><code>$/PATH/TO/SPARK_JOB/submit-spark.sh -A datascience -t 60 -n 2 -q pubnet-debug -w 10 \n</code></pre> The script will submit a COBALT job using the <code>datascience</code> allocation, for a maximum wall clock time of 60 minutes, request 2 nodes, using the <code>debug</code> queue.  The job will drop in to a shell environment on the compute node of the spark master.</p>"},{"location":"theta/data-science-workflows/apache-spark/#bugs-and-limitations","title":"Bugs and Limitations","text":"<p>Paths or environment variables containing quotes may break the scripts. Current JVM on Theta is not aware of NUMA, and we recommend the use of cache mode (<code>--attrs=mcdram=cache</code>).</p>"},{"location":"theta/data-science-workflows/balsam/","title":"Balsam","text":"<p>Balsam is a project run by the ALCF Data Science group, to optimize workflow execution on ALCF systems (and elsewhere). With Balsam, users can easily describe a large campaign of jobs to be run, with varying sizes and interdependencies, and let Balsam manage flowing the jobs out to the target systems. Balsam will stage in necessary inputs, submit jobs to the queues, and stage out specified output files. Users can also control the flow of jobs to prioritize particular workflows or jobs. </p> <p>Balsam was originally developed to support ATLAS workflows on ALCF systems, linking a serial preprocessing stage with large-scale parallel execution on Mira, followed by serial post-processing.</p> <ul> <li>Documentation: https://balsam.readthedocs.io/en/latest</li> <li>Source repository: https://github.com/argonne-lcf/balsam</li> </ul> <p>Note: All users of the Theta Balsam module with a database created before February 17, 2021, must take action to continue using the module. See this page for details. No action is needed if you have been using your own installation of Balsam in a virtual environment.</p>"},{"location":"theta/data-science-workflows/conda/","title":"Conda on Theta","text":"<p>Conda is a popular package and virtual environment management framework that is used for managing python packages. ALCF has installed this framework, with some default package that users can use for simulation, analysis, and machine learning on Theta.</p>"},{"location":"theta/data-science-workflows/conda/#getting-started","title":"Getting Started","text":"<p>Adding Conda to your environment:</p> <p><pre><code>module load conda/2021-09-22\nconda activate\n</code></pre> This will load Python 3. The installed modules are listed at the bottom of this page. At the time of writing, this is the most recent Anaconda build available on Theta that includes an optimized <code>mpi4py</code> library linked against the Cray MPICH installed on the machine. </p> <p>If all the Python packages you need are installed, then you can use this module as is. However, if you need custom modules installed, have a look at the section below about Installing Custom Python Modules.</p>"},{"location":"theta/data-science-workflows/conda/#probing-the-environment","title":"Probing the environment","text":"<p>Full Conda documentation can be found here, but we'll cover a few useful things here. After the module is loaded, one can list the Python modules installed and their versions using <code>conda list</code></p>"},{"location":"theta/data-science-workflows/conda/#adding-custom-python-modules-via-pip","title":"Adding custom python modules via <code>pip</code>","text":"<p>You can add custom python modules via <code>pip install --user &lt;module-name&gt;</code> and this module will be installed in <code>$HOME/.local/lib/python3.X/site-packages</code> which is always part of the <code>sys.path</code> in Python. This can help if you only need a few extra packages or small additions. For big changes see the next section on custom environments.</p>"},{"location":"theta/data-science-workflows/conda/#installing-custom-conda-environment","title":"Installing Custom Conda Environment","text":"<p>In order to add custom Python modules to the conda environment, one must create a custom conda environment. This can be done using:</p> <pre><code>conda create -p &lt;/path/to/new/env&gt; --clone $CONDA_PREFIX\n</code></pre> <p>This creates a custom environment in the path you specify and installs everything that existed in the base conda installation. Be aware that the actual conda binary will always come from the base installation.</p> <p>Next you can move into this custom environment with <code>conda activate &lt;/path/to/new/env&gt;</code></p> <p>Now you can install your own packages.</p> <pre><code>conda install &lt;python-module&gt;\n</code></pre>"},{"location":"theta/data-science-workflows/conda/#references","title":"References","text":"<p>Python for HPC: Best Practices</p>"},{"location":"theta/data-science-workflows/data-science-modules/","title":"Data Science Modules","text":"<p>The ALCF Data Science group provides modules to simplify the usage of common data science tools, such as TensorFlow, PyTorch, Horovod, and mpi4py. Users can see a list of available datascience modules with <code>module avail datascience</code>. More information about each module can be found by executing <code>module show &lt;MODULENAME&gt;</code>.</p>"},{"location":"theta/data-science-workflows/data-science-modules/#condayyyy-mm-dd","title":"conda/YYYY-MM-DD","text":"<p>We periodically build modules containing shared Anaconda environments with TensorFlow, PyTorch, Horovod, and mpi4py all in for the same Python binary. E.g. <code>conda/2021-09-22</code> was built on 2021-09-22 and can be used instead of individualized <code>datascience/*</code> modules discussed below. </p>"},{"location":"theta/data-science-workflows/data-science-modules/#datasciencempi4py","title":"datascience/mpi4py","text":"<p>This module loads the environment required to run MPI for Python (mpi4py) package. The version of mpi4py is 3.0.2.</p> <p>Note: This module loads <code>intelpython36</code> and <code>gcc</code>modules. </p>"},{"location":"theta/data-science-workflows/data-science-modules/#datasciencetensorflow-x","title":"datascience/tensorflow-X","text":"<p>This module loads the environment required to run TensorFlow on Theta. Available versions span 1.13 to 2.3.</p> <p>Note: This module loads <code>intelpython36</code> and <code>gcc/8.2.0</code> (currently missing) modules. You will get a core dump if you try to use TensorFlow on the login node, since TensorFlow library was compiled to use AVX512F instructions, which are available on compute nodes.</p>"},{"location":"theta/data-science-workflows/data-science-modules/#datasciencehorovod-x","title":"datascience/horovod-X","text":"<p>This module loads the environment required to run Horovod on Theta. Horovod is a distributed deep learning framework for TensorFlow, Keras, PyTorch, and MXNet. Available versions span 0.16.1 to 0.19.0-oneccl. </p> <p>Note: This module loads <code>intelpython36</code> and <code>gcc/8.2.0</code> (currently missing) modules. However, it doesn\u2019t load TensorFlow, Keras, or PyTorch. You have to load one of these modules in order to use it together with horovod.</p>"},{"location":"theta/data-science-workflows/data-science-modules/#datasciencekeras-x","title":"datascience/keras-X","text":"<p>This module loads the environment required to run Keras, which is a high-level Python API to run TensorFlow. You must manually load a TensorFlow module in addition to a Keras module.</p> <p>Note: This module loads <code>intelpython36</code>, <code>gcc/8.2.0</code> (currently missing) modules.</p>"},{"location":"theta/data-science-workflows/data-science-modules/#datasciencepytorch-x","title":"datascience/pytorch-X","text":"<p>This module loads the environment required to run PyTorch, a deep learning platform with Python and C++ API. </p> <p>Note: This module loads <code>intelpython36</code>, <code>gcc/8.2.0</code> (currently missing) modules.</p>"},{"location":"theta/data-science-workflows/data-science-modules/#references","title":"References","text":"<ul> <li>ALCF Data Science Program Overview</li> <li>ALCF Data and Learning Frameworks</li> </ul>"},{"location":"theta/data-science-workflows/deephyper/","title":"DeepHyper","text":""},{"location":"theta/data-science-workflows/deephyper/#deephyper-scalable-hyperparameter-search-for-deep-neural-networks","title":"DeepHyper: Scalable Hyperparameter Search for Deep Neural Networks","text":"<p>Deep learning (DL) algorithms typically require user-specified values for hyperparameters, which strongly influence performance factors such as training time and prediction accuracy. </p> <p>These hyperparameters include the number of hidden layers, the number of units per layer, sparsity/overfitting regularization parameters, batch size, learning rate, type of initialization, optimizer, and activation function specification. Traditionally, in machine learning research, finding performance-optimizing hyperparameter settings has been tackled by using a trial-and-error process or by brute-force grid/random search.  However, such approaches lead to far-from-optimal performance or are otherwise impractical for addressing large numbers of hyperparameters.</p> <p>DeepHyper is a Python package that provides a set of scalable hyperparameter search methods for automatically searching for high-performing hyperparameters for a given deep neural network architecture. It adopts the Balsam workflow system to hide the complexities of running large numbers of hyperparameter configurations in parallel on high-performance computing (HPC) systems such as Theta.</p> <p>DeepHyper adopts asynchronous model based search (AMBS) that relies on fitting a dynamically updated surrogate model that tries to learn the relationship between the hyperparameter configurations (input) and their validation errors (output). Key properties of the surrogate model are that it is cheap to evaluate and can be used to prune the search space and identify promising regions. The surrogate model is iteratively refined in the promising regions of the search space by obtaining new outputs at inputs that are predicted by the model to be high performing.  In addition to AMBS, DeepHyper has a random search and a batch synchronous genetic algorithm search.</p> <p>DeepHyper is already installed on Theta and can be directly loaded as a module using the following command: <code>module load deephyper</code></p> <p>To create a new hyperparameter search problem, please see the please follow the instructions from the official DeepHyper documentation: https://deephyper.readthedocs.io/en/latest/tutorials/index.html</p>"},{"location":"theta/data-science-workflows/deephyper/#references","title":"References","text":"<p>P. Balaprakash, M. Salim, T. Uram, V. Vishwanath, and S. M. Wild. DeepHyper: Asynchronous Hyperparameter Search for Deep Neural Networks. In 25th IEEE International Conference on High Performance Computing, Data, and Analytics. IEEE, 2018.</p> <ul> <li> <p>https://deephyper.readthedocs.io/en/latest</p> </li> <li> <p>https://github.com/deephyper/deephyper</p> </li> </ul>"},{"location":"theta/data-science-workflows/distributed-learning-horovod/","title":"Distributed Learning with Horovod","text":"<p>There are two schemes for distributed learning:  1. Model parallelization: in this scheme, disjoint subsets of a neural network are assigned to different devices. Therefore, all the computations associated to the subsets are distributed. Communication happens between devices whenever there is dataflow between two subsets. Model parallelization is suitable when the model is too large to be fitted into a single device (CPU/GPU) because of the memory capacity. However, partitioning the model into different subsets is not an easy task, and there might potentially introduce load imbalance issues limiting the scaling efficiency. </p> <ol> <li>Data parallelization: in this scheme, all the workers own a replica of the model. The global batch of data is split into multiple minibatches, and processed by different workers. Each worker computes the corresponding loss and gradients with respect to the data it possesses. Before the updating of the parameters at each epoch, the loss and gradients are averaged among all the workers through a collective operation. This scheme is relatively simple to implement. MPI_Allreduce is the only communication overhead that introduced. </li> </ol> <p>On Theta, we support data parallelization through Horovod. Horovod is an open source distributed deep learning framework developed by Uber. It is based on a bandwidth-optimal ring-allreduce algorithm proposed by Baidu \"Bringing HPC Techniques to Deep Learning.\" One could refer https://eng.uber.com/horovod and https://arxiv.org/abs/1802.05799 for a more thorough discussion of the detailed implementation and benchmarks. </p>"},{"location":"theta/data-science-workflows/distributed-learning-horovod/#datascience-horovod-module","title":"Datascience Horovod Module","text":"<p>We built Horovod module in the Cray programming environment on Theta using GCC/7.3.0.  It was linked to Cray MPICH library. This module could be loaded using \"module load datascience/horovod-0.13.11\". This module could NOT run on Login node/mom node. It must be run through \"aprun -n ... -N ...\" (mpirun does not work).</p>"},{"location":"theta/data-science-workflows/distributed-learning-horovod/#how-to-use-horovod","title":"How to use Horovod","text":""},{"location":"theta/data-science-workflows/distributed-learning-horovod/#loading-the-module","title":"Loading the module","text":"<p><code>module load datascience/horovod-0.15.2</code></p>"},{"location":"theta/data-science-workflows/distributed-learning-horovod/#changing-the-codes","title":"Changing the codes","text":"<p>The followings are the list of changes that one has to make changes to a Python script in order to run it using Horovod:    - Import Horovod and initialize it: \"import horovod.PACKAGE as hvd; hvd.init()\". PACKAGE could be tensorflow, pytorch, or Keras. After this initialization, the total number of ranks and the rank id could be access through hvd.rank(), hvd.size() functions.   - Scale the learning rate in the optimizer. Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally.    - Wrap the optimizer with Distributed Optimizer \"hvd.DistributedOptimizer\"   - Broadcast the model parameters from rank 0, so that all the workers will have the same starting point.    - Loading data according to rank ID: TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader.    - Write check point files only on rank 0. </p> <p>Specific instructions on how to make changes for different packages are shown on the respective documentations: TensorFlow, PyTorch. </p>"},{"location":"theta/data-science-workflows/distributed-learning-horovod/#profiling-with-horovod-timeline","title":"Profiling with Horovod timeline","text":"<p>Horovod has the ability to record the timeline of its activity, called Horovod Timeline. To record a Horovod Timeline, set the HOROVOD_TIMELINE environment variable to the location of the timeline file to be created. This file is only recorded on rank 0, but it contains information about activity of all workers. <pre><code>aprun -n ... -N ... -e HOROVOD_TIMELINE=/path/to/timeline.json python train.py \n</code></pre> One can then open the timeline file using <code>chrome://tracing</code> facility of the Chrome browser. Users can use Horovod Timelines to view exactly what each node was doing at each time step throughout a training job. This helps identify bugs and debug performance issues. </p> <p>We put examples of MNIST training using TensorFlow/PyTorch. The folders contain the python scripts, datasets and COBALT submission script. </p> <ul> <li>TensorFlow + Horovod: <code>/projects/SDL_Workshop/hzheng/examples/tensorflow/MNIST</code></li> <li>PyTorch + Horovod: <code>/projects/SDL_Workshop/hzheng/examples/pytorch/MNIST</code></li> </ul>"},{"location":"theta/data-science-workflows/hdf5/","title":"HDF5","text":"<p>HDF5 (Hierarchical Data Format 5) is a high-performance I/O technology suite, designed for complex data sets.  The HDF5 technology suite is composed of a data model, a file format, an API, a library, and a set of software tools.</p>"},{"location":"theta/data-science-workflows/hdf5/#getting-started","title":"Getting Started","text":"<p>Cray-optimized HDF5 libraries and software tools can be added to your environment by loading the cray-hdf5 or cray-hdf5-parallel modules. </p> <p>For Example: <pre><code>module load cray-hdf5\n</code></pre> Or, for parallel HDF5 support: <pre><code>module load cray-hdf5-parallel\n</code></pre> These commands will load the default hdf5 module (currently 1.10.1.1).  A list of all available HDF5 modules can be viewed by typing: <pre><code>module avail -t 2&gt;&amp;1 | grep -i cray-hdf5\n</code></pre> After the desired module is loaded, the location of the HDF5 library will be stored in the CRAY_HDF5_DIR environment variable.</p>"},{"location":"theta/data-science-workflows/hdf5/#using-hdf5-in-python","title":"Using HDF5 in Python","text":"<p>H5py is popular python interface to the HDF5 binary data format.  Currently, the  miniconda-3.6/conda-4.4.10 module provides parallel h5py support (while miniconda-2.7/conda-4.4.10 only provides serial support). </p> <p>To perform a custom pip-installation of parallel h5py in a local user directory, the following command can be used: <pre><code>HDF5_MPI=\"ON\" HDF5_DIR=$CRAY_HDF5_DIR pip install --user --no-binary=h5py h5py\n</code></pre> This command requires that the cray-hdf5-parallel module is first loaded, because parallel h5py will need an existing HDF5 library to build against.</p> <p>Warning: Users should always CLONE the module-provided miniconda environment before making changes.</p>"},{"location":"theta/data-science-workflows/hdf5/#other-io-libraries","title":"Other I/O Libraries","text":"<p>The libraries activated by the cray-hdf5-parallel modules leverage MPI-IO for parallel file-system access. Therefore, parallel performance is limited that of the active cray-mpich library (currently cray-mpich/7.7.0 by default).</p> <p>In addition to HDF5 and MPI-IO, netCDF and PnetCDF modules are also available on Theta.  Since PnetCDF leverages MPI-IO for parallelism, it's parallel performance is also limited to that of MPI-IO.</p>"},{"location":"theta/data-science-workflows/keras/","title":"Keras","text":""},{"location":"theta/data-science-workflows/keras/#introduction","title":"Introduction","text":"<p>Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. It allows for easy and fast prototyping, and support both convolutional networks and recurrent networks and the combination of the two. It runs seamlessly on CPU and GPU.</p>"},{"location":"theta/data-science-workflows/keras/#datascience-module","title":"Datascience module","text":"<p>On Theta, we support TensorFlow backend for Keras. To use the datascience Keras module on Theta, please load the following two modules: <pre><code>module load datascience/keras-2.2.4\n\nmodule load datascience/tensorflow-1.12\n</code></pre></p> <p>Notice that the <code>datascience/tensorflow-*</code> modules were compiled with AVX512 extension on Theta. Therefore, it could not run on login node, otherwise it will issue an \"illegal instruction\" fault. One has to submit the job to KNL nodes (see TensorFlow documentation for details).</p> <p>Since we use TensorFlow as the backend, all the optimal environmental setups (Threading + affinity) are applicable here. Please visit the TensorFlow documentation page for the optimal setting.</p> <p>We do not see any incompatibility issues in using different versions of keras and tensorflow as those specified above. Feel free to change other versions of Keras or TensorFlow. Currently, we support version 2.2.2 and 2.2.4.</p>"},{"location":"theta/data-science-workflows/keras/#distributed-learning-using-horovod","title":"Distributed learning using Horovod","text":"<p>We support distributed learning using Horovod. To use it please load datascience/horovod-0.15.2 module. Please change your python script accordingly</p>"},{"location":"theta/data-science-workflows/keras/#initialize-horovod-by-adding-the-following-lines-to-the-beginning-of-your-python-script","title":"Initialize Horovod by adding the following lines to the beginning of your Python script.","text":"<p><pre><code>import horovod.keras as hvd\n\nhvd.init()\n</code></pre> After this initialization, the total number of ranks and the rank id could be access through hvd.rank(), hvd.size() functions.</p>"},{"location":"theta/data-science-workflows/keras/#scale-the-learning-rate","title":"Scale the learning rate.","text":"<p>Typically, since we use multiple workers, the global batch is usually increased n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01).</p> <p><pre><code>opt = keras.optimizers.Adadelta(1.0 * hvd.size()\n</code></pre> In some case, <code>0.01*hvd.size()</code> might be too large, one might want to have some warming up steps with smaller learning rate.</p>"},{"location":"theta/data-science-workflows/keras/#wrap-the-optimizer-with-distributed-optimizer","title":"Wrap the optimizer with Distributed Optimizer","text":"<p><pre><code>opt = hvd.DistributedOptimizer(opt)\n</code></pre> In such case, opt will automatically average the loss and gradients among all the workers and then perform update.</p>"},{"location":"theta/data-science-workflows/keras/#broadcast-the-model-from-rank-0-so-that-all-the-workers-will-have-the-same-starting-point","title":"Broadcast the model from rank 0, so that all the workers will have the same starting point","text":"<p><pre><code>callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0)]\n</code></pre> Notice that by default, TensorFlow will initialize the parameters randomly. Therefore, by default, different workers will have different parameters. So it is crucial to broadcast the model from rank 0 to other ranks.</p>"},{"location":"theta/data-science-workflows/keras/#letting-only-rank-0-to-write-checkpoint","title":"Letting only rank 0 to write checkpoint","text":"<p><pre><code>if hvd.rank() == 0:\n</code></pre> callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))</p>"},{"location":"theta/data-science-workflows/keras/#loading-data-according-to-rank-id","title":"Loading data according to rank ID","text":"<p>Since we are using data parallel scheme. Different ranks shall process different data. One has to change the data loader part of the python script to ensure different ranks read different mini batches of data.</p>"},{"location":"theta/data-science-workflows/keras/#example","title":"Example","text":"<p>A simple example for doing linear regression using Keras + Horovod is put in the follwoing directory on Theta: /projects/SDL_Workshop/hzheng/examples/keras/linreg</p> <p>linreg_keras.py is the python script, and qsub.sc is the COBALT submission script.</p>"},{"location":"theta/data-science-workflows/machine-learning-tools/","title":"Machine Learning Tools","text":""},{"location":"theta/data-science-workflows/machine-learning-tools/#machine-learning-tools-on-theta","title":"Machine Learning Tools on Theta","text":"<p>The ALCF is working to support scalable machine learning on Theta. Our focus has been supporting TensorFlow on Theta as it has broad support from Cray &amp; Intel giving large performance boosts on the Intel KNL architecture versus other frameworks. </p> <p>We support two installations of TensorFlow, one via the Conda environment and one via a custom Cray plugin environment. We also provide easy to use datascience modules for mpi4py, TensorFlow, Keras, PyTorch, and Horovod.</p>"},{"location":"theta/data-science-workflows/machine-learning-tools/#generic-environment-settings","title":"Generic Environment Settings","text":"<p>First we'll mention some generic settings to play with while doing TensorFlow training on Theta.</p> <p>The Google TensorFlow guide describes these variables here: https://www.tensorflow.org/performance/performance_guide</p> <p>In your batch submit script use the following: - <code>export OMP_NUM_THREADS=62</code> This setting should be set to the number of physical cores, although, our local Cray expert suggested using 62. - <code>export KMP_BLOCKTIME=0</code> Sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping. - <code>export KMP_AFFINITY=\"granularity=fine,compact,1,0\"</code>  Enables the run-time library to bind threads to physical processing units.</p> <p>In addition, TensorFlow has the following internal settings that should be used to optimize performance: - intra_op_parallelism_threads: Setting this equal to the number of physical cores is recommended. Setting the value to 0, which is the default and will result in the value being set to the number of logical cores, is an option to try for some architectures. This value and OMP_NUM_THREADS should be equal. - inter_op_parallelism_threads: Setting this equal to the number of sockets is recommended. Setting the value to 0, which is the default, results in the value being set to the number of logical cores.</p> <p>This can be added to your model using this code: <pre><code>config = tf.ConfigProto() \nconfig.allow_soft_placement = True \nconfig.intra_op_parallelism_threads = FLAGS.num_intra_threads \nconfig.inter_op_parallelism_threads = FLAGS.num_inter_threads \ntf.session(config=config)\n</code></pre></p>"},{"location":"theta/data-science-workflows/machine-learning-tools/#tensorflow-via-conda-environment","title":"TensorFlow via Conda Environment","text":"<p>We've installed a Conda environment which has the latest TensorFlow Wheel from Intel installed. We've also installed the Horovod tool which uses MPI to run Tensorflow in a distributed way. This enables the training of Tensorflow models on Theta at large scales. Horovod provides examples for running Tensorflow natively or Tensorflow using Keras. This can be run using: <pre><code>#!/bin/bash\n#COBALT -n &lt;num-nodes&gt;\n#COBALT -t &lt;wall-time&gt;\n#COBALT -q &lt;queue&gt;\n#COBALT -A &lt;project&gt;\n\nmodule load miniconda-3.6/conda-4.5.12\n\naprun -n &lt;num-ranks&gt; -N &lt;mpi-ranks-per-node&gt; python script.py\n</code></pre></p>"},{"location":"theta/data-science-workflows/machine-learning-tools/#tensorflow-via-cray-ml-plugin","title":"TensorFlow via Cray ML Plugin","text":"<p>Cray has provided a custom ML Plugin for running TensorFlow on Theta that provides performance benefits when using smaller local mini-batch sizes. </p> <p>There are two example batch scripts for Python 2.7 or 3.6 which show how to setup the local environment: <pre><code>/lus/theta-fs0/projects/SDL_Workshop/mendygra/cpe_plugin_py2.batch\n/lus/theta-fs0/projects/SDL_Workshop/mendygra/cpe_plugin_py3.batch\n</code></pre> This is the environment setup for python 2.7: <pre><code>module load cray-python\nexport PYTHONUSERBASE=/lus/theta-fs0/projects/SDL_Workshop/mendygra/pylibs\nmodule load /lus/theta-fs0/projects/SDL_Workshop/mendygra/tmp_inst/modulefiles/craype-ml-plugin-py2/1.1.0\n</code></pre> and for python 3.6: <pre><code>module load cray-python/3.6.1.1\nexport PYTHONUSERBASE=/lus/theta-fs0/projects/SDL_Workshop/mendygra/pylibs\nmodule load /lus/theta-fs0/projects/SDL_Workshop/mendygra/tmp_inst/modulefiles/craype-ml-plugin-py3/1.1.0\n</code></pre> After setting up one of these environments, you can see an example of implementing the plugin in this script <pre><code>less $CRAYPE_ML_PLUGIN_BASEDIR/examples/tf_mnist/mnist.py\n</code></pre></p>"},{"location":"theta/data-science-workflows/machine-learning-tools/#references","title":"References","text":""},{"location":"theta/data-science-workflows/mongo-db/","title":"MongoDB","text":"<p>MongoDB is a NoSQL database which stores information as binary JSON's (BSONS). All of the details mentioned in this document has been tested against MongoDB version 3.4. Kindly consult the official online manual in case you would like to port another version of the MongoDB.In this section, we will see how to setup MongoDB on ALCF machines (i.e. Cooley and Theta). The central aspect of running MongoDB involves the definition of the server and the client. In the following sections, we will see how to: - Launch a Mongo server instance on the login node. - Establish the connection between the client and server.</p>"},{"location":"theta/data-science-workflows/mongo-db/#mongodb-at-alcf-cooley","title":"MongoDB at ALCF Cooley","text":"<p>As a first step, we must define a configuration ('.conf') file. An example script is shown below. <pre><code>processManagement:\nfork: true\nstorage:\ndbPath: /path/mongodb/db\nsystemLog:\ndestination: file\npath: \"/path/log/mongodb.log\"\nlogAppend: true\nnet:\nbindIp: 172.23.100.210\nport:27017  #--&gt; Has to be modified by the user!\n</code></pre></p> <p>Please save the script to mongodb.cooley.conf. Adapt the dbpath and path to your local path. The port number of 27017 is the default for MongoDB. Each server instance of MongoDB should have a unique port number and this should be changed to a sensible number (e.g.54392, 2459 etc.). Hence it is highly recommended to set port number by a random number generator in order to avoid conflict with other users.</p> <p>Note: Make sure the bindIpis set correctly by running the command \u2018ifconfig -v\u2019  and see if it matches with the ib0 IP.</p> <pre><code>$/path/bin/mongod -f /path/mongodb.cooley.conf;#Launches the server\n$/path/bin/mongod -f /path/mongodb.cooley.conf --shutdown #To shutdown the server\n</code></pre> <p>Once the server is launched, the user can connect to it. The default way to connect to the database server would be the Mongo shell as shown below.</p> <p>$/path/bin/mongo --host 172.23.100.210  --port 27017 # Must match with the  bindIP and port in the conf file!</p> <p>The connection request to the database server can come any number of methods such as a Jupyter notebook, or a standalone Python script or it can even come from a job running on the worker nodes. A sample Python script to connect to the server, that utilizes PyMongo driver version 3.4.0 is shown below. For more details, please refer to the official documentations of MongoDB.</p> <pre><code>from pymongo import MongoClient\nimport pymongo\nprint(pymongo.__version__) # To make sure your MongoDB version is consistent with the driver!\nclient = MongoClient('mongodb://172.23.100.210:27017')  # Must match with the  bindIP and port in the conf file!\ndb = client.Myproject       # Creates a new database with the name 'Myproject' \ndb.MyCollection.create_index([('Myindex', pymongo.ASCENDING)], unique=True)  # Creates a new collection with a unique index. \nsorted(list(db.MyCollection.index_information()))\n</code></pre>"},{"location":"theta/data-science-workflows/mongo-db/#mongodb-at-alcf-theta","title":"MongoDB at ALCF Theta","text":"<p>All the steps explained in the previous step are applicable to Theta as well, expect the configuration file.</p> <p>processManagement: <pre><code>fork: true\nstorage:\ndbPath: /path/mongodb/db\nsystemLog:\ndestination: file\npath: \"/path/log/mongodb.log\"\nlogAppend: true\nnet:\nbindIp: 10.236.1.194\nport: 27017  #--&gt; Has to be modified by the user!\n</code></pre></p> <p>Please save the script to mongodb.theta.conf. Adapt the dbpath and path to your local path. The port number of 27017 is the default for MongoDB. Each server instance of MongoDB should have a unique port number, and this should be changed to a sensible number (e.g.54392, 2459 etc.). Hence it is highly recommended to set port number by a random number generator in order to avoid conflict with other users. </p> <p>Note: Make sure the bindIp is set correctly by running the command ip a and see if it matches with the bindIP in the conf file <pre><code>$/path/bin/mongod -f /path/mongodb.theta.conf   #Launches the server\n\n$/path/bin/mongod -f /path/mongodb.theta.conf --shutdown  #To shutdown the server\n</code></pre></p> <p>Once the server is launched, the user can connect to it. Similar to the previous section, the connection to mongo shell can be achieved as shown below: $/path/bin/mongo --host 10.236.1.194 --port 27017 # Must match with the bindIP and port in the conf file!</p> <p>For more information please look at https://docs.mongodb.com/v3.4/introduction</p>"},{"location":"theta/data-science-workflows/postgre-sql-lite/","title":"PostgreSQL and SQLite","text":""},{"location":"theta/data-science-workflows/postgre-sql-lite/#theta","title":"Theta","text":"<p>The module <code>postgresql/9.6.12</code> is available on Theta.  Use <code>module load postgresql</code> to add the Postgres 9.6 binaries to your search <code>PATH</code>.</p>"},{"location":"theta/data-science-workflows/postgre-sql-lite/#cooley","title":"Cooley","text":"<p>On Cooley, the Postgres 9.6 binaries can be added to your search <code>PATH</code> via: <code>source /soft/datascience/balsam/postgres-envs.sh</code></p>"},{"location":"theta/data-science-workflows/postgre-sql-lite/#downloading-binaries","title":"Downloading binaries","text":"<p>To install your own Postgres binaries visit enterpriseDB.com.</p>"},{"location":"theta/data-science-workflows/postgre-sql-lite/#creating-a-new-database-cluster","title":"Creating a New Database Cluster","text":"<p>To create a new database cluster, use: <code>initdb -D &lt;Database Path&gt; -U $USER</code></p> <p>Inside the database directory, you will need to edit the file <code>postgresql.conf</code> as follows. Set the options below: <pre><code>listen_addresses = '*'\nport = 12345 # choose a random high-numbered port that isn't yet bound on the host\n</code></pre></p> <p>To allow connections on all interfaces without requiring authentication, you will need to edit the Host-Based Authentication rules as follows.  Edit the file <code>pg_hba.conf</code> by appending the following line: <pre><code>host all all 0.0.0.0/0 trust\n</code></pre></p> <p>The above instructions assume you are operating in a secure environment and are willing to blindly accept connections from any host in the network. This a convenient but insecure option.</p> <p>If you want to secure connections to your database, refer to the documentation on host-based authentication and consider using <code>md5</code> rather than <code>trust</code> based authentication.</p>"},{"location":"theta/data-science-workflows/postgre-sql-lite/#starting-and-stopping-the-database-cluster","title":"Starting and Stopping the Database Cluster","text":"<p>You can start the database located at path <code>./foo</code> with with: <pre><code>pg_ctl -D ./foo start\n</code></pre></p> <p>Stop the database with: <pre><code>pg_ctl -D ./foo stop\n</code></pre> or send a <code>SIGTERM</code> to the leader <code>postgres</code> process, which will shut down gracefully.</p>"},{"location":"theta/data-science-workflows/postgre-sql-lite/#connecting-to-a-database","title":"Connecting to a database","text":"<p>Assuming the database is running on port <code>12345</code> of host <code>thetalogin1</code>, you can start a psql session in the terminal as follows: <pre><code>psql -p 12345 -h thetalogin1 -d postgres\n</code></pre> The <code>-h</code> argument defaults to localhost as is therefore not necessary if you are connecting from the same machine that's currently hosting the database cluster.</p> <p>If using a Python ORM like Django, you will need to configure the <code>DATABASES</code> option in your <code>settings.py</code> to point at the same host and port, according to the documentation.</p> <p>Note: that we connected to the default superuser database called <code>postgres</code>. You will want to create a new database under your cluster as follows: <pre><code>createdb &lt;Database Name&gt; -h &lt;Host&gt; -p &lt;Port&gt;\n</code></pre></p> <p>For example, let's create a database called ToDoList and then connect to it: <pre><code>createdb ToDoList -h thetalogin1 -p 12345\npsql -p 12345 -h thetalogin1 -d ToDoList\n</code></pre></p>"},{"location":"theta/data-science-workflows/postgre-sql-lite/#creating-tables","title":"Creating tables","text":"<p>To get started with SQL follow the Postgres tutorial.</p> <p>A minimal example looks like: <pre><code>CREATE TABLE todo_list (\n   task varchar(200),\n   date date,\n   priority int\n);\n\nINSERT INTO todo_list VALUES ('Learn SQL', '2019-10-30', 4);\n\nSELECT * FROM todo_list;\n</code></pre></p>"},{"location":"theta/data-science-workflows/postgre-sql-lite/#sqlite","title":"SQLite","text":"<pre><code>import sqlite3\nconn = sqlite3.connect('example.db')\nc = conn.cursor()\n</code></pre>"},{"location":"theta/data-science-workflows/pytorch/","title":"PyTorch","text":"<p>PyTorch is an open source python-based library built to provide flexibility as a deep learning development platform. The workflow of PyTorch is as close as you can get to python\u2019s scientific computing library \u2013 numpy. For detailed instruction of PyTorch package, please visit https://pytorch.org. </p>"},{"location":"theta/data-science-workflows/pytorch/#datascience-pytorch-module","title":"Datascience PyTorch Module","text":"<p>The datascience PyTorch module was built with GCC/7.3.0 and Intel Python 3.5. It was built with KNL specific optimizations (e.g., AVX512 instruction), and is linked to high performance math libraries, such as MKL, MKL-DNN (home built with AVX512). Other dependent libraries, such as NumPy, SciPy, are also built with AVX512. Because the PyTorch package is built with AVX512, it only runs on KNL nodes on Theta. Since the login node, and mom nodes are not KNL, one has to run it on compute node through \"aprun ... python script.py\".</p>"},{"location":"theta/data-science-workflows/pytorch/#running-pytorch-on-theta","title":"Running PyTorch on Theta","text":""},{"location":"theta/data-science-workflows/pytorch/#loading-modules","title":"Loading modules","text":"<pre><code>module load datascience/pytorch-0.5.0-mkldnn (change to other version number)\nmodule load datascience/horovod-0.13.11\n</code></pre> <p>What this will do is essentially prepend PyTorch related path to your PYTHONPATH and PATH. </p> <ul> <li>Hyper-threading: on Theta, on could have 1, 2, or 4 threads per core (this corresponds to the -j option in aprun command). -j 2 is suggested for deep learning applications since it involved a lot of matrix multiplication kernels. </li> <li>OMP_NUM_THREADS:  The number of threads could be set in aprun command, for example: \"<code>aprun -n ... -N ... -e OMP_NUM_THRADS=4\"</code>sets 4 threads per MPI rank.</li> <li>OMP affinity settings: the user can specify the environmental variable KMP_AFFINITY to change the thread affinity. We suggest to use \"-cc depth\"in the aprun command, which corresponds to the following setting:  \"KMP_AFFINITY=granularity=fine,verbose,compact,1,0!\". The other option the user could try is \"-cc none\". We have found that \"-cc depth\" gives best performance for most cases.</li> <li>Submitting jobs (sample scripts in /soft/datascience/): Below is a typical submission script on Theta (sub.sc)</li> </ul> <pre><code>#!/bin/sh\n#COBALT -n 128 -t 1:00:00\n#COBALT -q default --attrs mcdram=cache:numa=quad\n#COBALT -A YOUR_PROJECT_NAME\nmodule load datascience/pytorch-0.5.0-mkldnn datascience/horovod-0.13.11\nNPROC_PER_NODE=4\nNPROC=$((NPROC_PER_NODE*COBALT_JOBSIZE))\naprun -n $NPROC -N $NPROC_PER_NODE -e KMP_BLOCKTIME=0 -j 2 -e OMP_NUM_THREADS=32 -cc depth -d 32 python PYTHON_SCRIPT ARG1 ARG2 ...\n</code></pre>"},{"location":"theta/data-science-workflows/pytorch/#data-parallelization-through-horovod","title":"Data parallelization through Horovod","text":"<p>PyTorch has its own distributed communication package -- torch.distributed, which provides an MPI-like interface for exchanging tensor data across multi-machine network, including send/recv, reduce/all_reduce, gather/all_gather, scatter, barrier, etc. The PyTorch on Theta, however, does not have this MPI support yet. We instead, provide Horovod package or distributing training through a data parallelization framework. It is easy to change your serial code to run data parallelization through Horovod. The followings are the procedures: </p> <ol> <li>Initialize Horovod</li> </ol> <pre><code>import horovod.torch as hvd\nhvd.init()\n</code></pre> <p>After this initialization, the total number of ranks and the rank id could be access through hvd.rank(), hvd.size() functions.</p> <ol> <li>Scale the learning rate in the optimizer function. Typically, when we use multiple workers, the global batch increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the initial learning rate is 0.01).</li> </ol> <pre><code>optimizer = optim.SGD(model.parameters(), lr=0.01*hvd.size(), momentum = args.momentum)\n</code></pre> <p>In some case, 0.01*hvd.size() might be too large, so one might want to have some warming up steps with a smaller learning rate.</p> <ol> <li>Wrap the optimizer with Distributed Optimizer</li> </ol> <pre><code>optimizer = hvd.DistributedOptimizer(optimizer, ...)\n</code></pre> <p>In such case, \"optimizer\" will automatically average the loss and gradients among all the workers and then perform update.</p> <ol> <li>Broadcast the model from rank 0, so that all the workers will have the same starting point.</li> </ol> <pre><code>hvd.broadcast_parameters(model.stat_dict(), root_rank=0)\n</code></pre> <ol> <li>Loading data according to rank ID: Torch has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader.</li> </ol>"},{"location":"theta/data-science-workflows/pytorch/#profilingpost-processing-with-tensorboardx","title":"Profiling/post-processing with TensorBoardX","text":"<p>TensorBoardX is for creating events in PyTorch, which can be process by Tensorboard. One could check tensorboardX for details on how to use it. Tensorboard events, including scalar, image, figure, histogram, audio, text, graph, onnx_graph, embedding, pr_curve and video summaries, could be created with a simple function call \"writer.add_XXX(...)\" as follows: </p> <pre><code>from tensorboardX \nimport SummaryWriter\nwriter = SummaryWriter() \nwriter.add_scale(...) \nwriter.add_audio(...) \nwriter.add_text(...) \nwriter.export_scalars_to_json(\"output.json\")\nwriter.close()\n</code></pre> <p>The created log files could then be read with \"tensorboard --logdir runs\". The data could be visualized on your local machine through ssh tunneling (see TensorBoard usage in TensorFlow).</p>"},{"location":"theta/data-science-workflows/pytorch/#examples-on-theta-mnist-imagenet-benchmarks","title":"Examples on Theta (MNIST, imagenet benchmarks)","text":"<p>Please check the test example on Theta:</p> <pre><code>/projects/SDL_Workshop/hzheng/examples/pytorch/MNIST\n</code></pre>"},{"location":"theta/data-science-workflows/pytorch/#faq-and-common-issues","title":"FAQ and common issues","text":"<p>Illegal instruction\" or \"AVX512F\" error This happens when you are trying to run AVX512 compiled applications on login nodes or mom nodes. Run use aprun on KNL nodes through qsub instead. If you hit this error while you are building python package, try to use \"aprun -n 1 -cc node python setup.py build ...\"</p> <p>Cannot download dataset When the job is submitted, it is submitted to KNL nodes which are not connected to outside internet. Therefore, it is suggested that the users download the datasets on login node (e.g., through wget), or transfer the data through scp or Globus. </p>"},{"location":"theta/data-science-workflows/pytorch/#references","title":"References:","text":"<ul> <li>PyTorch Website</li> <li>PyTorch on GitHub</li> <li>An introduction to PyTorch</li> <li>Introduction to PyTorch</li> </ul>"},{"location":"theta/data-science-workflows/tensorflow/","title":"TensorFlow","text":"<p>TensorFlow is an open source software library for machine learning and deep learning. It is one of the widely used frameworks. It supports a large variety of state-of-the-art neural network layers, activation functions, optimizers and tools for analyzing, profiling and debugging deep neural networks.</p> <p>One of the key features of TensorFlow is the dataflow graph representation of the computation. The operations (matmul, conv2D, ReLu ...) are the nodes in the graph; the tensors are the edges of the graph. The computation in TensorFlow is asynchronous. Different operations could execute in parallel and out of order. Through inter &amp; intra thread setting, one could specify how the operations execute in the hardware. TensorFlow support high level API, such as Keras. </p> <p>To learn more about TensorFlow, check TensorFlow tutorials.  </p>"},{"location":"theta/data-science-workflows/tensorflow/#datascience-tensorflow-module","title":"Datascience TensorFlow module","text":"<p>The datascience module on Theta contains: TensorFlow, PyTorch, Horovod, MPI4Py. The modules are built with GCC/7.3.0 and Intel Python 3.5 (intelpython35 module on Theta). All these packages were built with KNL specific optimizations for example (AVX512 instruction). They are linked to high performance math libraries, such as MKL, MKL-DNN (home built with AVX512). The dependent libraries, such as NumPy, SciPy are also built with AVX512.Because the TensorFlow package is built with AVX512. It only runs on KNL nodes on Theta. Since the login node, and mom nodes are not KNL, one has to create a script, and run it through \"aprun ... python script.py\".  </p> <p>For TensorFlow, we currently support 1.4, 1.6, 1.10, 1.12 versions. We are always keeping it updated to the most recent version on TensorFlow GitHub. If for some reason, your application depends on certain version this is not on Theta, please email datascience@alcf.anl.gov or support@alcf.anl.gov</p>"},{"location":"theta/data-science-workflows/tensorflow/#running-tensorflow-on-theta","title":"Running TensorFlow on Theta","text":""},{"location":"theta/data-science-workflows/tensorflow/#loading-modules","title":"Loading modules","text":"<p>To use the datascience modules, use \"module load\"</p> <pre><code>module load datascience/tensorflow-1.12\nmodule load datascience/horovod-0.15.2\n</code></pre> <p>What this will do is essentially to prepend some paths to your PYTHONPATH and PATH. </p>"},{"location":"theta/data-science-workflows/tensorflow/#threading-setup-affinity-settings","title":"Threading setup+ affinity settings","text":"<ul> <li>Hyper-threading: on Theta, on could have 1, 2, or 4 threads per core (this corresponds to the -j option in aprun command). -j 2 is suggested for deep learning applications since it involved a lot of matrix multiplication kernels. </li> <li>The affinity settings: the user could specify the environmental variable KMP_AFFINITY to change the thread affinity. We suggest to use \"-cc depth\" in the aprun command, which corresponds to the following setting:  \"KMP_AFFINITY=granularity=fine,verbose,compact,1,0!\". The other option the user could try is \"-cc none\". We have found that \"-cc depth\" gives best performance for most of the cases. </li> <li>Environmental variables setup (KMP_BLOCKTIME): KMP_BLOCKTIME is the time that a thread should wait, after completing the execution of a parallel region, before sleeping. MKL default is 200ms, which is not optimal. According to our benchmark, KMP_BLOCKTIME=0. To set this variable, use -e KMP_BLOCKTIME=0 in aprun command. </li> <li>Inter &amp; intra threads: In TensorFlow, two parameters are used to control the treading setup:<ol> <li>intra_op_parallelism_threads: this is the total number of threads available for compute operations. Typically this is set to be the same as OMP_NUM_THREADS;</li> <li>inter_op_parallelism_threads: this is the number of thread teams that could execute the TensorFlow operations concurrently. For example, if inter_op_parallelism_threads = 2, the thread pools will be divided into two teams to execute the two operations in parallel, provided that they are independent of each other. </li> </ol> </li> </ul> <p>According to our benchmark studies (AlexNet, ResNet50, Inception3), inter_op_parallism_threads=1, or 2 gives best performance. </p> <p>The following is the way to set the inter &amp; intra threads in the Python script. </p> <pre><code>config = tf.ConfigProto()  \nconfig.intra_op_parallelism_threads = int(os.environ['OMP_NUM_THREADS']) \nconfig.inter_op_parallelism_threads = 2 tf.Session(config=config)\n</code></pre>"},{"location":"theta/data-science-workflows/tensorflow/#submitting-jobs-on-theta-aprun","title":"Submitting jobs on Theta (aprun)","text":"<p>Below is a typical submission script on Theta (sub.sc)</p> <pre><code>#!/bin/sh\n#COBALT -n 128 -t 1:00:00\n#COBALT -q default --attrs mcdram=cache:numa=quad\n#COBALT -A YOUR_PROJECT_NAME\nmodule load datascience/tensorflow-1.12 datascience/horovod-0.15.2\nNPROC_PER_NODE=4\nNPROC=$((NPROC_PER_NODE*COBALT_JOBSIZE))\naprun -n $NPROC -N $NPROC_PER_NODE -e KMP_BLOCKTIME=0 -j 2 -e OMP_NUM_THREADS=32 -cc depth -d 32 python PYTHON_SCRIPT ARG1 ARG2 ..\n</code></pre>"},{"location":"theta/data-science-workflows/tensorflow/#distributed-learning-through-data-parallelization","title":"Distributed learning through data parallelization","text":"<p>TensorFlow has its own way of distributing learning through MPI. However, we suggest users to use Horovod or Cray PE ML plugins instead. These two packages help to distribute the learning through data parallelization. In the data parallelization scheme, one creates multiple workers (one worker per MPI rank). Each worker has a complete copy of the network model. The global batch is split into multiple mini-batches, and each worker processes its own mini-batches and compute the loss and gradients. The loss and gradients are then averaged among all the workers through MPI_AllReduce before updating the parameters in next epoch. </p> <p>The followings are instructions on how to change your code so that it could run through Horovod. </p>"},{"location":"theta/data-science-workflows/tensorflow/#horovod","title":"Horovod","text":"<p>Horovod was developed by Uber for distributing ML/DL. To use Horovod, there are essentially four things you need to do: </p> <p>1. Initialize Horovod </p> <pre><code>import horovod.tensorflow as hvd hvd.init()\n</code></pre> <p>After this initialization, the total number of ranks and the rank id could be access through hvd.rank(), hvd.size() functions. </p> <p>2. Scale the learning rate  Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01).</p> <pre><code>opt = tf.train.AdagradOptimizer(0.01*hvd.size())\n</code></pre> <p>In some case, 0.01*hvd.size() might be too large, one might want to have some warming up steps with smaller learning rate. </p> <p>3. Wrap the optimizer with Distributed Optimizer</p> <pre><code>opt = hvd.DistributedOptimizer(opt)\n</code></pre> <p>In such case, opt will automatically average the loss and gradients among all the workers and then perform update. </p> <p>4. Broadcast the model from rank 0, so that all the workers will have the same starting point.</p> <pre><code>hooks = [hvd.BroadcastGlobalVariablesHook(0)]\n</code></pre> <p>Notice that by default, TensorFlow will initialize the parameters randomly. Therefore, by default, different workers will have different parameters. So it is crucial to broadcast the model from rank 0 to other ranks. </p> <p>5. Loading data according to rank ID TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader.  - Cray ML plugin: Currently, the most recent version of Cray ML plugin is compatible with datascience/tensorflow-1.10. Please refer to the presentation by Cray in our SDL workshop.</p>"},{"location":"theta/data-science-workflows/tensorflow/#examples-mnist","title":"Examples (MNIST)","text":"<ul> <li>TensorFlow + Horovod</li> </ul> <p>A simple example is put in /projects/SDL_Workshop/hzheng/examples/tensorflow/MNIST</p> <p>tensorflow_mnist.py is the python script, and qsub.sc is the COBALT submission script. </p> <ul> <li>TensorFlow + Cray ML plugin</li> </ul> <pre><code>module load /projects/datascience/kristyn/modulefiles/craype-ml-plugin-py3/1.1.2 \nmodule load datascience/tensorflow-1.10\naprun -n 4 -N 1 -cc depth -b python $CRAYPE_ML_PLUGIN_BASEDIR/examples/tf_mnist/mnist.py --enable_ml_comm --data_dir=/lus/theta-fs0/projects/SDL_Workshop/mendygra/mnist_data --model_dir=[train dir]\n</code></pre>"},{"location":"theta/data-science-workflows/tensorflow/#advanced-topics","title":"Advanced topics","text":""},{"location":"theta/data-science-workflows/tensorflow/#installing-other-python-packages","title":"Installing other python packages","text":"<p>If you applications require other python packages, we suggest you do the following: </p> <p>Load the same environment:</p> <pre><code>module load datascience/tensorflow-1.10 gcc/7.3.0\n</code></pre> <p>Install the package, this could be done through </p> <p><code>module load datascience/tensorflow-1.10 gcc/7.3.0 pip install package_name --target=/path_to_install export PYTHONPATH=$PYTHONPATH:/path_to_install/ <pre><code>Build the package by your own. If you package is not available through pip install, you could built the package: \n</code></pre> module load intelpython35 gcc/7.3.0 datascience/tensorflow-1.10 python setup.py build  export PYTHONPATH= $PYTHONPATH:/path_to_install/lib/python3.5/site-packages   python setup.py install --prefix=/path_to_install/ <pre><code>**TensorFlow &amp; Horovod timeline**\nTensorFlow has its built in functionality, timeline tracing,  for profiling the code and understand which kernels are taking majority of the runtime. This could be done as follows: \n\n**Instrument training code to generate \"timelines\"**\n</code></pre> from tensorflow.python.client import timeline options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) run_metadata = tf.RunMetadata() sess.run(res, options=options, run_metadata=run_metadata) <pre><code>**Create the Timeline object, and write it to a json file!**\n</code></pre> fetched_timeline = timeline.Timeline(run_metadata.step_stats)!chrome_trace = fetched_timeline.generate_chrome_trace_format()!f=open('timeline_01.json', \u2019w\u2019); f.write(chrome_trace);f.close() <pre><code>- Analyze the output with google web tracing framework http://google.github.io/tracing-framework/\n- Install Chrome plugin: http://google.github.io/tracing-framework/\n- Go to chrome://tracing/, and load the generated json file\n\n**VTune profiling**\nThis is the same as you do for other applications. \n</code></pre> source /opt/intel/vtune_amplifier/amplxe-vars.sh  aprun -n ... -e OMP_NUM_THREADS=128 -e LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/intel/vtune_amplifier/lib64 \\ ampxle-cl -collect advance-hotspots -r output_dir python script.py <pre><code>Please remember to put $LD_LIBRARY_PATH in aprun. \n\n**TensorBoard**\nTensorBoard is a tool for post-processing and visualization data generated by TensorFlow. It could also be used to interactively manage TensorFlow runs. On Theta, currently, we only support post-processing, and visualization. Below is the procedure for visualizing TensorFlow from your local machine by SSH tunneling to Theta.\n\n1. SSH tunnel to Theta\n</code></pre> ssh -XL 16006:127.0.0.1:6006 user@theta.alcf.anl.gov <pre><code>2. Run tensorboard on Theta\n</code></pre> module load datascience/tensorboard  tensorboard --logdir DIR</code></p> <ol> <li>Open browser from local machine: https://localhost:16006</li> </ol> <p>Uncertainty Quantification &amp; TensorFlow probability TensorFlow Probability (TFP) is a library for probabilistic reasoning and statistical analysis in TensorFlow and is available as part of the TensorFlow 1.10 module on Theta. TFP provides several layers for statistical analysis, model building and probabilistic inference. See https://github.com/tensorflow/probability for end-to-end examples. On Theta TFP is scalability to large datasets and modelsusing Horovod for distributed computing.</p>"},{"location":"theta/data-science-workflows/tensorflow/#faq-and-common-issues","title":"FAQ and common issues","text":"<p>\"Illegal instruction\" or \"AVX512F\" error This happens when you are trying to run AVX512 compiled applications on login node or mom node. Try to run use aprun on KNL nodes. If you hit this error while you are building python package, try to use \"aprun -n 1 -cc node python setup.py build ...\"</p> <p>Cannot download dataset When the job is submitted, it is submitted to KNL nodes which are not connected to outside internet. Therefore, it is suggested that the users download the datasets on login node (e.g., through wget), or transfer the data through scp or Globus. </p>"},{"location":"theta/data-science-workflows/tensorflow/#references","title":"References","text":"<p>Horovod - https://eng.uber.com/horovod - https://arxiv.org/abs/1802.05799</p>"},{"location":"theta/data-science-workflows/containers/containers/","title":"Containers on Theta(KNL)","text":"<p>Containers are a method for shipping software that is pre-built inside a pre-defined static software environment.  At ALCF, users must run Singularity containers. Singularity is a container technology built for supercomputers with security in mind.  Typically we recommend users build Docker containers first, which can then be easily converted to Singularity containers.</p> <p>We will not repeat the detailed instructions for building docker containers, but do provide system specific examples of what a <code>Dockerfile</code> should look like below.</p> <ul> <li>General Docker documentation can be foundhere</li> <li>Specifics on building docker container recipes using <code>Dockerfile</code> can be found here</li> </ul> <p>The trickiest parts of building containers for ALCF systems is ensuring proper MPI support and GPU driver compatibility.</p>"},{"location":"theta/data-science-workflows/containers/containers/#docker","title":"Docker","text":"<p>The easiest way to build a container is from your laptop. First, install Docker. Then follow these steps.</p> <p>We have an example installation in the Container directory, which contains an <code>Dockerfile</code>. This is a container recipe file that we will use to tell Docker how to install our software.</p>"},{"location":"theta/data-science-workflows/containers/containers/#example-dockerfile","title":"Example <code>Dockerfile</code>","text":"<p>We include example build source code here: Local Example Source. This includes an example Dockerfile which we will describe line-by-line below.</p> <p><pre><code>FROM ubuntu:20.04\n</code></pre> The first line specifies a starting point for our container. In this instance, we start from a container that only has Ubuntu version 20.04 installed. However, we could start with any other container available on Docker Hub. We'll build everything else on top of this operating system.</p> <pre><code>RUN apt-get update -y \\\n&amp;&amp; DEBIAN_FRONTEND=noninteractive \\\n&amp;&amp; apt-get install -y build-essential libfabric-dev libibverbs-dev gfortran wget \\\n&amp;&amp; apt-get install -y python3 python3-distutils python3-pip gcc\n</code></pre> <p>Here we install system packages we need to build and run our code examples using the standard Ubuntu package manager <code>apt</code>.</p> <pre><code>WORKDIR /mpich\n# Source is available at http://www.mpich.org/static/downloads/\n# See installation guide of target MPICH version\n# Ex: https://www.mpich.org/static/downloads/4.0.2/mpich-4.0.2-installguide.pdf\n# These options are passed to the steps below\nARG MPICH_VERSION=\"3.3\"\nARG MPICH_CONFIGURE_OPTIONS=\"--prefix=/mpich/install --disable-wrapper-rpath\"\nARG MPICH_MAKE_OPTIONS=\"-j 4\"\nRUN wget http://www.mpich.org/static/downloads/${MPICH_VERSION}/mpich-${MPICH_VERSION}.tar.gz \\\n&amp;&amp; tar xfz mpich-${MPICH_VERSION}.tar.gz  --strip-components=1 \\\n&amp;&amp; ./configure ${MPICH_CONFIGURE_OPTIONS} \\\n&amp;&amp; make install ${MPICH_MAKE_OPTIONS}\nENV PATH $PATH:/mpich/install/bin\nENV LD_LIBRARY_PATH $LD_LIBRARY_PATH:/mpich/install/lib\n</code></pre> <p>Here we change our working directory to <code>/mpich</code> and then download and install MPI from scratch with some specific build options. You can find the installation documentation HERE. The key compilation option is the <code>--disable-wrapper-rpath</code> which makes it possible to build applications inside the container using this MPI library, but then replace those libraries with the Theta-specific libraries during runtime simply using the <code>LD_LIBRARY_PATH</code> environment variable. This is important since Theta uses high-speed network interfaces that require custom drivers and interface libraries to use.</p> <pre><code>RUN pip install mpi4py\n</code></pre> <p>Here we simply install <code>mpi4py</code> into our python environment and it will utilize the MPICH we installed.</p> <pre><code>WORKDIR /usr\nCOPY source/* /usr/source/\nCOPY submit.sh /usr/\nRUN chmod +x /usr/submit.sh\nRUN mpicc -o /usr/source/mpi_hello_world /usr/source/mpi_hello_world.c\n</code></pre> <p>Next we copy the source/ code examples from our repo (paths are with respect to the location of the <code>Dockerfile</code>) into our containers filesystem and build the C-file into a binary we can later execute on Theta.</p> <pre><code>ENTRYPOINT [\"/usr/submit.sh\"]\n</code></pre> <p>In Docker (and Singularity) you can simply \"run\" a container if an entry point is defined, so calling <code>docker run &lt;container-name&gt;</code> in this recipe executes our <code>submit.sh</code> script. Otherwise we can be more explicit can call any binary in our container using <code>docker exec &lt;container-name&gt; &lt;command&gt;</code>.</p>"},{"location":"theta/data-science-workflows/containers/containers/#building-the-docker-image-and-upload-to-docker-hub","title":"Building the Docker Image and Upload to Docker Hub","text":"<p>Side Note:  Docker uses the terms \"image\" and \"contianer\" this way: 'images' are a static software environment from which 'containers' can be launched and created. For instance, I would have one image named <code>my_analysis_app</code> which contains my applications. However, this application can be run on multiple input data files in parallel. I can therefore use this image to launch multiple concurrent 'containers' and provide each one with different input data. Each 'container' is an isolated application running. They can be 'started' and 'stopped' or 'restarted'. I can 'enter' these containers interactively and do things to the running environment, e.g. <code>apt install python</code>, but when my container stops running all these changes are gone. Only changes to the base image last (though you can create a new image from a running container).</p> <p>The first step is to create Docker Hub account here: DockerHub Hub. Then follow these steps.</p> <p>Create a Docker Hub Repository:</p> <p></p> <p>Then build your Image:</p> <p><pre><code># build image from Dockerfile, include the path to the folder that contains the Dockerfile\ncd /path/to/CompPerfWorkshop/03_containers\ndocker build -t jtchilders/alcf_cwp_example:latest ./Local/\n</code></pre> Here you see my username and repository name <code>jtchilders/alcf_cwp_example</code> and then attached with a <code>:</code> is the \"tag\" for this image. Just like GitHub, an image is tagged with a version. It's traditional in Docker to use <code>latest</code> for the most recent version, but otherwise, you can use any string you like.</p> <p>Then <code>./Local/</code> simply points to the folder which contains my <code>Dockerfile</code>. The build recipe will be run in that folder and have access to all the source files inside. As an example, the instructions to build images for Theta(KNL) and Theta(GPU) are different. In order to build the image for Theta(GPU) you need to use the <code>Dockerfile_thetagpu</code> and must change the build command so it knows to use this file instead of the default:</p> <pre><code>docker build -t jtchilders/alcf_cwp_example:thetagpu -f ./Local/Dockerfile_thetagpu ./Local/\n</code></pre> <p>In this case, I'm still naming the image with my <code>username/repo</code> but I've changed my tag name to <code>thetagpu</code> so I can distinguish it from the previous image we built. I've also instructed docker on which <code>Dockerfile</code> to use as the recipe.</p> <p>Last step is to make this image accessible on Theta, so we'll push the newly built image to Docker Hub using: <pre><code>docker push jtchilders/alcf_cwp_example:latest\ndocker push jtchilders/alcf_cwp_example:thetagpu\n</code></pre></p> <p>Quick demo:</p> <p></p>"},{"location":"theta/data-science-workflows/containers/containers/#singularity","title":"Singularity","text":""},{"location":"theta/data-science-workflows/containers/containers/#building-singularity-images-from-docker-images-on-theta","title":"Building Singularity images from Docker images on Theta","text":"<p>Anywhere singularity is installed, for instance on the Theta login nodes, you can run a build command based off a docker image using:</p> <pre><code>singularity build &lt;image_name&gt; docker://&lt;username&gt;/&lt;repository_name&gt;:&lt;tag&gt;\n</code></pre> <p>Example:</p> <p></p>"},{"location":"theta/data-science-workflows/containers/containers/#run-singularity-container-on-theta","title":"Run Singularity container on Theta","text":"<pre><code>qsub -A &lt;project-name&gt; /path/to/job_submission_theta.sh &lt;/path/to/image_name&gt;\n</code></pre>"},{"location":"theta/data-science-workflows/containers/containers/#example-job_submission_thetash","title":"Example <code>job_submission_theta.sh</code>","text":"<p>First we define the job submission parameters (number of nodes <code>-n</code>, queue name <code>-q</code>, wall time <code>-t</code>, etc.) that are needed to submit a job on Theta(KNL), the number of ranks per node, and the container is passed as an argument to the submission script.</p> <p><pre><code>#!/bin/bash\n#COBALT -t 30\n#COBALT -q debug-flat-quad\n#COBALT -n 2\n#COBALT -A &lt;project_name&gt;\n#COBALT --attrs filesystems=theta-fs0,home\nRANKS_PER_NODE=4\nCONTAINER=$1\n</code></pre> Next we load the proper Cray MPICH module for MPI support on Theta. ABI (Application Binary Independent) simply means, we can build our application inside the image using the MPICH we installed there. Then, at run-time, we will use the <code>LD_LIBRARY_PATH</code> to point our application at Cray's MPICH libraries instead. ABI enforces the use of a common interface to enable this swapping in and out. Otherwise, this would fail.</p> <pre><code># Use Cray's Application Binary Independent MPI build\nmodule swap cray-mpich cray-mpich-abi\n#Only needed when interactive debugging\n#module swap PrgEnv-intel PrgEnv-cray; module swap PrgEnv-cray PrgEnv-intel\n</code></pre> <p>These <code>ADDITIONAL_PATHS</code> are the paths to dependencies from the Cray modules.</p> <pre><code>export ADDITIONAL_PATHS=\"/opt/cray/diag/lib:/opt/cray/ugni/default/lib64/:/opt/cray/udreg/default/lib64/:/opt/cray/xpmem/default/lib64/:/opt/cray/alps/default/lib64/:/opt/cray/wlm_detect/default/lib64/\"\n</code></pre> <p>Now we add all these library paths to the <code>SINGULARITYENV_LD_LIBRARY_PATH</code> which will be used by Singularity to set the <code>LD_LIBRARY_PATH</code> environment variable inside the container at runtime.</p> <pre><code># The LD_LIBRARY_PATH and/or PATH environment variables in a \n# Singularity container can be altered only using the SINGULARITYENV_LD_LIBRARY_PATH \n# or SINGULARITYENV_PATH environment variables prior to execution.\nexport SINGULARITYENV_LD_LIBRARY_PATH=\"$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH:$ADDITIONAL_PATHS\"\n</code></pre> <p>We need all the libraries to be accessible inside the container, therefore, we \"bind-mount\" the base path to all our dependencies using the <code>-B</code> option.</p> <pre><code># need to mount these folders inside the container so that the Cray MPICH libraries will be found.\nBINDINGS=\"-B /opt -B /etc/alternatives\"\n</code></pre> <p>Next the actual run commands that combine the <code>aprun</code> launcher call that handles the MPI, and the <code>singularity</code> call to handle the containerized environment.</p> <pre><code>TOTAL_RANKS=$(( $COBALT_JOBSIZE * $RANKS_PER_NODE ))\n# run my containner like an application\naprun -n $TOTAL_RANKS -N $RANKS_PER_NODE singularity exec $BINDINGS $CONTAINER /usr/source/mpi_hello_world\naprun -n $TOTAL_RANKS -N $RANKS_PER_NODE singularity exec $BINDINGS $CONTAINER python3 /usr/source/mpi_hello_world.py\n</code></pre> <p>The output should look like this: <pre><code>Hello world from processor nid00020, rank 2 out of 8 processors\nHello world from processor nid00020, rank 3 out of 8 processors\nHello world from processor nid00020, rank 0 out of 8 processors\nHello world from processor nid00020, rank 1 out of 8 processors\nHello world from processor nid00021, rank 6 out of 8 processors\nHello world from processor nid00021, rank 7 out of 8 processors\nHello world from processor nid00021, rank 4 out of 8 processors\nHello world from processor nid00021, rank 5 out of 8 processors\nApplication 26449404 resources: utime ~14s, stime ~8s, Rss ~39912, inblocks ~64022, outblocks ~0\nHello world from processor nid00021, rank 7 out of 8 processors\nHello world from processor nid00021, rank 6 out of 8 processors\nHello world from processor nid00021, rank 5 out of 8 processors\nHello world from processor nid00021, rank 4 out of 8 processors\nHello world from processor nid00020, rank 2 out of 8 processors\nHello world from processor nid00020, rank 3 out of 8 processors\nHello world from processor nid00020, rank 1 out of 8 processors\nHello world from processor nid00020, rank 0 out of 8 processors\nApplication 26449405 resources: utime ~14s, stime ~8s, Rss ~39392, inblocks ~83290, outblocks ~0\n</code></pre></p>"},{"location":"theta/data-science-workflows/containers/containers/#references","title":"References","text":"<p>Using Containers on Theta</p>"},{"location":"theta/debugging-tools/arm-ddt/","title":"Arm DDT on Theta","text":""},{"location":"theta/debugging-tools/arm-ddt/#availability","title":"Availability","text":"<p>You can use the latest Arm (Allinea) DDT debugger on the XC40 system (Theta).</p> <p>There is no limit to the number of users; however, users have to share the available license tokens.</p>"},{"location":"theta/debugging-tools/arm-ddt/#modules-and-soft-keys","title":"Modules and Soft keys","text":"<p>On Theta, for the latest version load the module forge/22.0.4.  Note: When using the Arm Forge Remote Client, specify remote installation directory, <code>\"/soft/debuggers/forge-22.0.4-2022-08-02\u201d</code>. The Remote Client version must match.</p> <p>On the systems managed with softenv (Cooley), use the soft key \"+forge\" for the latest version available. When using the Allinea Remote Client, specfiy remote installation directory <code>\"/soft/debuggers/forge\"</code>.</p>"},{"location":"theta/debugging-tools/arm-ddt/#debugging-with-ddt","title":"Debugging with DDT","text":"<p>DDT may be started in two ways.</p> <ol> <li>Via Remote Client from your laptop or workstation (Recommended)</li> <li>Running the DDT client on a login node and displaying back to you via X11.</li> </ol>"},{"location":"theta/debugging-tools/arm-ddt/#option-a-via-remote-client-recommended","title":"Option A: Via Remote Client (Recommended)","text":"<p>This method is best for remote use of DDT because the GUI client runs directly on your laptop or workstation. This has much lower remote bandwith requirements than the other method.</p> <ol> <li>Download the remote client at https://developer.arm.com/downloads/-/arm-forge. </li> <li>Note: There is a link near the bottom of the page for versions prior to the latest release. You have to use a similar version to the loaded forge module on Theta. For example, you may download Arm Forge Client 22.0.4 for the Theta module forge/22.0.4. </li> <li>Run the client on your local machine and select Remote Launch-&gt;Configure to set up a configuration to connect to the login node.</li> <li>Run your application on Theta with one of the following command lines:</li> <li>ddt --offline aprun -n 48 ./example</li> <li>ddt --connect aprun -n 48 ./example</li> </ol> <p> </p> <p>On an interactive job mode, run your application with ddt --connect. <pre><code>jkwack@thetalogin6:~&gt; qsub -I -n 1 -t 60 -q debug-cache-quad -A Performance\nConnecting to thetamom2 for interactive qsub...\nJob routed to queue \"debug-cache-quad\".\nMemory mode set to cache quad for queue debug-cache-quad\nWARNING: Filesystem attribute not set for this job submission.\nWait for job 613897 to start...\nOpening interactive session to 3834\n\njkwack@thetamom2:/lus/swift/home/jkwack&gt; module load forge\n\njkwack@thetamom2:/lus/swift/home/jkwack&gt; cd HPC_benchmarks/JKBench_GeoSeries\n\njkwack@thetamom2:~/HPC_benchmarks/JKBench_GeoSeries&gt; ddt --connect aprun -n 64 ./Comp_GeoSeries_omp_cpu_cc_DP 4096 100\n</code></pre></p> <p> </p>"},{"location":"theta/debugging-tools/arm-ddt/#references","title":"References","text":"<ul> <li>Arm Forge Website</li> <li>Arm DDT User Guide</li> <li>Debugging and Profiling with DDT and Map (SDL Workshop 2019)</li> </ul>"},{"location":"theta/debugging-tools/atp-and-stat/","title":"ATP and STAT on Theta","text":""},{"location":"theta/debugging-tools/atp-and-stat/#introduction","title":"Introduction","text":"<p>ATP and STAT are tools to debug abnormal program terminations such as segfaults. ATP (Abnormal Termination Processing) monitors a program while it runs.  If the program crashes, ATP will invoke STAT (the Stack Trace Analysis Tool) to merge the stack backtraces of the application processes to an output file \"atpMergedBT.dot\". This merged stack backtrace file may then be visualized using STAT's visualization tool, stat-view.</p>"},{"location":"theta/debugging-tools/atp-and-stat/#using-atp-with-stat-view","title":"Using ATP with stat-view","text":"<p>Scenario: When you try to run you get a segfault.   After running, the job's <code>`stderr file (which defaults to $COBALT_JOBID.error)</code> contains: <pre><code>user@thetalogin6:~&gt; cat $COBALT_JOBID.error\n_pmiu_daemon(SIGCHLD): [NID 03834] [c7-1c2s14n2] [Sat Aug 18 03:21:19 2018] \\\n                                   PE RANK 30 exit signal Segmentation fault\n[NID 03834] 2018-08-18 03:21:19 Apid 4938801: initiated application termination\n</code></pre> ATP and stat-view can be used to look into the segfault.</p>"},{"location":"theta/debugging-tools/atp-and-stat/#compilelink-setup","title":"Compile/Link Setup","text":"<p>To use ATP, the ATP module should be loaded before linking your application . By default it is loaded on Theta, but to verify this, run module list, and check that the atp module is loaded.</p> <pre><code>user@thetalogin6:~&gt; module list\nCurrently Loaded Modulefiles:\n  1) modules/3.2.10.6\n  2) intel/18.0.0.128 \n  3) craype-network-aries\n  ...     \n  16) atp/2.1.2\n  17) perftools-base/7.0.2\n  ...\n\nuser@thetalogin6:~&gt; make\n</code></pre>"},{"location":"theta/debugging-tools/atp-and-stat/#running-the-code","title":"Running the code","text":"<p>Next, the environment variable ATP_ENABLED must be set in the job script to enable ATP.</p> <p><pre><code>export ATP_ENABLED=1 # in bash\n</code></pre> For Intel Fortran programs, also set the environment variable <code>FOR_IGNORE_EXCEPTIONS</code> in the job script: <pre><code>export FOR_IGNORE_EXCEPTIONS=true # in bash\n</code></pre> Then start your program with aprun as normal. <pre><code>#!/bin/bash\n#COBALT -t 10 -n 1 -q debug-cache-quad\n\nexport ATP_ENABLED=1\nexport FOR_IGNORE_EXCEPTIONS=true\naprun -n 64 -N 64 test.exe\n</code></pre> If the program crashes, a atpMergedBT.dot should be produced, and the first process to die will write a stack backtrace to the job's stderr file (by default $COBALT_JOBID.error).  For the scenario, this would look like:</p> <pre><code>user@thetalogin6:~&gt; ls -ltr\n-rw-r--r-- 1 user users      813 Aug 18 03:47 atpMergedBT_line.dot\n-rw-r--r-- 1 user users      768 Aug 18 03:48 atpMergedBT.dot\n-rw-r--r-- 1 user users     1742 Aug 18 03:48 262891.error\n-rw-r--r-- 1 user users      143 Aug 18 03:49 262891.output\n-rw-r--r-- 1 user cobalt    1965 Aug 18 03:49 262891.cobaltlog\nuser@thetalogin6:~&gt; cat 262891.error\nApplication 4938811 is crashing. ATP analysis proceeding...\n\nATP Stack walkback for Rank 31 starting:\n  _start@start.S:118\n  __libc_start_main@libc-start.c:289\n  main@test.c:19\nATP Stack walkback for Rank 31 done\nProcess died with signal 11: 'Segmentation fault'\nView application merged backtrace tree with: stat-view atpMergedBT.dot\nYou may need to: module load stat\n\n_pmiu_daemon(SIGCHLD): [NID 03834] [c7-1c2s14n2] [Sat Aug 18 03:48:09 2018] \\\n                                               PE RANK 1 exit signal Killed\n[NID 03834] 2018-08-18 03:48:09 Apid 4938811: initiated application termination\n</code></pre>"},{"location":"theta/debugging-tools/atp-and-stat/#looking-at-the-output","title":"Looking at the output","text":"<p>The program stat-view (the Stack Trace Analysis Tool viewer) can be used to view the backtrace file atpMergedBT.dot, which shows what the program was doing at the time of abnormal termination.</p> <p><code>user@thetalogin6:~&gt; module load stat</code></p> <p><code>user@thetalogin6:~&gt; stat-view atpMergedBT_line.dot</code></p> <p>The difference between atpMergedBT_line.dot and atpMergedBT.dot is that atpMergedBT_line.dot should contain line numbers.</p> <p>The output will look like the following:</p> <p> </p> Difference between atpMergedBT_line.dot and atpMergedBT.dot is that atpMergedBT_line.dot <p>This shows that there is a segfault at line 19 of test.c, the test file. Each box is typically a function in a stack backtrace or a comment, denoted by brackets ( \"[ ]\" ) and discussed later.</p> <p>Moving from top to bottom, following the arrows, follows a backtrace. The arrow connecting the boxes is labeled with how many ranks are active for the function, and which ranks. Here we see that for every function, 64 ranks were active, and they are ranks 0-63 (this makes sense since this was done with a single node, 64 rank job). If only a subset of ranks had had a segault, then the graph would label how many segfaulted and which ones.</p> <p>Some of the boxes are not functions, but are comments, which is denoted by the use of bracket (\"[ ]\") in the name.  Here, the \"[Fault Summary]\" box is a summary tree saying that all 64 ranks, 0-63 faulted due to a SIGSEV.</p> <p>Note: To use stat-view, an X display will need to be forwarded from your local machine.  When you connect to the Theta login node, use the -Y or -X option to ssh: <pre><code>user@thetalogin6:~&gt; ssh -Y user@theta.alcf.anl.gov\n</code></pre></p>"},{"location":"theta/debugging-tools/atp-and-stat/#references","title":"References","text":"<ul> <li>Man pages: user@thetalogin6:~&gt; man intro_atp</li> <li>Debugging on ALCF Systems</li> </ul>"},{"location":"theta/debugging-tools/debugging-overview/","title":"Introduction of Debugging","text":""},{"location":"theta/debugging-tools/debugging-overview/#initial-setups","title":"Initial Setups","text":"<ul> <li>Submitting an interactive job \u2013 Save time by queuing once and running multiple times.</li> <li>Using VNC with a Debugger \u2013 when displaying an X11 client (e.g., DDT) remotely over the network, interactive response is typically slow. Using the VNC server can often help you improve the situation.</li> </ul>"},{"location":"theta/debugging-tools/debugging-overview/#debugging-tools","title":"Debugging Tools","text":"<ul> <li>Arm Forge (formerly Allinea DDT) \u2013 a full-featured GUI-based debugging tool supporting scalar, multi-threaded, and large-scale parallel applications.</li> <li>ATP \u2013 Cray\u2019s Abnormal Termination Processing can generate a merged stack backtrace if your program dies.</li> <li>STAT \u2013 Generate a merged stack backtrace snapshot while your run is in progress.  Useful to diagnose deadlock.</li> <li>lgdb \u2013 Provides a command-line interface for debugging a parallel application with gdb (the GNU Project Debugger).</li> </ul>"},{"location":"theta/debugging-tools/debugging-overview/#common-debugging-issues","title":"Common Debugging Issues","text":"<ul> <li>Determining Memory Use \u2013 learn how to use the glibc mallinfo call to get information on used/available memory in your code.</li> </ul>"},{"location":"theta/debugging-tools/debugging-overview/#references","title":"References","text":""},{"location":"theta/debugging-tools/gdb/","title":"GDB","text":""},{"location":"theta/debugging-tools/gdb/#introduction","title":"Introduction","text":"<p>GDB4HPC is a command-line debugging tool provided by Cray. It works similarly to GBD, but allows the user to debug multiple parallel processes without multiple windows. GDB4HPC can be used to investigate deadlocked code, segfaults, and other errors for C/C++ and Fortran code. Users can single-step code and focus on specific processes groups to help identify unexpected code behavior.</p> <p>This page focuses on using GDB4HPC during an interactive login session by launching the code through the debugger. (GDB4HPC can also be used to attach to a running process, but this is not covered here.)</p>"},{"location":"theta/debugging-tools/gdb/#using-gdb4hpc","title":"Using GDB4HPC","text":""},{"location":"theta/debugging-tools/gdb/#a-launching-the-code-through-the-debugger","title":"A. Launching the code through the debugger","text":"<p>1. Compile Code To add debugging symbols to your code, compile it with the \u201c-g\u201d flag. Note that sometimes it is useful to decrease the compiler optimization level. At higher optimization levels, the compiler will reorganize or optimize out variables, and when you step through the code with the debugger, the line numbers may no longer match what you see.</p> <p>For example, for source file test.c: <code>user@thetalogin1:~&gt; cc -g -O0 test.c</code></p> <p>2. Log into an interactive session To debug interactively, log into an interactive session. An interactive session logs you into a MOM/launch node (\"thetamom1\" below), where GDB4HPC will be run. (GDB4HPC will iitself launch jobs from the MOM/launch node to the compute nodes.)</p> <p>For example, submitting a interactive qsub job for 2 nodes in the debug-cache-quad queue: <pre><code>user@thetalogin1:~&gt; qsub -I -n 2 -q debug-cache-quad -t 60 -A &lt;project&gt;\nJob routed to queue \"debug-cache-quad\".\nMemory mode set to cache quad for queue debug-cache-quad\nWait for job 314931 to start...\nOpening interactive session to 3827\nuser@thetamom1:~&gt;\n</code></pre></p> <p>3. Setup the environment To use GDB4HPC, the appropriate modules should be loaded so that the correct libraries and applications are in your path: <code>user@thetamom1:~&gt; module load gdb4hpc</code></p> <p>4. Start the debugger Start the debugger by calling \"gdb4hpc\" from the shell on the MOM node: <pre><code>user@thetamom1:~&gt; gdb4hpc\ngdb4hpc 3.0 - Cray Line Mode Parallel Debugger\nWith Cray Comparative Debugging Technology.\nCopyright 2007-2018 Cray Inc. All Rights Reserved.\nCopyright 1996-2016 University of Queensland. All Rights Reserved.\nType \"help\" for a list of commands.\nType \"help &lt;cmd&gt;\" for detailed help about a command.\ndbg all&gt;\n</code></pre> 5. Launch the application through the debugger As an example, say that we want to debug a job we would normally run with <code>aprun -n 128 -N 64 ./a.out</code> on Theta, which launches 128 MPI ranks total, with 64 on each node.</p> <p>To debug the executable \"a.out\", the run can be launched with the command <code>launch $a{128} --launcher-args=\"-N 64\" ./a.out</code> (as shown below). The \"launch\" command invokes aprun to run the job on the compute nodes, and then pauses the execution for commands from the user.</p> <p>A short explanation of the syntax for the launch command: In GDB4HPC, MPI ranks are grouped into process sets. Here the process set that we want to launch is defined as $a{128}, which defines a set of 128 MPI ranks to launch, with the variable array name $a. Additional arguments to the launcher can be passed with the <code>--launcher-args=</code> flag. Here, we pass <code>--launcher-args=\"-N 64\"</code> to pass the number of ranks per node to the launcher.</p> <p><pre><code>dbg all&gt; launch $a{128} --launcher-args=\"-N 64\" ./a.out\nlaunch $a{128} --launcher-args=\"-N 64\" ./a.out\nStarting application, please wait...\nCreating MRNet communication network...\nWaiting for debug servers to attach to MRNet communications network...\nTimeout in 400 seconds. Please wait for the attach to complete.\nNumber of dbgsrvs connected: [21];  Timeout Counter: [0]\nNumber of dbgsrvs connected: [128];  Timeout Counter: [0]\nFinalizing setup...\nLaunch complete.\na{0..127}: Initial breakpoint, main at /gpfs/mira-home/user/test.c:8\ndbg all&gt;\n</code></pre> 6.  Investigating the execution of the program Many of the standard gdb commands can be issued to investigate the execution of the code. Several examples are shown below, using the example of a 128 MPI rank-application with 64 ranks per node.</p> <p>List source lines to see where execution is: <pre><code>a{0..127}: Initial breakpoint, main at /gpfs/mira-home/user/test.c:8\ndbg all&gt; list\nlist\na{0..127}: 8      int *a = NULL;\na{0..127}: 9      int passed = 0;\na{0..127}: 10 \na{0..127}: 11     MPI_Init( &amp;argc, &amp;argv );\na{0..127}: 12 \na{0..127}: 13     MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank );\na{0..127}: 14     MPI_Comm_size( MPI_COMM_WORLD, &amp;nranks );\na{0..127}: 15 \na{0..127}: 16     a = (int *)malloc( N*sizeof(int));\na{0..127}: 17\ndbg all&gt;\n</code></pre> Note: a{0} corresponds to MPI rank 0, and a{0..127} corresponds to all 128 MPI ranks.</p>"},{"location":"theta/debugging-tools/gdb/#single-step","title":"Single step","text":"<pre><code>dbg all&gt; step\nstep\na{0..127}: main at /gpfs/mira-home/user/test.c:9\ndbg all&gt; list\nlist\na{0..127}: 9      int passed = 0;\na{0..127}: 10   \na{0..127}: 11     MPI_Init( &amp;argc, &amp;argv );\na{0..127}: 12   \na{0..127}: 13     MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank );\na{0..127}: 14     MPI_Comm_size( MPI_COMM_WORLD, &amp;nranks );\na{0..127}: 15   \na{0..127}: 16     a = (int *)malloc( N*sizeof(int));\na{0..127}: 17   \na{0..127}: 18     // all ranks initialize\ndbg all&gt;\n</code></pre>"},{"location":"theta/debugging-tools/gdb/#set-a-breakpoint-at-line-32-of-testc","title":"Set a breakpoint at line 32 of test.c","text":"<pre><code>dbg all&gt; break test.c:32\nbreak test.c:32\na{0..127}: Breakpoint 1: file /gpfs/mira-home/user/test.c, line 32.\ndbg all&gt;\n</code></pre>"},{"location":"theta/debugging-tools/gdb/#continue","title":"Continue","text":"<pre><code>dbg all&gt; continue\ncontinue\na{0..127}: Breakpoint 1, main at /gpfs/mira-home/user/test.c:32\ndbg all&gt; list\nlist\na{0..127}: 32     if( sum == (N*(N-1)/2) )\na{0..127}: 33       {\na{0..127}: 34         passed = 1;\na{0..127}: 35         printf( \"passed for rank %d!\", myrank );\na{0..127}: 36       }\na{0..127}: 37     else\na{0..127}: 38       {\na{0..127}: 39         printf( \"failed for rank %d!recieved: %d correct: %d\",\na{0..127}: 40                 myrank, sum, N*(N-1)/2 );\na{0..127}: 41       }\ndbg all&gt;\n</code></pre>"},{"location":"theta/debugging-tools/gdb/#backtrace","title":"Backtrace","text":"<pre><code>dbg all&gt; backtrace\nbacktrace\na{0..127}: #0  0x000000000040a7ac in main at /gpfs/mira-home/user/test.c:32\ndbg all&gt;\n</code></pre>"},{"location":"theta/debugging-tools/gdb/#see-what-all-ranks-have-for-variable-value","title":"See what all ranks have for variable \"value\"","text":"<pre><code>dbg all&gt; print value\nprint value\na{104}: 3416\na{105}: 3423\na{106}: 3430\na{107}: 3437\na{108}: 3444\na{109}: 3451\na{110}: 3458\na{111}: 3465\na{112}: 3472\n...\ndbg all&gt;\n</code></pre>"},{"location":"theta/debugging-tools/gdb/#see-what-rank-104-has-for-variable-value","title":"See what rank 104 has for variable \"value\"","text":"<p><pre><code>dbg all&gt; print $a{104}::value\nprint $a{104}::value\na{104}: 3416\ndbg all&gt;\n</code></pre> The code used in the example is shown below: <pre><code>user@thetamom1:~&gt; cat test.c\ncat test.c\n#include &lt;mpi.h&gt;\n#include \"stdio.h\"\n#define N 1000\n\nint main( int argc, char *argv[] )\n{\n  int myrank, nranks, i, sum, value;\n  int *a = NULL;\n  int passed = 0;\n\n  MPI_Init( &amp;argc, &amp;argv );\n\n  MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &amp;nranks );\n\n  a = (int *)malloc( N*sizeof(int));\n\n  // all ranks initialize\n  for( i=0; i &lt; N; i++ )\n    a[i] = i;\n\n  value = 0;\n\n  // each rank only sums up part of the array\n  for( i=myrank; i &lt; N; i+=nranks )\n    value += a[i];\n\n  MPI_Allreduce( &amp;value, &amp;sum, 1, MPI_INT, MPI_SUM,\n                 MPI_COMM_WORLD );\n\n  if( sum == (N*(N-1)/2) )\n    {\n      passed = 1;\n      printf( \"passed for rank %d!\\n\", myrank );\n    }\n  else\n    {\n      printf( \"failed for rank %d!\\nrecieved: %d correct: %d\\n\",\n              myrank, sum, N*(N-1)/2 );\n    }\n\n  free( a );\n  MPI_Finalize();\n  return 0;\n\n}\n</code></pre></p>"},{"location":"theta/debugging-tools/gdb/#references-and-resources","title":"References and Resources","text":"<ul> <li>GDB4HPC manual page on Theta (gdb4hpc module should be loaded first): <code>man gdb4hpc</code></li> </ul>"},{"location":"theta/hardware-overview/aries-network/","title":"Aries Network on Theta","text":"<p>The Cray Aries network is the high-speed interconnect used on Theta. This network is a 3-level Dragonfly topology. The Dragonfly topology consists of the first level as a small number of nodes connected on the same switch. The next level is a full-mesh connectivity of two racks of nodes. The last level is a set of links that interconnect all of the 2-rack groups.</p> <p>Nodes allocated to a job can be placed anywhere across the network topology. The allocation of nodes will affect the application\u2019s overall latency and available bandwidth based on the number and type of links used. The different levels of the interconnect are a shared resource among different nodes and thus the workload of running jobs may affect another job\u2019s network performance.</p> <p>The Aries routing protocol uses adaptive routing to select the best path through the network on a packet-by-packet basis. This routing method will avoid congestion but also allows for packets to flow over non-minimal routes.</p> <p>The level of variability with respect to MPI performance can be significant when an interfering job produces a large burst of MPI traffic. The variable performance should be accounted for when analyzing MPI performance by running many data points to ensure statistical accuracy.</p>"},{"location":"theta/hardware-overview/aries-network/#benchmarks","title":"Benchmarks","text":"<p>The following is a set of basic benchmarks that characterize the Aries network performance using MPI. </p> <p>Figure 1 shows the latency for a 4k message size across several node counts for MPI_Bcast, MPI_Gather, and MPI_Allreduce. Figure 2 is the point-to-point bandwidth for several different process counts and message sizes between two nodes that are located on the first level on the Dragonfly.</p> <p> </p> Figure 1 - MPI Collective Latency <p> </p> Figure 2 - MPI Message Bandwidth"},{"location":"theta/hardware-overview/aries-network/#references","title":"References","text":"<p>Cray Aries</p>"},{"location":"theta/hardware-overview/machine-overview/","title":"Theta","text":"<p>Theta is a Cray XC40 and consists of several types of nodes. Table 1 summarizes the system\u2019s capabilities.</p>"},{"location":"theta/hardware-overview/machine-overview/#table-1-machine-overview","title":"Table 1: Machine Overview","text":"THETA DESCRIPTION AGGREGATE Compute Nodes Intel KNL 7230 4,392 Compute Memory - DDR4 192 GiB 843,264 GiB Compute Cores 64 281,088 Compute Memory - MCDRAM 16 GiB 70,272 GiB Compute SSD 128 GiB 561,176 GiB LNET Service node for Lustre 30 DVS Service node for Cray DVS 60 Compute Racks 24 LINPACK RMax (Rpeak) Top500 LINPACK results 6.92 PFLOP/s(11.69 PFLOP/s) Tier 2 Service Node 13 MOM Service Node 3 eLogin Login Node 6 Project File system Lustre 10PB Home file system GPFS 1PB High Speed Network Aries Dragonfly 14400 rank 1 ports14400 rank 2 ports9600 rank 3 ports"},{"location":"theta/hardware-overview/machine-overview/#login-nodes","title":"Login Nodes","text":"<p>Theta has six login nodes that are Cray eLogin machines, which means they are nodes outside the main Cray system. These nodes are Intel Haswell E5-2698 v3 nodes with 256GB of DDR4 memory. These frontend nodes are used for editing code, building code, and submitting jobs. All users of the Theta system share these nodes. Please note that these nodes are not 100% binary compatible with AVX-512 instructions so applications compiled for the compute node will not run on the login nodes.</p>"},{"location":"theta/hardware-overview/machine-overview/#service-nodes","title":"Service Nodes","text":"<p>Service nodes are Cray\u2019s generic name for various nodes that provide internal infrastructure for the overall system. These nodes appear in the xtnodestat output as service nodes and consume part of the Node Identifier (NID) space. This is important with respect to job scheduling because requesting a NID that is a service node will cause your job not to run.</p>"},{"location":"theta/hardware-overview/machine-overview/#mom","title":"MOM","text":"<p>The Machine Oriented Mini-server (MOM) nodes run various part of the Cray software infrastructure. These are Intel E5-2695 v4 nodes with 128 GiB of DDR4 memory. These nodes also run the Cobalt scheduler and execute the user batch scripts. It is critical that users do not put computational or memory intensive tasks within the batch script and instead run those on the compute resources.</p>"},{"location":"theta/hardware-overview/machine-overview/#lnet","title":"LNET","text":"<p>The Lustre LNET routers serve as a gateway between the high-speed Aries fabric and the Infiniband FDR storage network. These nodes are Intel Sandy Bridge E5-2670 with 64 GiB of DDR3 memory.</p>"},{"location":"theta/hardware-overview/machine-overview/#dvs","title":"DVS","text":"<p>The Cray Data Virtualization Service (DVS) server provide a gateway between the high-speed Aries fabric and other external file systems. The DVS server primarily provide access to the GPFS file systems, such as the home file system. These nodes are physically identical to the LNET nodes, which are Intel Sandy Bridge E5-2670 with 64 GiB of DDR3 memory.</p>"},{"location":"theta/hardware-overview/machine-overview/#tier-2","title":"Tier 2","text":"<p>The Tier 2 nodes provide infrastructure to the Cray software stack and aggregate sets of compute nodes. These nodes are physically identical to the MOM nodes and have Intel E5-2695 v4 CPUs with 128 GiB of DDR4 memory.</p>"},{"location":"theta/hardware-overview/machine-overview/#compute-nodes","title":"Compute Nodes","text":"<p>Theta provides only a single compute node type: the Intel Knights Landing 7230 processors with 16 GiB of MCDRAM and 192 GiB of DDR4. These nodes have 64 cores each and each core has 4 SMT hardware threads available.</p>"},{"location":"theta/hardware-overview/theta-memory-modes/","title":"Theta Memory Modes","text":"<p>The Intel Xeon Phi Generation 2 (KNL) has the ability to change the memory and clustering modes. The memory and cluster mode affect the overall performance of memory access. While many combinations of the memory and clustering modes exist, this discussion will address only the most commonly used configurations.</p> <p>The primary suggestion is to begin with the quadrant clustering mode and the cache memory mode.</p>"},{"location":"theta/hardware-overview/theta-memory-modes/#clustering-mode","title":"Clustering Mode","text":"<p>The clustering mode defines an infinity between the thread\u2019s memory allocations and the memory domain in which the allocation is placed. This placement also influences which cache tag directory on the mesh is used. The clustering mode will never prevent memory from being read by any thread. The clustering mode only influences the performance. The possible clustering modes are All2All, Hemisphere, Quadrant, Sub-NUMA Clustering-2 (SNC-2), and Sub-NUMA Clustering-4 (SNC-4). These options can be selected during your Cobalt job submission using the --attrs option. Table 1 lists the clustering mode and the associated Cobalt selection.</p>"},{"location":"theta/hardware-overview/theta-memory-modes/#clustering-modes","title":"Clustering Modes","text":"Clustering Mode Cobalt Syntax All2All --attrs=numa=a2a Hemisphere --attrs=numa=hemi Quadrant --attrs=numa=quad SNC-2 --attrs=numa=snc2 SNC-4 --attrs=numa=snc4"},{"location":"theta/hardware-overview/theta-memory-modes/#usage","title":"Usage","text":"<p>For the a2a, hemi, and quad modes, no special configuration is needed as all memory is placed into a single NUMA domain. For the SNC-2 and SNC-4 modes, memory is split into two or four NUMA domains and each process must be assigned to a particular NUMA domain. This can be done via libnuma API calls or via the numactl command.</p>"},{"location":"theta/hardware-overview/theta-memory-modes/#flat-snc-4-example","title":"Flat SNC-4 Example","text":"<pre><code>aprun -N 16 -d 1 -cc depth -n 4 numactl -m 4 ./app : -n 4 numactl -m 5 ./app : -n 4 numactl -m 6 ./app : -n 4 numactl -m 7 ./app\n</code></pre> <p>Note: The -m parameter for numactl defines the region from which to allocate memory. The valid memory regions are determined by the mode selected. The All2All, Hemisphere, and Quadrant modes all define a single NUMA domain. The SNC-2 and SNC-4 modes define two or four domains, respectively. The memory mode when using flat, split, or equal will define twice the number of memory regions that the clustering mode defined. For example, the flat-quadrant mode would have two domains, 0 (DDR), and 1 (MCDRAM). The flat-snc4 mode has eight domains, 0..3 (DDR), and 4..7 (MCDRAM).</p>"},{"location":"theta/hardware-overview/theta-memory-modes/#memory-mode","title":"Memory Mode","text":"<p>The memory mode defines how the KNL MCDRAM (high bandwidth memory) will be addressed. Two possible ways are \u201ccache\u201d (where the MCDRAM is transparent to the user and acts as a last-level cache between the L2 and DDR4 memory) and \u201cflat\u201d (in which the MCDRAM is directly addressable and appears as a separate NUMA domain). </p> <p>The cache and flat modes can be combined in two hybrid modes, where a portion of the MCDRAM is directly addressable and a portion is cache. The possible memory modes are flat, cache, equal, and split. Table 2 lists the memory modes and associated Cobalt syntax.</p>"},{"location":"theta/hardware-overview/theta-memory-modes/#memory-modes","title":"Memory Modes","text":"Memory Mode Flat (%) Cache (%) Cobalt Syntax Cache 0 100 --attrs=mcdram=cache Equal 50 50 --attrs=mcdram=equal Split 75 25 --attrs=mcdram=split Flat 100 0 --attrs=mcdram=flat"},{"location":"theta/hardware-overview/theta-memory-modes/#usage_1","title":"Usage","text":"<p>There is no special configuration for addressing the MCDRAM in cache mode. The caching model will fetch data from DRAM and store it in MCDRAM as needed. The flat memory mode can be accessed one of two ways. The user can bind the process specifically to the MCDRAM using the \u201cnumactl\u201d command or use the libmemkind library to allocate memory from a specific area. These two methods can be used in combination as well.</p>"},{"location":"theta/hardware-overview/theta-memory-modes/#flat-quadrant-example","title":"Flat Quadrant Example","text":"<p>The application binds to MCDRAM so that all allocations are within the high bandwidth memory. If the program tries to allocate more than the 16 GB available MCDRAM, it will fail with an error:</p> <pre><code>aprun -n 64 -N 64 -d 1 -cc depth numactl -m 1 ./app\n</code></pre> <p>An alternative is to prefer allocation in MCDRAM, but allow allocations to spill over into DDR if more than 16 GB is allocated: <pre><code>aprun -n 64 -N 64 -d 1 -cc depth numactl -p 1 ./app\n</code></pre> See the numactl man page for more details.</p>"},{"location":"theta/hardware-overview/theta-memory-modes/#results","title":"Results","text":"<p>Table 3 shows the STREAM Triad memory bandwidth for common configurations.</p>"},{"location":"theta/hardware-overview/theta-memory-modes/#stream-triad-performance","title":"STREAM Triad Performance","text":"Mode Size Triad Flat - MCDRAM 7.5 GB 485 GB/s Flat - DRAM 7.5 GB 88 GB/s Cache 7.5 GB 352 GB/s Cache 120.0 GB 59 GB/s"},{"location":"theta/hardware-overview/theta-memory-modes/#references","title":"References","text":"<p>Intel HotChips Presentation PDF Colfax Research MCDRAM Colfax Research NUMA</p>"},{"location":"theta/performance-tools/arm-map/","title":"Arm MAP","text":""},{"location":"theta/performance-tools/arm-map/#introduction","title":"Introduction","text":"<p>Build reliable and optimized code for the right results on multiple Server and HPC architectures, from the latest compilers and C++ standards to Intel, 64-bit Arm, AMD, OpenPOWER and Nvidia GPU hardware. Arm Forge combines Arm DDT, the leading debugger for time-saving high performance application debugging, Arm MAP, the trusted performance profiler for invaluable optimization advice across native and Python HPC codes, and Arm Performance Reports for advanced reporting capabilities. Arm DDT and Arm MAP are also available as standalone products.</p>"},{"location":"theta/performance-tools/arm-map/#availability","title":"Availability","text":"<p>You can use the latest Arm (Allinea) MAP performance profiler on the XC40 system (Theta).</p> <p>There is no limit to the number of users; however, users have to share the available license tokens.</p>"},{"location":"theta/performance-tools/arm-map/#modules-and-soft-keys","title":"Modules and Soft keys","text":"<p>On Theta, for the latest version load the module forge/22.0.4. </p> <p>Note: When using the Arm Forge Remote Client, specify remote installation directory, <code>\"/soft/debuggers/forge-22.0.4-2022-08-02\u201d</code>. The Remote Client version must match.</p> <p>On the systems managed with softenv (Cooley), use the soft key \"+forge\" for the latest version available.  When using the Allinea Remote Client, specfiy remote installation directory <code>\"/soft/debuggers/forge\"</code>.</p>"},{"location":"theta/performance-tools/arm-map/#profiling-with-map","title":"Profiling with MAP","text":"<p>MAP may be started in two ways.</p> <ul> <li>Via Remote Client from your laptop or workstation (Recommended)</li> <li>Running the MAP client on a login node and displaying back to you via X11.</li> </ul>"},{"location":"theta/performance-tools/arm-map/#option-a-via-remote-client-recommended","title":"Option A: Via Remote Client (Recommended)","text":"<p>This method is best for remote use of MAP because the GUI client runs directly on your laptop or workstation.  This has much lower remote bandwith requirements than the other method.</p> <ol> <li>Download the remote client at https://developer.arm.com/downloads/-/arm-forge. </li> <li>Note: There is a link near the bottom of the page for versions prior to the latest release. You have to use a similar version to the loaded forge module on Theta. For example, you may download Arm Forge Client 22.0.4 for the Theta module forge/22.0.4. </li> <li>Run the client on your local machine and select Remote Launch-&gt;Configure to set up a configuration to connect to the login node.</li> <li>Run your application on Theta with one of the following command lines:</li> <li>map --offline aprun -n 48 ./example</li> <li>map --connect aprun -n 48 ./example</li> </ol> <p> </p>"},{"location":"theta/performance-tools/arm-map/#references","title":"References","text":"<ul> <li>Arm Forge Website</li> <li>Arm MAP User Guide</li> <li>Debugging and Profiling with DDT and Map (SDL Workshop 2019)</li> </ul>"},{"location":"theta/performance-tools/craypat/","title":"CrayPat","text":""},{"location":"theta/performance-tools/craypat/#introduction","title":"Introduction","text":"<p>The Cray Performance Measurement and Analysis Tools (or CrayPat) are a suite of utilities that enable the user to capture and analyze performance data generated during the execution of a program on a Cray system. It includes CrayPat, CrayPat-lite, Cray Apprentices2, Reveal and the Cray PAPI components. </p>"},{"location":"theta/performance-tools/craypat/#references","title":"References","text":"<ul> <li>Performance Profiling on KNL with Cray Perftools-lite</li> <li>CrayPAT User Guide Webpage on Cray Documentation Portal</li> <li>Manual pages (after module is loaded): <code>$ man perftools-lite</code></li> <li>Detailed information about the output report: <code>$ man pat_report</code></li> </ul>"},{"location":"theta/performance-tools/craypat/#craypat-lite","title":"CrayPat-lite","text":"<p>This section focuses on CrayPat-lite, the simplified, easy-to-use version of the Cray Performance Measurement and Analysis Tool set. CrayPat-lite provides basic performance analysis information automatically, with a minimum of the user interaction. While CrayPat provides more functionalities. </p> <p>1. Environment Setup To use CrayPat-lite, the appropriate module should be loaded, and darshan should be unloaded. <pre><code>$ module unload darshan\n\n$ module load perftools-lite\n</code></pre> Note:  - Darshan needs to be unloaded since it uses environment variables which CrayPat-lite also uses. - Note that the perftools-base module should already be listed under \u201cmodule list\u201d If not, load perftools base with \u201cmodule load perftools-base\u201d. This provides access to man pages and help system, and does not instrument code. (Should be loaded by default.) - \u201cmodule load perftools-lite\u201d loads performance instrumentation module, and will instrument programs when they are compiled.</p> <p>Tip: To verify that the correct modules are loaded, run \"module list\" and check that perftools-lite and perftools-base are loaded and that darshan is not present.</p> <p>2. Compile the Code to Use CrayPat-lite Rebuild code as usual. When the program is built, there should be output like below, noting that CrayPat-lite was used. <pre><code>$ make clean \n$ make ftn -O3 -qopt-report=5 -g -align array64byte test.f90 -o test \nINFO: creating the CrayPat-instrumented executable 'test' (sample_profile) ...OK\n</code></pre> Tip: To verify that the executable was instrumented with CrayPat-lite, use the following command to search the executable for CrayPat strings: <pre><code>$ strings test | grep \"CrayPat/X\" \nCrayPat/X: Version 7.0.0 Revision 5c29ce2 12/11/17 15:26:24\n</code></pre> 3. Run the Code Run the code as usual <pre><code>$ qsub -n 8 -t 30 -A project jobscript.sh\n\n$ cat jobscript.sh\n\n#!/bin/bash\n\naprun -n 512 -N 64 test\n</code></pre> 4. Output After the code finishes executing, CrayPat-lite output should be printed to stdout (likely at the end of the jobid.output file). More output will be saved in .rpt files and .ap2 files, which might be under a new directory created in the directory the run occurred in.</p> <p>An example of output is shown below. Note that it is truncated after Table 1, since this is just for illustration purposes. <pre><code>$ cat 309226.output\n\n\u2026\n\nNormal program output\n\n\u2026\n\n#################################################################\n#                                                               #\n#            CrayPat-lite Performance Statistics                #\n#                                                               #\n#################################################################\n\nCrayPat/X:  Version 7.0.0 Revision 5c29ce2  12/11/17 15:26:24\nExperiment:                  lite  lite/sample_profile\nNumber of PEs (MPI ranks):    512\nNumbers of PEs per Node:       64  PEs on each of  8  Nodes\nNumbers of Threads per PE:      1\nNumber of Cores per Socket:    64\nExecution start time:  Thu Mar 22 18:09:16 2018\nSystem name and speed:  nid00690  1.301 GHz (nominal)\nIntel Knights Landing CPU  Family:  6  Model: 87  Stepping:  1\nMCDRAM: 7.2 GHz, 16 GiB available as quad, cache (100% cache)\n\nAvg Process Time:           74.06 secs\nHigh Memory:             21,298.6 MBytes     41.6 MBytes per PE\nInstr per Cycle:             1.16\nObserved CPU cycle rate:     1.39 GHz\nI/O Read Rate:           1.899058 MBytes/sec\nI/O Write Rate:          0.447113 MBytes/sec\n\nTable 1:  Profile by Function\n\n  Samp% |    Samp |  Imb. |  Imb. | Group\n        |         |  Samp | Samp% |  Function=[MAX10]\n        |         |       |       |   PE=HIDE\n 100.0% | 7,355.9 |    -- |    -- | Total\n|-------------------------------------------------------------\n|  67.0% | 4,925.5 |    -- |    -- | USER\n||------------------------------------------------------------\n||  48.7% | 3,582.1 | 338.9 |  8.7% | genral_\n||   9.9% |   726.7 |  85.3 | 10.5% | xyzint_\n||   6.5% |   481.6 |  80.4 | 14.3% | rt123_\n||============================================================\n|  21.5% | 1,580.0 |    -- |    -- | BLAS\n||------------------------------------------------------------\n||   8.6% |   634.0 |  70.0 | 10.0% | gotoblas_dgetrf_single_knl\n||   5.7% |   417.1 |  64.9 | 13.5% | gotoblas_dgemm_kernel_knl\n||   2.3% |   168.5 |  34.5 | 17.0% | gotoblas_dlaswp_plus_knl\n||   2.0% |   143.8 |  34.2 | 19.2% | gotoblas_dgemv_n_knl\n||============================================================\n|   8.7% |   637.3 |    -- |    -- | MPI\n||------------------------------------------------------------\n||   8.3% |   610.0 | 443.0 | 42.2% | MPI_ALLREDUCE\n||============================================================\n|   2.8% |   206.5 |    -- |    -- | ETC\n||------------------------------------------------------------\n||   2.6% |   193.8 |  60.2 | 23.7% | __svml_exp8_mask_b3\n|=============================================================\n</code></pre> By default, the sampling rate for CrayPat-lite is 100 times per second (or once every 104 microseconds) (check the runtime environment variable <code>PAT_RT_SAMPLING_INTERVAL</code> (given in microseconds)to verify the sampling rate).</p> <p>Samp% is the percent of total samples taken which occurred in the given routine, averaged over all processes.</p> <p>Samp is the number of samples which occurred in the given routine, averaged over all processes.</p> <p>Imb. Samp is (maximum number of samples taken in given routine by one process - number of samples taken in given routine, averaged over all processes).</p> <p>Imb. Samp% is (Imb. Samp) / (maximum number of samples taken in given routine by one process) * (number of processes / (number of processes - 1)) * 100%</p> <p>Based on the above, 48.7% of total samples occurred in the genral_ routine, averaged over all processes, and the most samples taken in genral_ by one process process differed from the average by 338.9 samples. (If it was perfectly load balanced, Imb. Samp would be 0.) Additionally, 21.1% samples occurred in BLAS routines, averaged over all processes.</p> <p>We can also see that this was run on 8 KNL nodes with 64 MPI ranks/node, for a total of 512 MPI processes.</p> <p>Details explaining the output and how to get more information can be found in the references, training assets and the man page (above).</p>"},{"location":"theta/performance-tools/craypat/#craypat_1","title":"CrayPat","text":"<p>CrayPat supports two categories of performance analysis experiments: tracing experiments, which count some event such as the number of times a specific system call is executed, and asynchronous (sampling) experiments, which capture values at specified time intervals or when a specified counter overflows.</p> <p>1. Environmental Setup To use CrayPat, unload darshan module and load perftools-base and perftools modules.  <pre><code>jkwack@thetamom2:~/CrayPat_Test&gt; module unload darshan\njkwack@thetamom2:~/CrayPat_Test&gt; module load perftools-base perftools\n</code></pre> 2. Build your application and instrument it with pat_build The pat_build command is the instrumenting component of the CrayPat performance analysis tool. After loading the perftools-base and perftools instrumentation modules and recompiling the program, use the pat_build command to instrument the program for data capture. There are several ways to instrument your application as follows:</p> <p>For the default Automatic Profiling Analysis: <code>$ pat_build a.out</code></p> <p>For predefined trace groups: <code>$ pat_build -g tracegroup a.out</code></p> <p>For enabling tracing and the CrayPat API: <code>$ pat_build -w a.out</code></p> <p>For instrumenting a single function: <code>$ pat_build -T tracefunc a.out</code></p> <p>For instrumenting a list of functions: <code>$ pat_build -t tracefile a.out</code></p> <p>The pat_build command is documented in more detail in the pat_build(1) man page. For additional information and examples, see pat_help build.</p> <p>The following is an example on an interactive job mode on Theta: <pre><code>jkwack@thetamom2:~/CrayPat_Test&gt; ftn -dynamic -Ofast -fopenmp test.f90 -o test\nWARNING: PerfTools is saving object files from a temporary directory into directory '/home/jkwack/.craypat/test/70303'\n</code></pre></p> <pre><code>jkwack@thetamom2:~/CrayPat_Test&gt; pat_build test\njkwack@thetamom2:~/CrayPat_Test&gt; ls -al test*\n-rwxr-xr-x 1 jkwack users   46568 Mar  4 19:57 test\n-rw-r--r-- 1 jkwack users    6143 Mar  4 19:57 test.f90\n-rwxr-xr-x 1 jkwack users 1399496 Mar  4 19:59 test+pat\n</code></pre> <p>3. Run your instrumented binary Run your instrumented executable binary ending with +pat with your usual runtime parameters. <pre><code>jkwack@thetamom2:~/CrayPat_Test&gt; aprun -n 64 ./test+pat 4096 100\n\nCrayPat/X:  Version 7.0.5 Revision 82c1110  11/21/18 11:50:49\n\n                      Number of MPI process:     64\n\n                                  Precision: double\n\n       Number of rows/columns of the matrix:   4096\n\n      The highest order of geometric series:    100\n\n                      Number of repetitions:     10\n\n                  Memory Usage per MPI rank:   268.435456 MB\n\n          Warming up ......\n\n          Main Computations  10 repetitions ......\n\n          0%                     25%                      50%                     75%                     100%\n\n          ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\n                        Error_MPI_{Min,Mean,Max}/MPI =   0.7878E-11    0.7878E-11    0.7878E-11\n\n                       GFLOP-rate_{Min,Mean,Max}/MPI =     0.449981      0.451127      0.451217\n\n                                           Wall Time =    74.568634 sec\n\n                                           FLOP-rate =    28.798753 GFLOP/sec\n\nExperiment data directory written:\n\n/gpfs/mira-home/jkwack/CrayPat_Test/test+pat+9755-3826s\n\nApplication 19251348 resources: utime ~5395s, stime ~106s, Rss ~304068, inblocks ~20862, outblocks ~0\n</code></pre></p> <p>4. Post-processing with pat_report The pat_report command is the text reporting component of the Cray Performance Analysis Tools suite. After using the pat_build command to instrument the program, set the run time environment variables as desired, and then execute the program, use the pat_report command to generate text reports from the resulting data and export the data for use in other applications.</p> <p>The pat_report command is documented in detail in the pat_report(1) man page. Additional information can be found in the online help system under pat_help report.</p> <p>The following is for the above example on an interactive mode on Theta: <pre><code>kwack@thetamom2:~/CrayPat_Test&gt; pat_report -o test.CrayPat_report.txt test+pat+9755-3826s\n\nProcessing step 1 of 9\n\nSuggested trace options file:   test+pat+9755-3826s/build-options.apa\n\nProcessing step 9 of 9\n</code></pre></p> <p>See test.CrayPat_report.txt file for the detailed performance data: <pre><code>jkwack@thetamom2:~/CrayPat_Test&gt; cat test.CrayPat_report.txt\n\nCrayPat/X:  Version 7.0.5 Revision 82c1110  11/21/18 11:50:49\n\nNumber of PEs (MPI ranks):   64\n\nNumbers of PEs per Node:     64\n\nNumbers of Threads per PE:    1\n\nNumber of Cores per Socket:  64\n\nExecution start time:  Wed Mar  4 20:03:22 2020\n\nSystem name and speed:  nid03826  1.301 GHz (nominal)\n\nIntel Knights Landing CPU  Family:  6  Model: 87  Stepping:  1\n\nDRAM: 192 GiB DDR4-2400 on 1.3 GHz nodes\n\nMCDRAM: 7.2 GHz, 16 GiB available as quad, cache (100% cache)\n\nCurrent path to data file:\n\n  /home/jkwack/CrayPat_Test/test+pat+9755-3826s   (RTS, 64 data files)\n\nNotes for table 1:\n\n  This table shows functions that have significant exclusive sample\n\n    hits, averaged across ranks.\n\n  For further explanation, see the \"General table notes\" below,\n\n    or use:  pat_report -v -O samp_profile ...\n\nTable 1:  Profile by Function (limited entries shown)\n\n  Samp% |    Samp | Imb. |  Imb. | Group\n\n        |         | Samp | Samp% |  Function\n\n        |         |      |       |   PE=HIDE\n\n100.0% | 8,396.2 |   -- |    -- | Total\n\n|-------------------------------------------\n\n|  99.1% | 8,324.3 | 16.7 |  0.2% | USER\n\n||------------------------------------------\n\n||  99.1% | 8,324.3 | 16.7 |  0.2% | MAIN__\n\n|===========================================\n\nNotes for table 2:\n\n  This table shows functions that have the most significant exclusive\n\n    time, taking the maximum time across ranks and threads.\n\n  For further explanation, see the \"General table notes\" below,\n\n    or use:  pat_report -v -O profile_max ...\n\nTable 2:  Profile of maximum function times (limited entries shown)\n\n  Samp% |    Samp | Imb. |  Imb. | Function\n\n        |         | Samp | Samp% |  PE=[max,min]\n\n|-----------------------------------------------\n\n| 100.0% | 8,341.0 | 16.7 |  0.2% | MAIN__\n\n||----------------------------------------------\n\n|| 100.0% | 8,341.0 |   -- |    -- | pe.0\n\n||  99.6% | 8,308.0 |   -- |    -- | pe.10\n\n||==============================================\n\n|   1.0% |    87.0 | 15.8 | 18.5% | __powidf2\n\n||----------------------------------------------\n\n||   1.0% |    87.0 |   -- |    -- | pe.10\n\n||   0.7% |    60.0 |   -- |    -- | pe.3\n\n|===============================================\n\nNotes for table 3:\n\n  This table shows functions, and line numbers within functions, that\n\n    have significant exclusive sample hits, averaged across ranks.\n\n  For further explanation, see the \"General table notes\" below,\n\n    or use:  pat_report -v -O samp_profile+src ...\n\nTable 3:  Profile by Group, Function, and Line (limited entries shown)\n\n  Samp% |    Samp | Imb. |  Imb. | Group\n\n        |         | Samp | Samp% |  Function\n\n        |         |      |       |   Source\n\n        |         |      |       |    Line\n\n        |         |      |       |     PE=HIDE\n\n100.0% | 8,396.2 |   -- |    -- | Total\n\n|--------------------------------------------------------------------\n\n|  99.1% | 8,324.3 |   -- |    -- | USER\n\n||-------------------------------------------------------------------\n\n||  99.1% | 8,324.3 |   -- |    -- | MAIN__\n\n3|        |         |      |       |  /home/jkwack/CrayPat_Test/test.f90\n\n||||-----------------------------------------------------------------\n\n4|||  95.2% | 7,990.0 | 25.0 |  0.3% | line.180\n\n4|||   2.0% |   168.2 | 28.8 | 14.9% | line.183\n\n|====================================================================\n\nNotes for table 4:\n\n  This table shows HW performance counter data for the whole program,\n\n    averaged across ranks or threads, as applicable.\n\n  For further explanation, see the \"General table notes\" below,\n\n    or use:  pat_report -v -O hwpc ...\n\nTable 4:  Program HW Performance Counter Data (limited entries shown)\n\nPE=HIDE\n\n==============================================================================\n\n  Total\n\n------------------------------------------------------------------------------\n\n  Thread Time                                     84.260105 secs\n\n  UNHALTED_CORE_CYCLES                      116,375,995,355\n\n  UNHALTED_REFERENCE_CYCLES                 108,063,453,270\n\n  INSTRUCTION_RETIRED                        77,007,487,327\n\n  OFFCORE_RESPONSE_0:ANY_REQUEST:DDR             24,281,189\n\n  OFFCORE_RESPONSE_1:ANY_REQUEST:MCDRAM          78,534,610\n\n  CPU_CLK                             1.38GHz               \n\n  CPU CLK Boost                                        1.08 X\n\n  Retired Inst per Clock                               0.66\n\n  Memory traffic GBytes              0.078G/sec        6.58 GB\n\n  MCDRAM Memory traffic GBytes       0.060G/sec        5.03 GB\n\n  DDR Memory traffic GBytes          0.018G/sec        1.55 GB\n\n==============================================================================\n\nNotes for table 5:\n\n  This table show the average time and number of bytes written to each\n\n    output file, taking the average over the number of ranks that\n\n    wrote to the file.  It also shows the number of write operations,\n\n    and average rates.\n\n  For further explanation, see the \"General table notes\" below,\n\n    or use:  pat_report -v -O write_stats ...\n\nTable 5:  File Output Stats by Filename (limited entries shown)\n\n      Avg |      Avg |  Write Rate | Number |    Avg | Bytes/ | File Name=!x/^/(proc|sys)/\n\n    Write |    Write | MiBytes/sec |     of | Writes |   Call |  PE=HIDE\n\nTime per |  MiBytes |             | Writer |    per |        |\n\n   Writer |      per |             |  Ranks | Writer |        |\n\n     Rank |   Writer |             |        |   Rank |        |\n\n          |     Rank |             |        |        |        |\n\n|-----------------------------------------------------------------------------\n\n| 0.000761 | 0.000921 |    1.211195 |      1 |   35.0 |  27.60 | stdout\n\n|=============================================================================\n\nNotes for table 6:\n\n  This table shows energy and power usage for the nodes with the\n\n    maximum, mean, and minimum usage, as well as the sum of usage over\n\n    all nodes.\n\n    Energy and power for accelerators is also shown, if applicable.\n\n  For further explanation, see the \"General table notes\" below,\n\n    or use:  pat_report -v -O program_energy ...\n\nTable 6:\n\n  Program energy and power usage (from Cray PM) (limited entries shown)\n\n   Node |     Node |   Process | PE=HIDE\n\nEnergy |    Power |      Time |\n\n    (J) |      (W) |           |\n\n14,264 |  169.042 | 84.382329 | Total\n\nNotes for table 7:\n\n  This table shows memory traffic to DDR and MCDRAM for numa nodes,\n\n    taking for each numa node the maximum value across nodes. It also\n\n    shows the balance in memory traffic by showing the top 3 and\n\n    bottom 3 node values.\n\n  For further explanation, see the \"General table notes\" below,\n\n    or use:  pat_report -v -O mem_bw_mcdram ...\n\nTable 7:  Memory Bandwidth by Numanode (limited entries shown)\n\n  Memory |     DDR |  MCDRAM |    Thread |  Memory | Numanode\n\nTraffic |  Memory |  Memory |      Time | Traffic |  PE=HIDE\n\n  GBytes | Traffic | Traffic |           |  GBytes |\n\n         |  GBytes |  GBytes |           |   / Sec |\n\n|--------------------------------------------------------------\n\n|  421.13 |   99.46 |  321.68 | 84.492077 |    4.98 | numanode.0\n\n|==============================================================\n\nNotes for table 8:\n\n  This table shows total wall clock time for the ranks with the\n\n    maximum, mean, and minimum time, as well as the average across\n\n    ranks.\n\n    It also shows maximum memory usage from /proc/self/numa_maps for\n\n    those ranks, and on average.  The usage is total size of all\n\n    pages, including huge pages, that were actually mapped into\n\n    physical memory from both private and shared memory segments.\n\n  For further explanation, see the \"General table notes\" below,\n\n    or use:  pat_report -v -O program_time ...\n\nTable 8:  Wall Clock Time, Memory High Water Mark (limited entries shown)\n\n   Process |   Process | PE=[mmm]\n\n      Time |     HiMem |\n\n           | (MiBytes) |\n\n84.382329 |     282.2 | Total\n\n|--------------------------------\n\n| 84.616624 |     284.2 | pe.0\n\n| 84.377615 |     281.8 | pe.31\n\n| 84.345358 |     282.2 | pe.49\n\n|================================\n\n========================  Additional details  ========================\n\nGeneral table notes:\n\n    The default notes for a table do not account for the effects\n\n    of additional command-line options, but the detailed notes\n\n    produced by the -v option do account for them.\n\n    An imbalance metric in a line is based on values in main threads\n\n    across multiple ranks, or on values across all threads, as applicable.\n\n    An imbalance percent in a line is relative to the maximum value\n\n    for that line across ranks or threads, as applicable.\n\nExperiment:  samp_cs_time\n\nSampling interval:  10000 microsecs\n\nOriginal path to data file:\n\n  /gpfs/mira-home/jkwack/CrayPat_Test/test+pat+9755-3826s/xf-files   (RTS, 64 data files)\n\nOriginal program:  /gpfs/mira-home/jkwack/CrayPat_Test/test\n\nInstrumented with:  pat_build test\n\n  Option file \"apa\" contained:\n\n    -Drtenv=PAT_RT_PERFCTR=default_samp\n\n    -Drtenv=PAT_RT_EXPERIMENT=samp_cs_time\n\n    -Drtenv=PAT_RT_SAMPLING_MODE=3\n\n    -g upc\n\n    -g caf\n\n    -g mpi\n\n    -g shmem\n\n    -g syscall\n\n    -g io\n\nInstrumented program:  ./test+pat\n\nProgram invocation:  ./test+pat 4096 100\n\nExit Status:  0 for 64 PEs\n\nMemory pagesize:  4 KiB\n\nMemory hugepagesize:  Not Available\n\nProgramming environment:  GNU\n\nRuntime environment variables:\n\n  CRAYPAT_ALPS_COMPONENT=/opt/cray/pe/perftools/7.0.5/sbin/pat_alps\n\n  CRAYPAT_COMPILER_OPTIONS=1\n\n  CRAYPAT_LD_LIBRARY_PATH=/opt/cray/pe/gcc-libs:/opt/cray/gcc-libs:/opt/cray/pe/perftools/7.0.5/lib64\n\n  CRAYPAT_OPTS_EXECUTABLE=sbin/pat-opts\n\n  CRAYPAT_ROOT=/opt/cray/pe/perftools/7.0.5\n\n  CRAYPE_VERSION=2.6.1\n\n  CRAY_LIBSCI_VERSION=19.06.1\n\n  GCC_VERSION=8.3.0\n\n  GNU_VERSION=8.3.0\n\n  LIBSCI_VERSION=19.06.1\n\n  MODULE_VERSION=3.2.11.3\n\n  MODULE_VERSION_STACK=3.2.11.3\n\n  MPICH_ABORT_ON_ERROR=1\n\n  MPICH_DIR=/opt/cray/pe/mpt/7.7.10/gni/mpich-gnu/8.2\n\n  PAT_BUILD_PAPI_BASEDIR=/opt/cray/pe/papi/5.6.0.5\n\n  PAT_REPORT_PRUNE_NAME=_cray$mt_execute_,_cray$mt_start_,__cray_hwpc_,f_cray_hwpc_,cstart,__pat_,pat_region_,PAT_,OMP.slave_loop,slave_entry,_new_slave_entry,_thread_pool_slave_entry,THREAD_POOL_join,__libc_start_main,_start,__start,start_thread,__wrap_,UPC_ADIO_,_upc_,upc_,__caf_,__pgas_,syscall,__device_stub\n\n  PAT_RT_EXPERIMENT=samp_cs_time\n\n  PAT_RT_PERFCTR=default_samp\n\n  PAT_RT_SAMPLING_MODE=3\n\n  PERFTOOLS_VERSION=7.0.5\n\n  PMI_FORK_RANK=0\n\n  PMI_GNI_COOKIE=4202364928:4202430464\n\n  PMI_GNI_DEV_ID=0:0\n\n  PMI_GNI_LOC_ADDR=4978:4978\n\n  PMI_GNI_PTAG=24:25\n\nReport time environment variables:\n\n    CRAYPAT_ROOT=/opt/cray/pe/perftools/7.0.5\n\n    PAT_REPORT_PRUNE_NAME=_cray$mt_execute_,_cray$mt_start_,__cray_hwpc_,f_cray_hwpc_,cstart,__pat_,pat_region_,PAT_,OMP.slave_loop,slave_entry,_new_slave_entry,_thread_pool_slave_entry,THREAD_POOL_join,__libc_start_main,_start,__start,start_thread,__wrap_,UPC_ADIO_,_upc_,upc_,__caf_,__pgas_,syscall,__device_stub\n\nNumber of MPI control variables collected:  108\n\n  (To see the list, specify: -s mpi_cvar=show)\n\nReport command line options:  -o test.CrayPat_report.txt\n\nOperating system:\n\n  Linux 4.4.103-6.38_4.0.154-cray_ari_c #1 SMP Fri Aug 30 18:08:26 UTC 2019 (0e37142)\n\nHardware performance counter events:\n\n   UNHALTED_CORE_CYCLES                   Unhalted core cycles\n\n   UNHALTED_REFERENCE_CYCLES              Unhalted reference cycle\n\n   INSTRUCTION_RETIRED                    Instructions retired (any thread modifier supported in fixed counter)\n\n   OFFCORE_RESPONSE_0:ANY_REQUEST:DDR     Offcore response event (must provide at least one request type andeither any_response or any combination of supplier + snoop):Counts any request:accounts for responses from DDR (local and far)\n\n   OFFCORE_RESPONSE_1:ANY_REQUEST:MCDRAM  Offcore response event (must provide at least one request type andeither any_response or any combination of supplier + snoop):Counts any request:accounts for responses from MCDRAM (local and far)\n</code></pre></p> <p>5. In-depth Analysis: Using Cray Apprentice2 Cray Apprentice2 is a GUI tool for visualizing and manipulating the performance analysis data captured during program execution. After using pat_report to open the initial .xf data file(s) and generate the .ap2 file(s), use Cray Apprentice2 to open and explore the .ap2 file(s) in further detail.</p> <p>The following is an example on a login node on Theta: <pre><code>jkwack@thetalogin5:~&gt; module unload darshan\njkwack@thetalogin5:~&gt; module load perftools-base perftools\njkwack@thetalogin5:~&gt; app2\n</code></pre></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"theta/performance-tools/darshan/","title":"Darshan","text":""},{"location":"theta/performance-tools/darshan/#introduction","title":"Introduction","text":"<p>Darshan is a lightweight I/O instrumentation library that can be used to investigate the I/O behavior of production applications. It records statistics, such as the number of files opened, time spent performing I/O, and the amount of data accessed by an application.</p>"},{"location":"theta/performance-tools/darshan/#references","title":"References","text":"<ul> <li>Darshan Project Site</li> <li>Guide to Darshan-util Usage</li> </ul>"},{"location":"theta/performance-tools/darshan/#overview","title":"Overview","text":"<p>The Theta environment includes the Darshan module by default. <pre><code>$ module list 2&gt;&amp;1 | grep darshan\n21) darshan/3.1.4\n</code></pre> In most cases, no additional steps are needed to enable Darshan instrumentation. Code compiled with the Cray compiler wrappers {cc, CC, ftn} will include the Darshan library by default. Dynamically linked applications are the most notable exception. See the \u201cDynamic Linking\u201d section later in this document for instructions on how to enable Darshan instrumentation if you plan to use dynamic libraries.</p> <p>When a Darshan-enabled job completes, it will generate a single output file containing I/O characterization results. Each output file is placed in the following directory based on the start time of the job: <code>/lus/theta-fs0/logs/darshan/theta/&lt;YEAR&gt;/&lt;MONTH&gt;/&lt;DAY&gt;</code></p> <p>The name of the output file will be in the format: <code>&lt;USERNAME&gt;_&lt;BINARY_NAME&gt;_id&lt;COBALT_JOB_ID&gt;_&lt;DATE&gt;-&lt;UNIQUE_ID&gt;_&lt;TIMING&gt;.darshan</code></p> <p>A graphical summary of I/O behavior can be generated using the darshan-job-summary.pl utility. The utility should be available in your default path, but if not, it can be loaded using the module command: <pre><code>$ module load darshan\n</code></pre> The following example shows how to execute the utility: <pre><code>$ darshan-job-summary.pl /lus/theta-fs0/logs/darshan/theta/carns_my-app_id114525_7-27-58921_19.darshan --output ~/job-summary.pdf\n</code></pre> The entire contents of the output file can be translated into text format for more detailed analysis using the following command: <pre><code>$ darshan-parser /lus/theta-fs0/logs/darshan/theta/carns_my-app_id114525_7-27-58921_19.darshan &gt; ~/job-characterization.txt\n</code></pre></p> <p>Note: The resulting text file will be verbose. To interpret its contents, use the guidelines in the Guide to Darshan-parser Output.</p>"},{"location":"theta/performance-tools/darshan/#dynamic-linking","title":"Dynamic Linking","text":"<p>Darshan can also be used with applications that have been dynamically linked, but in this case you must set explicit environment variables in your job script and your qsub command in order to enable Darshan. See the following example. The DARSHAN_PRELOAD variable will be set automatically when the Darshan module is loaded; the commands below just relay it to the application runtime environment.</p> <pre><code># job_script.sh\naprun \u2013n &lt;n&gt; -N &lt;N&gt; -e LD_PRELOAD=$DARSHAN_PRELOAD &lt;binary&gt; &lt;args&gt;\n\n$ qsub &lt;..&gt; --env DARSHAN_PRELOAD=$DARSHAN_PRELOAD job_script.sh\n</code></pre>"},{"location":"theta/performance-tools/darshan/#possible-reasons-for-missing-output-files","title":"Possible Reasons for Missing Output Files","text":"<p>Darshan will not produce output files in the following scenarios:</p> <ul> <li>Use of languages besides C, C++, or FORTRAN</li> <li>Use of non-standard MPI libraries or linkers</li> <li>Use of other MPI profilers that conflict with Darshan</li> <li>Use of dynamic linking without using LD_PRELOAD</li> <li>Job did not call MPI_Finalize(). Reasons may include:</li> <li>Job hit wall time limit</li> <li>Abnormal termination</li> <li>The executable is not an MPI program</li> </ul> <p>In such cases, contact ALCF Support for help. Depending on your situation, it may still be possible to use Darshan.</p>"},{"location":"theta/performance-tools/darshan/#disabling-darshan","title":"Disabling Darshan","text":"<p>We do not recommend disabling Darshan unless you have a specific problem or have been instructed by the ALCF support team to do so. Disabling Darshan limits the ALCF\u2019s ability to assist in supporting your application, and Darshan instrumentation does not add significant overhead to execution time.</p>"},{"location":"theta/performance-tools/darshan/#disabling-at-compile-time","title":"Disabling at Compile Time","text":"<p>The Darshan module can be unloaded, and when an application is linked, the intercept library will no longer be included. <pre><code>$ module unload darshan\n$ make\n</code></pre></p>"},{"location":"theta/performance-tools/darshan/#disabling-at-runtime","title":"Disabling at Runtime","text":"<p>Darshan can be disabled by setting the DARSHAN_DISABLE=1 environment variable on the aprun command. This does not require relinking the application, and Darshan can be deactivated on a case-by-case basis for existing executables.</p> <pre><code># job_script.sh\naprun \u2013n &lt;n&gt; -N &lt;N&gt; -e DARSHAN_DISABLE=1 &lt;binary&gt; &lt;args&gt;\n</code></pre>"},{"location":"theta/performance-tools/hpctoolkit/","title":"HPCToolkit on Theta","text":""},{"location":"theta/performance-tools/hpctoolkit/#introduction","title":"Introduction","text":"<p>HPCToolkit is an open-source suite of tools for profile-based performance analysis of applications. Below is a brief description on how to use the HPCToolkit on the ALCF XC40 system. For more detailed information on using HPCToolkit and its capabilities, see the HPCToolkit website.</p>"},{"location":"theta/performance-tools/hpctoolkit/#references","title":"References","text":"<p>HPCToolkit Website</p> <p>HPCT Documentation Page</p>"},{"location":"theta/performance-tools/hpctoolkit/#installation-on-the-alcf-theta-system","title":"Installation on the ALCF Theta System","text":"<p>The HPCToolkit is installed in: <pre><code>/projects/Tools/hpctoolkit/pkgs-theta/hpctoolkit\n</code></pre></p>"},{"location":"theta/performance-tools/hpctoolkit/#using-hpctoolkit","title":"Using HPCToolkit","text":""},{"location":"theta/performance-tools/hpctoolkit/#environment-setup","title":"Environment Setup","text":"<p>To avoid conflicts with the default loaded I/O performance monitoring tool, unload the darshan moduile: <pre><code>module unload darshan\n</code></pre> Add the HPCToolkit bin directory to your path as follows: <pre><code>export PATH=$PATH:/projects/Tools/hpctoolkit/pkgs-theta/hpctoolkit/bin (bash) \nsetenv PATH $PATH:/projects/Tools/hpctoolkit/pkgs-theta/hpctoolkit/bin (csh)\n</code></pre></p>"},{"location":"theta/performance-tools/hpctoolkit/#compile-the-code-to-use-hpctoolkit","title":"Compile the Code to Use HPCToolkit","text":"<p>It is not necessary to modify your source code to use the HPCToolkit. However, for statically linked executables; the executable must be built using the HPCToolkit hpclink command as the linker.</p> <p>The build procedure when using HPCToolkit is: - Compile the individual source files as usual, adding the flag for debugging symbols if not already used. For example: <pre><code>cc -g -O3 -c routine1.c \nftn -g -O3 -c routine2.for\n</code></pre> This example shows compilation using a medium level of optimization (-O3). Any level of optimization is supported by HPCToolkit. However, to avoid possible problems, programs should be compiled without inter-procedural optimization (IPA) by specifying the \"-qnoipa\" option.</p> <ul> <li>Link using the hpclink command before the name of the compiler normally used to link the program. For example: <pre><code>hpclink cc -o myprog routine1.o routine2.o ...\n</code></pre> Note: The link step may take longer than using the usual linker as additional steps are performed during the linking process.</li> </ul>"},{"location":"theta/performance-tools/hpctoolkit/#run-the-code","title":"Run the Code","text":"<p>To run the program, submit to the queues as usual (using the executable built with the hpclink command) but set the HPCT environment variables (HPCRUN_TRACE, HPCRUN_EVENT_LIST, HPCRUN_PROCESS_FRACTION) to specify the data to be collected. For example: <pre><code>$ qsub -n 8 -t 30 -A project jobscript.sh\n$ cat jobscript.sh \n\n#!/bin/bash \nexport HPCRUN_EVENT_LIST=\"REALTIME@10000 PAPI_TOT_CYC@14000000 CYCLES@f100\" \nexport HPCRUN_TRACE=1 \naprun -n 512 -N 64 &lt;executable&gt;\n</code></pre> Possible profiling events include:</p> <p>Time - profile the program using a time interval. Time-based profiling is specified by using the event: - <code>REALTIME@&lt;value&gt;</code> - profile based on wallclock time with a sampling interval  in micro-seconds   Hardware Performance Counter Events - profile based on hardware events accessed through PAPI. Lists of the available PAPI standard and native events can be found by running <code>/opt/cray/pe/papi/5.6.0.4/bin/papi_avail and /opt/cray/pe/papi/5.6.0.4/bin/papi_native_avail</code> <p>Setting the environment variable HPCRUN_PROCCESS_FRACTION will limit the number of process that write HPCToolkit output files to the specified fraction of processes. For runs using greater than 50-100 nodes it recommended that a fraction be set to limit the volume of output data.</p> <p>HPCToolkit supports gathering of trace information, tracing can produce large volumes of information and it is recommended that it be performed only after profile data has been successfully gathered and reviewed. Tracing is off by default and may be enabled by adding the additional the environment variable:  - HPCRUN_TRACE=1 (enabled)</p> <p>A few notes on profiling events and options:  - Use a space-separated list to sample multiple events in the same run.   After execution, a directory <code>hpctoolkit-myprogram-measurements-XXXXXX/</code> will have been created containing the collected performance data.</p>"},{"location":"theta/performance-tools/hpctoolkit/#analyze-the-binary-and-correlate-with-performance-data","title":"Analyze the Binary and Correlate with Performance Data","text":"<p>Before looking at the gathered performance data, it is also necessary to gather data from the executable by running the hpcstruct command on the executable used to gather the performance data. To do this, simply run the hpcstruct command followed by the program name. For example: <pre><code>hpcstruct --loop-fwd-subst=no &lt;executable&gt;\n</code></pre> This step can be carried out at any point after the executable is built with the hpclink command: before, after, or during the program execution. The hpcstruct command will produce the file myprogram.hpcstruct. Note that this step may take a little while depending on the size of the binary file.</p> <p>Once the profile data has been gathered by running the program and the information from the executable has been gathered using the hpcstruct the two sets of data need to be correlated by running the hpcprof command. </p> <p>An example of this would be: <pre><code>hpcprof -S myprogram.hpcstruct -I path-to-myprogram-src/+ \\ \nhpctoolkit-myprogram-measurements-XXXXXX\n</code></pre> If the source is in multiple files, the specification of the path to the source should be given in the form \"path-to-myprogram-src/+\". Multiple -I source paths can be specified.</p> <p>If a large number of profile files have been generated (for instance, if the code runs on many cores), it is best to select only a subset of profile files to view. </p> <p>This can be done as: <pre><code>hpcprof -S myprogram.hpcstruct -I /home/mydir/myprogram-dir/+ \\ \nhpctoolkit-myprogram-measurements-XXXXXX/{myprogram-000446-000-83cac10-546.hpcrun,myprogram-000328-000-83cac10-428.hpcrun}\n</code></pre></p> <p>where the brackets {} contain a list of individual profile files from specific MPI ranks. The hpcsummary can be used to see summary information about individual profile files to help you decide which ones to include.</p> <p>Running the hpcprof command will produce a database hpctoolkit-myprogram-database containing all of the information a viewer needs.</p>"},{"location":"theta/performance-tools/hpctoolkit/#view-the-performance-data","title":"View the Performance Data","text":"<p>The performance data may be viewed using the hpcviewer command. <pre><code>hpcviewer hpctoolkit-myprogram-database\n</code></pre> The hpcviewer command may be run on the ALCF system with the display exported back to your local machine (login should be with ssh -X), or the hpcviewer program may be installed on your local machine (see HPCToolkit website) and the results database downloaded to your local machine and viewed locally.</p> <p>If trace data was collected (with HPCRUN_TRACE=1), the program hpctraceviewer may be used to view the trace data: <pre><code>hpctraceviewer\n</code></pre></p>"},{"location":"theta/performance-tools/hpctoolkit/#resources","title":"Resources","text":""},{"location":"theta/performance-tools/intel-advisor/","title":"Intel Advisor","text":""},{"location":"theta/performance-tools/intel-advisor/#introduction","title":"Introduction","text":"<p>Advisor is an advanced profiling tool which helps you to optimize your code on KNL architecture. It allows you to analyze the vectorization efficiency of your code and compares your code\u2019s performance to the theoretical limits of the hardware. See [1] for more details on the roofline model.</p>"},{"location":"theta/performance-tools/intel-advisor/#step-by-step-guide","title":"Step-by-step guide","text":"<ol> <li>Build your target application with all optimizations enabled e.g. <code>-O3 -xMIC-AVX512</code> and <code>set debugging -g flag</code>.</li> <li>Submit your job using sample batch script:</li> </ol> <p><pre><code>#!/bin/bash\n#COBALT -t 60\n#COBALT -n 1\n#COBALT -A Intel\n#COBALT -q debug-flat-quad\n# COBALT -q debug-cache-quad\n\n# Job Size\nexport n_nodes=$COBALT_JOBSIZE\nexport n_mpi_ranks_per_node=1\nexport n_mpi_ranks=$(($n_nodes * $n_mpi_ranks_per_node))\nexport n_openmp_threads_per_rank=1\nexport n_hyperthreads_per_core=1\nexport n_hyperthreads_skipped_between_ranks=1\n\n# Make a temp external wrapper\necho \"#!/bin/bash\" &gt;&gt; profile1.sh\necho \"export PE_RANK=\\$ALPS_APP_PE\" &gt;&gt; profile1.sh\necho \"export PMI_NO_FORK=1\" &gt;&gt; profile1.sh\n# if you want to profile a rank other than 0 change here\necho \"if [ \"\\$PE_RANK\" == 0 ];then\" &gt;&gt; profile1.sh\necho \"\\$1 -- \\$2\" &gt;&gt; profile1.sh\necho \"else\" &gt;&gt; profile1.sh\necho \"\\$2\" &gt;&gt; profile1.sh\necho \"fi\" &gt;&gt; profile1.sh\necho \"\" &gt;&gt; profile1.sh\nchmod 744 ./profile1.sh\n\n# OpenMP Settings\nexport OMP_NUM_THREADS=$n_openmp_threads_per_rank\nexport OMP_AFFINITY=compact,granularity=core\nexport OMP_STACKSIZE=16G\n# Big stacks to prevent segfaults and disable DARSHAN IO profiling\nulimit -s unlimited\nexport DARSHAN_DISABLE=1\nexport PMI_NO_FORK=1\n\n# Setup Intel Advisor\nmodule swap intel/18.0.0.128 intel/19.0.3.199\n\n# Binary name and directory to be used in naming result dir\nBIN=$1\nBINDIR=$(dirname $(realpath $1))\nBINNAME=$(basename $BIN)\necho \"using $BINNAME  at $BIN\"\n# If 2nd argument is provided, use it to better idenfity result\nif [ $# -eq 2 ]; then\n    INPUT=$2\n    INPUTNAME=$(basename $2)\n    echo \"with $INPUTNAME at $BIN\"\nfi\n\n# Time stamp down to the minute - useful when binary names don't change\nT=$(date +%F-%H-%M)\n\n# Modify/Add Additional search paths to locate sources\nSEARCH=\" --search-dir src:=${BINDIR} \"\nSEARCH+=\" --search-dir bin:=${BINDIR} \"\nRESDIR=\" advixe_${BINNAME}_${INPUTNAME}_${T} \"\n\n# Set job size and run. Line # 68 to profile rank 0, Line # 69 to profile all ranks\naprun -n $n_mpi_ranks -N $n_mpi_ranks_per_node \\\n      -cc depth \\\n      -d $n_hyperthreads_skipped_between_ranks \\\n      -j $n_hyperthreads_per_core \\\n      ./profile1.sh \"advixe-cl -c survey --project-dir ${RESDIR} ${SEARCH}\" \"${BIN} ${INPUT}\"\n      #advixe-cl -c survey --project-dir ${RESDIR} ${SEARCH} -- ${BIN} ${INPUT}\naprun -n $n_mpi_ranks -N $n_mpi_ranks_per_node \\\n      -cc depth \\\n      -d $n_hyperthreads_skipped_between_ranks \\\n      -j $n_hyperthreads_per_core \\\n      ./profile1.sh \"advixe-cl -c tripcounts -flop --project-dir ${RESDIR} ${SEARCH}\" \"${BIN} ${INPUT}\"\n      #advixe-cl -c tripcounts -flops --project-dir ${RESDIR} ${SEARCH} -- ${BIN} ${INPUT}\n\n# Save the cobalt files along with result \nmv $COBALT_JOBID.* $RESDIR\n\nrm ./profile1.sh\n</code></pre>   3. The survey results will be located in  after the Advisor run has been completed.    4. Connect to Theta with X-11 forwarding enabled and launch the Advisor GUI with advixe-gui to view your results.    5. Click \u201cShow My Result\u201d and the survey data will load.    6. There is a plethora of information here on how to analyze survey data."},{"location":"theta/performance-tools/intel-advisor/#additional-collections","title":"Additional collections:","text":"<p>There are three other types of collections that can be performed with Advisor for more advanced analysis: tripcounts, memory access pattern, and dependencies. This data can be collected by changing survey to tripcounts, map, and dependencies. The roofline data can be obtained from collecting the survey and tripcounts data back-to-back or by changing survey to roofline.</p>"},{"location":"theta/performance-tools/intel-advisor/#additional-information","title":"Additional Information:","text":"<p>There are many command line options. See [2] for more details on all of the options, and its more comprehensive user guide also available on Intel\u2019s website. - [1] Williams, Samuel, Andrew Waterman, and David Patterson. \"Roofline: an insightful visual performance model for multicore architectures.\" Communications of the ACM 52.4 (2009): 65-76. - [2] Intel. \u201cGet Started with Intel Advisor.\u201d Intel\u00ae Software, Intel, 18 Oct. 2018</p>"},{"location":"theta/performance-tools/intel-vtune/","title":"Intel Vtune","text":"<p>VTune is an advanced profiling tool which helps you to optimize your code for various architectures. It allows you to track how well your code is threaded and vectorized to take advantage of multiple CPUs/FPUs and how well the code is utilizing the non-uniform memory architecture and caches.</p> <p>VTune is a core and node-level profiler. In general, it is not a good to try to profile many ranks. As such this tutorial show you how to setup and run profiling on a single or a small number of ranks. </p>"},{"location":"theta/performance-tools/intel-vtune/#step-by-step-guide","title":"Step-by-step guide","text":"<ol> <li>Build your target application with all optimizations enabled e.g. -O3 -xMIC-AVX512 and enable debug symbols and dynamic linking: -g -dynamic    Note: -dynamic enables dynamic linking. Required for SW sampling, optionalfor HW sampling, but always recommended.  </li> <li>Copy and modify sample batch script <code>/soft/perftools/intel/vtune/amplxe.qsub</code></li> </ol> <p>You need to at least change the project name to your allocation and add any environment variables specific to your application. After that you can submit your job as follows:</p> <p><code>qsub ./amplxe.qsub ./myBinary ./inputfile</code></p> <pre><code>#!/bin/bash\n#COBALT -t 60\n#COBALT -n 1\n#COBALT -A Intel\n#COBALT -q debug-flat-quad\n# COBALT -q debug-cache-quad\n\n# Job Size\nexport n_nodes=$COBALT_JOBSIZE\nexport n_mpi_ranks_per_node=1\nexport n_mpi_ranks=$(($n_nodes * $n_mpi_ranks_per_node))\nexport n_openmp_threads_per_rank=1\nexport n_hyperthreads_per_core=1\nexport n_hyperthreads_skipped_between_ranks=1\n\n# Make a temp external wrapper\necho \"#!/bin/bash\" &gt;&gt; profile1.sh\necho \"export PE_RANK=\\$ALPS_APP_PE\" &gt;&gt; profile1.sh\necho \"export PMI_NO_FORK=1\" &gt;&gt; profile1.sh\n# if you want to profile a rank other than 0 change here\necho \"if [ \"\\$PE_RANK\" == 0 ];then\" &gt;&gt; profile1.sh\necho \"\\$1 -- \\$2\" &gt;&gt; profile1.sh\necho \"else\" &gt;&gt; profile1.sh\necho \"\\$2\" &gt;&gt; profile1.sh\necho \"fi\" &gt;&gt; profile1.sh\necho \"\" &gt;&gt; profile1.sh\nchmod 744 ./profile1.sh\n\n# OpenMP Settings\nexport OMP_NUM_THREADS=$n_openmp_threads_per_rank\nexport OMP_AFFINITY=compact,granularity=core\nexport OMP_STACKSIZE=16G\n# Big stacks to prevent segfaults and disable DARSHAN IO profiling\nulimit -s unlimited\nexport DARSHAN_DISABLE=1\nexport PMI_NO_FORK=1\n\n# Setup Intel Vtune\nmodule load vtune\n\n# Binary name and directory to be used in naming result dir\nBIN=$1\nBINDIR=$(dirname $(realpath $1))\nBINNAME=$(basename $BIN)\necho \"using $BINNAME  at $BIN\"\n# If 2nd argument is provided, use it to better idenfity result\nif [ $# -eq 2 ]; then\n    INPUT=$2\n    INPUTNAME=$(basename $2)\n    echo \"with $INPUTNAME at $BIN\"\nfi\n\n# Time stamp down to the minute - useful when binary names don't change\nT=$(date +%F-%H-%M)\n\n# Modify/Add Additional search paths to locate sources\nSEARCH=\" --search-dir src:=${BINDIR} \"\nSEARCH+=\" --search-dir bin:=${BINDIR} \"\nRESDIR=\" amplxe_${BINNAME}_${INPUTNAME}_${T} \"\n\n# Set job size and run. Line # 68 to profile rank 0, Line # 69 to profile all ranks\naprun -n $n_mpi_ranks -N $n_mpi_ranks_per_node \\\n      -cc depth \\\n      -d $n_hyperthreads_skipped_between_ranks \\\n      -j $n_hyperthreads_per_core \\\n      ./profile1.sh \"amplxe-cl -c hotspots -r ${RESDIR} ${SEARCH}\" $@\n      #amplxe-cl -c hotspots -r ${RESDIR} ${SEARCH} -- $@\n\n# Save the cobalt files along with result \nmv $COBALT_JOBID.* $RESDIR\n\nrm ./profile1.sh\n</code></pre> <p>Note: Make sure to add source and binary search direction. Inclusion of these will allow you to view the sources files in the amplxe-gui, in addition to providing execution time costs line-by-line. IMPORTANT: Use the \u201crealpath\u201d command to specify these, as sometimes Vtune sees the realbath and not the user alias. </p> <p>Example: <pre><code>$&gt; pwd\n/home/pvelesko/projects/distress\n$&gt; realpath `pwd`\n/lus/theta-fs0/projects/intel/pvelesko/distress\n</code></pre> There are many command line options such as the following (amplxe-cl -h collect).   - uarch-exploration - Microarchitecture Exploration   - hpc-performance - HPC Performance Characterization   - io - Input and Output</p> <p>Results will be collected in a directory called amplxe_EXE_TIMESTAMP. It is recommended to add the \u2013no-auto-finalize flag to collections that will be creating large results. The finalization step is compute intensive and runs serially which may take a long time on the KNL. Finalization can be done on another machine after copying the results off the KNL. The data collected may be very large for longer runs with many threads active. If you find that you are reaching the data limit, use the flag -data-limit=. The default limit is 500MB. The integer specifies the size in MB. Use \u2013data-limit=0 for no limit. <ul> <li>The script above is setup for profile only rank 0</li> <li>You can modify the login on line #19 to include additional ranks.  </li> <li>If you wish to profile all ranks, use line # 69 instead of line # 68</li> </ul> <p>After step 2 has been completed, i.e. a results file has been created you can conveniently finalize the results by doing the following: <code>amplxe-cl -finalize -r &lt;vtune-result-dir&gt; -search-dir ./</code></p> <ul> <li>The finalized results can be examined in either the GUI or the command line interface.</li> <li>To examine the results using the GUI interface do the following:</li> <li>Copy the results directory to a machine of your choice (on which you have already installed the VTune GUI)</li> <li>Launch the GUI</li> <li>Click on the \u201cOpen Result\u201d link.While the GUI is very convenient, the command line interface provides a quick way to generate reports directly on Theta.   <code>amplxe-cl -report &lt;report-type&gt; -r &lt;vtune-result-dir&gt; [report-options]</code></li> </ul>"},{"location":"theta/performance-tools/intel-vtune/#command-line-options-and-help-system","title":"Command line options and Help system","text":"<p>After setting up the VTune environment variables you can view the set of options using 'amplxe-cl -h' The available actions are:   - collect   - collect-with   - command   - finalize   - help   - import   - report   - version</p> <p>Type <code>'amplxe-cl -help &lt;action&gt;'</code> for help on a specific action.</p>"},{"location":"theta/performance-tools/intel-vtune/#analysis-types-in-vtune","title":"Analysis types in VTune","text":"<p>Intel\u00ae VTune Analyzer supports two types of profiling: Time Based profiling (SW) and Event Based profiling (HW). Time based profiling utilizes a system clock and reports how time is spent in various parts of a program. This is the \u201ctraditional\u201d method of profiling. Event Based profiling utilizes hardware counters to count the number of events generated by various parts of a program. Events one may want to track are, for example, cache hits and cache misses at various levels of cache.</p> <p>VTune organizes its various analysis types into templates. The templates \u201cConcurrency\u201d and \u201cLocks and Waits\u201d are time-based analyses. Hotspots is SW based by default but can use HW sampling using -knob sampling-mode=hw switch. The rest of the templates are Event Based analyses.  The most common event-based analysis one can run is called \u201cuarch-exploration\u201d. This analysis tracks most of the available HW counters. Because only a finite number of hardware counters can be collected, Vtune uses time multiplexing and as such, the job should run for over 5 minutes or so for the results to be accurate. Users new to Event Based analysis can start with \u201chotspots -knob sampling-mode=hw\u201d or jump right into \u201cuarch-exploration. A short description is provided below. For more details the user should consult the documentation system built into the tool. All VTune documentation is also available online: https://software.intel.com/en-us/vtune-amplifier-help.</p>"},{"location":"theta/performance-tools/intel-vtune/#hotspots-hw-analysis","title":"Hotspots HW analysis","text":"<p>The Hotspots HW Analysis will show where your application is spending its time, including information related to OpenMP parallelism. Use the Bottom-up view in the GUI to see time spent at various granularities; for example, Function or Module granularities. This can be changed in the Grouping drop-down menu. Focus tuning efforts on the hot portions of your application.</p>"},{"location":"theta/performance-tools/intel-vtune/#uarch-exploration","title":"uArch Exploration","text":"<p>Most useful for identifying sections of code with high CPI (cycles per instruction) and gathering L1 and L2 cache hit/miss ratios. KNL supports 512-bit vector instructions. To optimize for KNL, an application should take advantage of these large vector units with heavily vectorized code. Look at the metric VPU Utilization to determine the areas of high and low vectorization. The VPU Utilization metric is also available in the Bottom-up view of the General Exploration viewpoint. Locate hotspots with low VPU Utilization and try to improve their usage of the AVX512 capabilities.</p>"},{"location":"theta/performance-tools/papi/","title":"PAPI on Theta","text":""},{"location":"theta/performance-tools/papi/#introduction","title":"Introduction","text":"<p>The Performance Application Programming Interface (PAPI) provides a standardized user level API for accessing processor and other system programmer counter information (such as instruction counts, cache misses, etc.).</p> <p>PAPI details are provided on the PAPI Website.</p>"},{"location":"theta/performance-tools/papi/#references","title":"References","text":"<ul> <li>PAPI Website</li> <li>PAPI Documentation</li> </ul>"},{"location":"theta/performance-tools/papi/#papi-on-theta_1","title":"PAPI on Theta","text":"<p>PAPI is available on Theta through the \u201cpapi\u201d or \u201cperftools-base\u201d modules.</p> <p>The preset and native events available on Theta may be listed by running the 'papi_avail' and 'papi_native_avail' utilities on a compute node via the qsub command. Accessing native events requires using the event names reported by 'papi_native_avail' as a string and converting them into an event code with the function PAPI_event_name_to_code().</p>"},{"location":"theta/performance-tools/papi/#papi-via-papi-module-on-theta","title":"PAPI via papi module on Theta","text":"<pre><code>jkwack@thetalogin5:~&gt; qsub -I -n 1 -t 60 -q debug-cache-quad -A Performance\n\nConnecting to thetamom3 for interactive qsub...\n\nJob routed to queue \"debug-cache-quad\".\n\nMemory mode set to cache quad for queue debug-cache-quad\n\nWait for job 414557 to start...\n\nOpening interactive session to 3834\n\njkwack@thetamom3:~&gt; module load papi\n\njkwack@thetamom3:~&gt; module show papi\n\n-------------------------------------------------------------------\n\n/opt/cray/pe/modulefiles/papi/5.7.0.2:\n\nconflict papi\n\nconflict perftools\n\nconflict perftools-lite\n\nconflict perftools-lite-events\n\nconflict perftools-lite-gpu\n\nconflict perftools-lite-hbm\n\nconflict perftools-lite-loops\n\nconflict perftools-nwpc\n\nconflict perftools-preload\n\nconflict perftools-base\n\nprepend-path PE_PKGCONFIG_LIBS papi:pfm\n\nprepend-path PKG_CONFIG_PATH /opt/cray/pe/papi/5.7.0.2/lib64/pkgconfig\n\nprepend-path PE_PKGCONFIG_PRODUCTS PE_PAPI\n\nsetenv PE_PAPI_PKGCONFIG_VARIABLES PE_PAPI_ACCEL_LIBS_@accelerator@ PE_PAPI_ACCEL_FAMILY_LIBS_@accelerator_family@\n\nsetenv PE_PAPI_ACCEL_LIBS_nvidia35 ,-lcupti,-lcudart,-lcuda\n\nsetenv PE_PAPI_ACCEL_LIBS\n\nsetenv PE_PAPI_ACCEL_FAMILY_LIBS_nvidia ,-lcupti,-lcudart,-lcuda\n\nsetenv PE_PAPI_ACCELL_FAMILY_LIBS\n\nprepend-path CRAY_LD_LIBRARY_PATH /opt/cray/pe/papi/5.7.0.2/lib64\n\nprepend-path PATH /opt/cray/pe/papi/5.7.0.2/bin\n\nprepend-path MANPATH /opt/cray/pe/papi/5.7.0.2/share/man\n\nsetenv PAPI_VERSION 5.7.0.2\n\nsetenv LIBPFM_DISABLED_PMUS rapl,fam15h_nb\n\nmodule-whatis PAPI - The Performance API (PAPI) project specifies a standard application programming interface (API) for accessing hardware performance counters available on most Cray systems.\n\n-------------------------------------------------------------------\n\njkwack@thetamom3:~&gt; papi_avail\n\nAvailable PAPI preset and user defined events plus hardware information.\n\n--------------------------------------------------------------------------------\n\nPAPI version             : 5.7.0.2\n\nOperating system         : Linux 4.4.103-6.38_4.0.154-cray_ari_s\n\nVendor string and code   : GenuineIntel (1, 0x1)\n\nModel string and code    : Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHz (79, 0x4f)\n\nCPU revision             : 1.000000\n\nCPUID                    : Family/Model/Stepping 6/79/1, 0x06/0x4f/0x01\n\nCPU Max MHz              : 2101\n\nCPU Min MHz              : 1200\n\nTotal cores              : 72\n\nSMT threads per core     : 2\n\nCores per socket         : 18\n\nSockets                  : 2\n\nCores per NUMA region    : 36\n\nNUMA regions             : 2\n\nRunning in a VM          : no\n\nNumber Hardware Counters : 11\n\nMax Multiplex Counters   : 384\n\nFast counter read (rdpmc): no\n\n--------------------------------------------------------------------------------\n\n================================================================================\n\n  PAPI Preset Events\n\n================================================================================\n\n    Name        Code    Avail Deriv Description (Note)\n\nPAPI_L1_DCM  0x80000000  Yes   No   Level 1 data cache misses\n\nPAPI_L1_ICM  0x80000001  Yes   No   Level 1 instruction cache misses\n\nPAPI_L2_DCM  0x80000002  Yes   Yes  Level 2 data cache misses\n\nPAPI_L2_ICM  0x80000003  Yes   No   Level 2 instruction cache misses\n\nPAPI_L3_DCM  0x80000004  No    No   Level 3 data cache misses\n\nPAPI_L3_ICM  0x80000005  No    No   Level 3 instruction cache misses\n\nPAPI_L1_TCM  0x80000006  Yes   Yes  Level 1 cache misses\n\nPAPI_L2_TCM  0x80000007  Yes   No   Level 2 cache misses\n\nPAPI_L3_TCM  0x80000008  Yes   No   Level 3 cache misses\n\nPAPI_CA_SNP  0x80000009  Yes   No   Requests for a snoop\n\nPAPI_CA_SHR  0x8000000a  Yes   No   Requests for exclusive access to shared cache line\n\nPAPI_CA_CLN  0x8000000b  Yes   No   Requests for exclusive access to clean cache line\n\nPAPI_CA_INV  0x8000000c  Yes   No   Requests for cache line invalidation\n\nPAPI_CA_ITV  0x8000000d  Yes   No   Requests for cache line intervention\n\nPAPI_L3_LDM  0x8000000e  Yes   No   Level 3 load misses\n\nPAPI_L3_STM  0x8000000f  No    No   Level 3 store misses\n\nPAPI_BRU_IDL 0x80000010  No    No   Cycles branch units are idle\n\nPAPI_FXU_IDL 0x80000011  No    No   Cycles integer units are idle\n\nPAPI_FPU_IDL 0x80000012  No    No   Cycles floating point units are idle\n\nPAPI_LSU_IDL 0x80000013  No    No   Cycles load/store units are idle\n\nPAPI_TLB_DM  0x80000014  Yes   Yes  Data translation lookaside buffer misses\n\nPAPI_TLB_IM  0x80000015  Yes   No   Instruction translation lookaside buffer misses\n\nPAPI_TLB_TL  0x80000016  No    No   Total translation lookaside buffer misses\n\nPAPI_L1_LDM  0x80000017  Yes   No   Level 1 load misses\n\nPAPI_L1_STM  0x80000018  Yes   No   Level 1 store misses\n\nPAPI_L2_LDM  0x80000019  Yes   No   Level 2 load misses\n\nPAPI_L2_STM  0x8000001a  Yes   No   Level 2 store misses\n\nPAPI_BTAC_M  0x8000001b  No    No   Branch target address cache misses\n\nPAPI_PRF_DM  0x8000001c  Yes   No   Data prefetch cache misses\n\nPAPI_L3_DCH  0x8000001d  No    No   Level 3 data cache hits\n\nPAPI_TLB_SD  0x8000001e  No    No   Translation lookaside buffer shootdowns\n\nPAPI_CSR_FAL 0x8000001f  No    No   Failed store conditional instructions\n\nPAPI_CSR_SUC 0x80000020  No    No   Successful store conditional instructions\n\nPAPI_CSR_TOT 0x80000021  No    No   Total store conditional instructions\n\nPAPI_MEM_SCY 0x80000022  No    No   Cycles Stalled Waiting for memory accesses\n\nPAPI_MEM_RCY 0x80000023  No    No   Cycles Stalled Waiting for memory Reads\n\nPAPI_MEM_WCY 0x80000024  Yes   No   Cycles Stalled Waiting for memory writes\n\nPAPI_STL_ICY 0x80000025  Yes   No   Cycles with no instruction issue\n\nPAPI_FUL_ICY 0x80000026  Yes   Yes  Cycles with maximum instruction issue\n\nPAPI_STL_CCY 0x80000027  Yes   No   Cycles with no instructions completed\n\nPAPI_FUL_CCY 0x80000028  Yes   No   Cycles with maximum instructions completed\n\nPAPI_HW_INT  0x80000029  No    No   Hardware interrupts\n\nPAPI_BR_UCN  0x8000002a  Yes   Yes  Unconditional branch instructions\n\nPAPI_BR_CN   0x8000002b  Yes   No   Conditional branch instructions\n\nPAPI_BR_TKN  0x8000002c  Yes   Yes  Conditional branch instructions taken\n\nPAPI_BR_NTK  0x8000002d  Yes   No   Conditional branch instructions not taken\n\nPAPI_BR_MSP  0x8000002e  Yes   No   Conditional branch instructions mispredicted\n\nPAPI_BR_PRC  0x8000002f  Yes   Yes  Conditional branch instructions correctly predicted\n\nPAPI_FMA_INS 0x80000030  No    No   FMA instructions completed\n\nPAPI_TOT_IIS 0x80000031  No    No   Instructions issued\n\nPAPI_TOT_INS 0x80000032  Yes   No   Instructions completed\n\nPAPI_INT_INS 0x80000033  No    No   Integer instructions\n\nPAPI_FP_INS  0x80000034  No    No   Floating point instructions\n\nPAPI_LD_INS  0x80000035  Yes   No   Load instructions\n\nPAPI_SR_INS  0x80000036  Yes   No   Store instructions\n\nPAPI_BR_INS  0x80000037  Yes   No   Branch instructions\n\nPAPI_VEC_INS 0x80000038  No    No   Vector/SIMD instructions (could include integer)\n\nPAPI_RES_STL 0x80000039  Yes   No   Cycles stalled on any resource\n\nPAPI_FP_STAL 0x8000003a  No    No   Cycles the FP unit(s) are stalled\n\nPAPI_TOT_CYC 0x8000003b  Yes   No   Total cycles\n\nPAPI_LST_INS 0x8000003c  Yes   Yes  Load/store instructions completed\n\nPAPI_SYC_INS 0x8000003d  No    No   Synchronization instructions completed\n\nPAPI_L1_DCH  0x8000003e  No    No   Level 1 data cache hits\n\nPAPI_L2_DCH  0x8000003f  No    No   Level 2 data cache hits\n\nPAPI_L1_DCA  0x80000040  No    No   Level 1 data cache accesses\n\nPAPI_L2_DCA  0x80000041  Yes   No   Level 2 data cache accesses\n\nPAPI_L3_DCA  0x80000042  Yes   Yes  Level 3 data cache accesses\n\nPAPI_L1_DCR  0x80000043  No    No   Level 1 data cache reads\n\nPAPI_L2_DCR  0x80000044  Yes   No   Level 2 data cache reads\n\nPAPI_L3_DCR  0x80000045  Yes   No   Level 3 data cache reads\n\nPAPI_L1_DCW  0x80000046  No    No   Level 1 data cache writes\n\nPAPI_L2_DCW  0x80000047  Yes   No   Level 2 data cache writes\n\nPAPI_L3_DCW  0x80000048  Yes   No   Level 3 data cache writes\n\nPAPI_L1_ICH  0x80000049  No    No   Level 1 instruction cache hits\n\nPAPI_L2_ICH  0x8000004a  Yes   No   Level 2 instruction cache hits\n\nPAPI_L3_ICH  0x8000004b  No    No   Level 3 instruction cache hits\n\nPAPI_L1_ICA  0x8000004c  No    No   Level 1 instruction cache accesses\n\nPAPI_L2_ICA  0x8000004d  Yes   No   Level 2 instruction cache accesses\n\nPAPI_L3_ICA  0x8000004e  Yes   No   Level 3 instruction cache accesses\n\nPAPI_L1_ICR  0x8000004f  No    No   Level 1 instruction cache reads\n\nPAPI_L2_ICR  0x80000050  Yes   No   Level 2 instruction cache reads\n\nPAPI_L3_ICR  0x80000051  Yes   No   Level 3 instruction cache reads\n\nPAPI_L1_ICW  0x80000052  No    No   Level 1 instruction cache writes\n\nPAPI_L2_ICW  0x80000053  No    No   Level 2 instruction cache writes\n\nPAPI_L3_ICW  0x80000054  No    No   Level 3 instruction cache writes\n\nPAPI_L1_TCH  0x80000055  No    No   Level 1 total cache hits\n\nPAPI_L2_TCH  0x80000056  No    No   Level 2 total cache hits\n\nPAPI_L3_TCH  0x80000057  No    No   Level 3 total cache hits\n\nPAPI_L1_TCA  0x80000058  No    No   Level 1 total cache accesses\n\nPAPI_L2_TCA  0x80000059  Yes   Yes  Level 2 total cache accesses\n\nPAPI_L3_TCA  0x8000005a  Yes   No   Level 3 total cache accesses\n\nPAPI_L1_TCR  0x8000005b  No    No   Level 1 total cache reads\n\nPAPI_L2_TCR  0x8000005c  Yes   Yes  Level 2 total cache reads\n\nPAPI_L3_TCR  0x8000005d  Yes   Yes  Level 3 total cache reads\n\nPAPI_L1_TCW  0x8000005e  No    No   Level 1 total cache writes\n\nPAPI_L2_TCW  0x8000005f  Yes   No   Level 2 total cache writes\n\nPAPI_L3_TCW  0x80000060  Yes   No   Level 3 total cache writes\n\nPAPI_FML_INS 0x80000061  No    No   Floating point multiply instructions\n\nPAPI_FAD_INS 0x80000062  No    No   Floating point add instructions\n\nPAPI_FDV_INS 0x80000063  No    No   Floating point divide instructions\n\nPAPI_FSQ_INS 0x80000064  No    No   Floating point square root instructions\n\nPAPI_FNV_INS 0x80000065  No    No   Floating point inverse instructions\n\nPAPI_FP_OPS  0x80000066  No    No   Floating point operations\n\nPAPI_SP_OPS  0x80000067  Yes   Yes  Floating point operations; optimized to count scaled single precision vector operations\n\nPAPI_DP_OPS  0x80000068  Yes   Yes  Floating point operations; optimized to count scaled double precision vector operations\n\nPAPI_VEC_SP  0x80000069  Yes   Yes  Single precision vector/SIMD instructions\n\nPAPI_VEC_DP  0x8000006a  Yes   Yes  Double precision vector/SIMD instructions\n\nPAPI_REF_CYC 0x8000006b  Yes   No   Reference clock cycles\n\n--------------------------------------------------------------------------------\n\nOf 108 possible events, 60 are available, of which 16 are derived.\n\njkwack@thetamom3:~&gt; which papi_avail\n\n/opt/cray/pe/papi/5.7.0.2/bin/papi_avail\n</code></pre>"},{"location":"theta/performance-tools/papi/#papi-via-perftools-base-module-on-theta","title":"PAPI via perftools-base module on Theta","text":"<pre><code>jkwack@thetamom3:~&gt; module unload papi\n\njkwack@thetamom3:~&gt; module load perftools-base\n\njkwack@thetamom3:~&gt; module show perftools-base\n\n-------------------------------------------------------------------\n\n/opt/cray/pe/modulefiles/perftools-base/7.0.5:\n\nsetenv PERFTOOLS_VERSION 7.0.5\n\nmodule-whatis The Performance Tools module sets up environments for CrayPat, Apprentice2 and Reveal\n\nconflict perftools-base\n\nconflict perftools\n\nconflict perftools-nwpc\n\nconflict perftools-lite\n\nconflict perftools-lite-loops\n\nconflict perftools-lite-events\n\nconflict perftools-lite-gpu\n\nconflict perftools-lite-hbm\n\nconflict perftools-preload\n\nconflict xt-mpich2\n\nconflict papi\n\nconflict craypat\n\nconflict cuda\n\nconflict cudatools\n\nmodule use /opt/cray/pe/perftools/7.0.5/modulefiles\n\nprepend-path MODULEPATH /opt/cray/pe/perftools/7.0.5/modulefiles\n\nsetenv CRAYPAT_ALPS_COMPONENT /opt/cray/pe/perftools/7.0.5/sbin/pat_alps\n\nsetenv PAT_BUILD_PAPI_BASEDIR /opt/cray/pe/papi/5.6.0.5\n\nprepend-path MANPATH /opt/cray/pe/papi/5.6.0.5/share/pdoc/man\n\nprepend-path PATH /opt/cray/pe/papi/5.6.0.5/bin\n\nprepend-path LD_LIBRARY_PATH /opt/cray/pe/papi/5.6.0.5/lib64\n\nsetenv CHPL_CG_CPP_LINES 1\n\nsetenv PAT_REPORT_PRUNE_NAME _cray$mt_execute_,_cray$mt_start_,__cray_hwpc_,f_cray_hwpc_,cstart,__pat_,pat_region_,PAT_,OMP.slave_loop,slave_entry,_new_slave_entry,_thread_pool_slave_entry,THREAD_POOL_join,__libc_start_main,_start,__start,start_thread,__wrap_,UPC_ADIO_,_upc_,upc_,__caf_,__pgas_,syscall,__device_stub\n\nsetenv OFFLOAD_INIT on_start\n\nsetenv CRAYPAT_LD_LIBRARY_PATH /opt/cray/pe/gcc-libs:/opt/cray/gcc-libs:/opt/cray/pe/perftools/7.0.5/lib64\n\nprepend-path PATH /opt/cray/pe/perftools/7.0.5/bin\n\nprepend-path MANPATH /opt/cray/pe/perftools/7.0.5/man\n\nsetenv CRAYPAT_OPTS_EXECUTABLE sbin/pat-opts\n\nsetenv CRAYPAT_ROOT /opt/cray/pe/perftools/7.0.5\n\nsetenv APP2_STATE 7.0.5\n\nprepend-path CRAY_LD_LIBRARY_PATH /opt/cray/pe/perftools/7.0.5/lib64\n\nappend-path PE_PRODUCT_LIST PERFTOOLS\n\nappend-path PE_PRODUCT_LIST CRAYPAT\n\n-------------------------------------------------------------------\n\njkwack@thetamom3:~&gt; papi_avail\n\nAvailable PAPI preset and user defined events plus hardware information.\n\n--------------------------------------------------------------------------------\n\nPAPI version             : 5.6.0.5\n\nOperating system         : Linux 4.4.103-6.38_4.0.154-cray_ari_s\n\nVendor string and code   : GenuineIntel (1, 0x1)\n\nModel string and code    : Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHz (79, 0x4f)\n\nCPU revision             : 1.000000\n\nCPUID                    : Family/Model/Stepping 6/79/1, 0x06/0x4f/0x01\n\nCPU Max MHz              : 2101\n\nCPU Min MHz              : 1200\n\nTotal cores              : 72\n\nSMT threads per core     : 2\n\nCores per socket         : 18\n\nSockets                  : 2\n\nCores per NUMA region    : 36\n\nNUMA regions             : 2\n\nRunning in a VM          : no\n\nNumber Hardware Counters : 11\n\nMax Multiplex Counters   : 384\n\nFast counter read (rdpmc): no\n\n--------------------------------------------------------------------------------\n\n================================================================================\n\n  PAPI Preset Events\n\n================================================================================\n\n    Name        Code    Avail Deriv Description (Note)\n\nPAPI_L1_DCM  0x80000000  Yes   No   Level 1 data cache misses\n\nPAPI_L1_ICM  0x80000001  Yes   No   Level 1 instruction cache misses\n\nPAPI_L2_DCM  0x80000002  Yes   Yes  Level 2 data cache misses\n\nPAPI_L2_ICM  0x80000003  Yes   No   Level 2 instruction cache misses\n\nPAPI_L3_DCM  0x80000004  No    No   Level 3 data cache misses\n\nPAPI_L3_ICM  0x80000005  No    No   Level 3 instruction cache misses\n\nPAPI_L1_TCM  0x80000006  Yes   Yes  Level 1 cache misses\n\nPAPI_L2_TCM  0x80000007  Yes   No   Level 2 cache misses\n\nPAPI_L3_TCM  0x80000008  Yes   No   Level 3 cache misses\n\nPAPI_CA_SNP  0x80000009  Yes   No   Requests for a snoop\n\nPAPI_CA_SHR  0x8000000a  Yes   No   Requests for exclusive access to shared cache line\n\nPAPI_CA_CLN  0x8000000b  Yes   No   Requests for exclusive access to clean cache line\n\nPAPI_CA_INV  0x8000000c  Yes   No   Requests for cache line invalidation\n\nPAPI_CA_ITV  0x8000000d  Yes   No   Requests for cache line intervention\n\nPAPI_L3_LDM  0x8000000e  Yes   No   Level 3 load misses\n\nPAPI_L3_STM  0x8000000f  No    No   Level 3 store misses\n\nPAPI_BRU_IDL 0x80000010  No    No   Cycles branch units are idle\n\nPAPI_FXU_IDL 0x80000011  No    No   Cycles integer units are idle\n\nPAPI_FPU_IDL 0x80000012  No    No   Cycles floating point units are idle\n\nPAPI_LSU_IDL 0x80000013  No    No   Cycles load/store units are idle\n\nPAPI_TLB_DM  0x80000014  Yes   Yes  Data translation lookaside buffer misses\n\nPAPI_TLB_IM  0x80000015  Yes   No   Instruction translation lookaside buffer misses\n\nPAPI_TLB_TL  0x80000016  No    No   Total translation lookaside buffer misses\n\nPAPI_L1_LDM  0x80000017  Yes   No   Level 1 load misses\n\nPAPI_L1_STM  0x80000018  Yes   No   Level 1 store misses\n\nPAPI_L2_LDM  0x80000019  Yes   No   Level 2 load misses\n\nPAPI_L2_STM  0x8000001a  Yes   No   Level 2 store misses\n\nPAPI_BTAC_M  0x8000001b  No    No   Branch target address cache misses\n\nPAPI_PRF_DM  0x8000001c  Yes   No   Data prefetch cache misses\n\nPAPI_L3_DCH  0x8000001d  No    No   Level 3 data cache hits\n\nPAPI_TLB_SD  0x8000001e  No    No   Translation lookaside buffer shootdowns\n\nPAPI_CSR_FAL 0x8000001f  No    No   Failed store conditional instructions\n\nPAPI_CSR_SUC 0x80000020  No    No   Successful store conditional instructions\n\nPAPI_CSR_TOT 0x80000021  No    No   Total store conditional instructions\n\nPAPI_MEM_SCY 0x80000022  No    No   Cycles Stalled Waiting for memory accesses\n\nPAPI_MEM_RCY 0x80000023  No    No   Cycles Stalled Waiting for memory Reads\n\nPAPI_MEM_WCY 0x80000024  Yes   No   Cycles Stalled Waiting for memory writes\n\nPAPI_STL_ICY 0x80000025  Yes   No   Cycles with no instruction issue\n\nPAPI_FUL_ICY 0x80000026  Yes   Yes  Cycles with maximum instruction issue\n\nPAPI_STL_CCY 0x80000027  Yes   No   Cycles with no instructions completed\n\nPAPI_FUL_CCY 0x80000028  Yes   No   Cycles with maximum instructions completed\n\nPAPI_HW_INT  0x80000029  No    No   Hardware interrupts\n\nPAPI_BR_UCN  0x8000002a  Yes   Yes  Unconditional branch instructions\n\nPAPI_BR_CN   0x8000002b  Yes   No   Conditional branch instructions\n\nPAPI_BR_TKN  0x8000002c  Yes   Yes  Conditional branch instructions taken\n\nPAPI_BR_NTK  0x8000002d  Yes   No   Conditional branch instructions not taken\n\nPAPI_BR_MSP  0x8000002e  Yes   No   Conditional branch instructions mispredicted\n\nPAPI_BR_PRC  0x8000002f  Yes   Yes  Conditional branch instructions correctly predicted\n\nPAPI_FMA_INS 0x80000030  No    No   FMA instructions completed\n\nPAPI_TOT_IIS 0x80000031  No    No   Instructions issued\n\nPAPI_TOT_INS 0x80000032  Yes   No   Instructions completed\n\nPAPI_INT_INS 0x80000033  No    No   Integer instructions\n\nPAPI_FP_INS  0x80000034  No    No   Floating point instructions\n\nPAPI_LD_INS  0x80000035  Yes   No   Load instructions\n\nPAPI_SR_INS  0x80000036  Yes   No   Store instructions\n\nPAPI_BR_INS  0x80000037  Yes   No   Branch instructions\n\nPAPI_VEC_INS 0x80000038  No    No   Vector/SIMD instructions (could include integer)\n\nPAPI_RES_STL 0x80000039  Yes   No   Cycles stalled on any resource\n\nPAPI_FP_STAL 0x8000003a  No    No   Cycles the FP unit(s) are stalled\n\nPAPI_TOT_CYC 0x8000003b  Yes   No   Total cycles\n\nPAPI_LST_INS 0x8000003c  Yes   Yes  Load/store instructions completed\n\nPAPI_SYC_INS 0x8000003d  No    No   Synchronization instructions completed\n\nPAPI_L1_DCH  0x8000003e  No    No   Level 1 data cache hits\n\nPAPI_L2_DCH  0x8000003f  No    No   Level 2 data cache hits\n\nPAPI_L1_DCA  0x80000040  No    No   Level 1 data cache accesses\n\nPAPI_L2_DCA  0x80000041  Yes   No   Level 2 data cache accesses\n\nPAPI_L3_DCA  0x80000042  Yes   Yes  Level 3 data cache accesses\n\nPAPI_L1_DCR  0x80000043  No    No   Level 1 data cache reads\n\nPAPI_L2_DCR  0x80000044  Yes   No   Level 2 data cache reads\n\nPAPI_L3_DCR  0x80000045  Yes   No   Level 3 data cache reads\n\nPAPI_L1_DCW  0x80000046  No    No   Level 1 data cache writes\n\nPAPI_L2_DCW  0x80000047  Yes   No   Level 2 data cache writes\n\nPAPI_L3_DCW  0x80000048  Yes   No   Level 3 data cache writes\n\nPAPI_L1_ICH  0x80000049  No    No   Level 1 instruction cache hits\n\nPAPI_L2_ICH  0x8000004a  Yes   No   Level 2 instruction cache hits\n\nPAPI_L3_ICH  0x8000004b  No    No   Level 3 instruction cache hits\n\nPAPI_L1_ICA  0x8000004c  No    No   Level 1 instruction cache accesses\n\nPAPI_L2_ICA  0x8000004d  Yes   No   Level 2 instruction cache accesses\n\nPAPI_L3_ICA  0x8000004e  Yes   No   Level 3 instruction cache accesses\n\nPAPI_L1_ICR  0x8000004f  No    No   Level 1 instruction cache reads\n\nPAPI_L2_ICR  0x80000050  Yes   No   Level 2 instruction cache reads\n\nPAPI_L3_ICR  0x80000051  Yes   No   Level 3 instruction cache reads\n\nPAPI_L1_ICW  0x80000052  No    No   Level 1 instruction cache writes\n\nPAPI_L2_ICW  0x80000053  No    No   Level 2 instruction cache writes\n\nPAPI_L3_ICW  0x80000054  No    No   Level 3 instruction cache writes\n\nPAPI_L1_TCH  0x80000055  No    No   Level 1 total cache hits\n\nPAPI_L2_TCH  0x80000056  No    No   Level 2 total cache hits\n\nPAPI_L3_TCH  0x80000057  No    No   Level 3 total cache hits\n\nPAPI_L1_TCA  0x80000058  No    No   Level 1 total cache accesses\n\nPAPI_L2_TCA  0x80000059  Yes   Yes  Level 2 total cache accesses\n\nPAPI_L3_TCA  0x8000005a  Yes   No   Level 3 total cache accesses\n\nPAPI_L1_TCR  0x8000005b  No    No   Level 1 total cache reads\n\nPAPI_L2_TCR  0x8000005c  Yes   Yes  Level 2 total cache reads\n\nPAPI_L3_TCR  0x8000005d  Yes   Yes  Level 3 total cache reads\n\nPAPI_L1_TCW  0x8000005e  No    No   Level 1 total cache writes\n\nPAPI_L2_TCW  0x8000005f  Yes   No   Level 2 total cache writes\n\nPAPI_L3_TCW  0x80000060  Yes   No   Level 3 total cache writes\n\nPAPI_FML_INS 0x80000061  No    No   Floating point multiply instructions\n\nPAPI_FAD_INS 0x80000062  No    No   Floating point add instructions\n\nPAPI_FDV_INS 0x80000063  No    No   Floating point divide instructions\n\nPAPI_FSQ_INS 0x80000064  No    No   Floating point square root instructions\n\nPAPI_FNV_INS 0x80000065  No    No   Floating point inverse instructions\n\nPAPI_FP_OPS  0x80000066  No    No   Floating point operations\n\nPAPI_SP_OPS  0x80000067  Yes   Yes  Floating point operations; optimized to count scaled single precision vector operations\n\nPAPI_DP_OPS  0x80000068  Yes   Yes  Floating point operations; optimized to count scaled double precision vector operations\n\nPAPI_VEC_SP  0x80000069  Yes   Yes  Single precision vector/SIMD instructions\n\nPAPI_VEC_DP  0x8000006a  Yes   Yes  Double precision vector/SIMD instructions\n\nPAPI_REF_CYC 0x8000006b  Yes   No   Reference clock cycles\n\n--------------------------------------------------------------------------------\n\nOf 108 possible events, 60 are available, of which 16 are derived.\n\njkwack@thetamom3:~&gt; which papi_avail\n\n/opt/cray/pe/papi/5.6.0.5/bin/papi_avail\n</code></pre>"},{"location":"theta/performance-tools/tau/","title":"TAU","text":""},{"location":"theta/performance-tools/tau/#introduction","title":"Introduction","text":"<p>The TAU (Tuning and Analysis Utilities) Performance System is a portable profiling and tracing toolkit for performance analysis of parallel programs written in Fortran, C, C++, Java, and Python. TAU gathers performance information while a program executes through instrumentation of functions, methods, basic blocks, and statements. The instrumentation consists of calls to TAU library routines, which can be incorporated into a program in several ways: - Automatic instrumentation of the code at the source level using the Program Database Toolkit (PDT) - Automatic instrumentation of the code using the compiler - Manual instrumentation using the instrumentation API - At runtime using library call interception - Dynamically using DyninstAPI - At runtime in the Java virtual machine</p> <p>For more information on TAU instrumentation options, see: http://www.cs.uoregon.edu/Research/tau/docs/newguide/bk01ch01.html</p>"},{"location":"theta/performance-tools/tau/#references","title":"References","text":"<ul> <li>TAU Project Site</li> <li>TAU Instrumentation Methods</li> <li>TAU Compilation Options</li> <li>TAU Fortran Instrumentation FAQ</li> <li>TAU Performance Workshop 2018 Presentation</li> </ul>"},{"location":"theta/performance-tools/tau/#compiling-your-application-with-tau","title":"Compiling Your Application with TAU","text":"<p>While there are several methods of incorporating TAU instrumentation into a program, the two most common are automatic insertion (using the PDT source instrumentation method) and compiler instrumentation insertion. With either of these methods users must compile an application in a specific way so as to insert the TAU instrumentation that enables data collection. This involves invoking wrapper scripts that manage the compiling and linking process.</p>"},{"location":"theta/performance-tools/tau/#build-time-module","title":"Build Time Module","text":"<p>Start by loading the TAU module <pre><code>module load tau\n</code></pre> This will set add TAU scripts to your path and set the $TAU environment variable.</p> <p>Two additional environment variables will also need to be set, but the settings will depend on the performance data to be collected and the method used for collection. Examples of settings for these values are: <pre><code>TAU_MAKEFILE = $TAU/Makefile.tau-intel-mpi-pdt\nTAU_OPTIONS = '-optVerbose -optNoRevert'\n</code></pre> It is also possible to specify these parameters at the command line. <pre><code>tau_cc.sh -tau_makefile=[makefile with complete path] tau_options=[options] sampleCprogram.c\n</code></pre> The default method of instrumentation is to use PDT to insert calls into the source code. Alternatively, instrumentation may be inserted by the compiler, which requires adding -optCompInst to the TAU_OPTIONS setting. While PDT provides the widest array of instrumentation features, in some cases it may not be able to properly parse the source code and the instrumentation and will fail. In these cases, compiler instrumentation is the recommended alternative.</p> <p>The TAU_MAKEFILE option largely determines what type of information TAU collects during program execution. This variable must specify the name of the TAU configuration file. Some of the options are: <pre><code>Makefile.tau-gnu-papi-mpi-pdt\nMakefile.tau-gnu-papi-mpi-pthread-pdt\nMakefile.tau-gnu-papi-pthread-pdt\nMakefile.tau-intel-19.0.3.199-papi-mpi-pthread-pdt\nMakefile.tau-intel-19.0.3.199-papi-ompt-v5-mpi-pdt-openmp\nMakefile.tau-intel-datascience_tensorflow_113-papi-mpi-pthread-python-pdt\nMakefile.tau-intel-mpi-pdt\nMakefile.tau-intel-papi-mpi-pdt\nMakefile.tau-intel-papi-ompt-tr6-mpi-pdt-openmp\nMakefile.tau-intel-papi-pdt\n...\n</code></pre> The Makefile extension is a descriptive name for the configuration. Selecting a makefile will involve making choices among the following: - Compiler: For GNU, look for those containing \u2018gnu.\u2019 For Intel, look for those containing \u2018intel\u2019 - Parallelism: MPI is well supported.  - PAPI: Interfaces with the PAPI hardware counters. - Scalasca: Interfaces with Scalasca, http://www.scalasca.org. Note: Dynamic linking is not available (e.g., it will not work with Python).</p>"},{"location":"theta/performance-tools/tau/#tau_options","title":"TAU_OPTIONS","text":"<p>The TAU_OPTIONS affect how TAU inserts the instrumentation and various options are documented. Here is a list of commonly used TAU_OPTIONS.</p> <pre><code>-optVerbose          self-explanatory\n-optNoRevert         causes hard-failure when there is a TAU error, default behavior is to revert to an uninstrumented compile\n-optKeepFiles        output your source file after it has been processed by the PDT parser\n-optPreProcess       use if preprocess directives are present in Fortran code\n-optPdtCOpts         pass special options to the PDT parse\n-optCompInst         enables compiler-based instrumentation\n-optTauSelectFile    enables selective instrumentation; cannot be used with compiler-based instrumentation\n-optShared           linked against shared libraries, not recommended unless you know what you are doing\n</code></pre>"},{"location":"theta/performance-tools/tau/#compiling","title":"Compiling","text":"<p>Once the TAU environment has been fully specified, an application may be compiled with TAU instrumentation by replacing the standard compiler names in the applications make file with the TAU compiler wrapper scripts: <pre><code>ftn -&gt;   tau_f90.sh (add -qfixed for Fortran77)\ncc  -&gt;   tau_cc.sh\nCC  -&gt;   tau_cxx.sh\n</code></pre> These scripts will perform the instrumentation and compilation of the code by invoking TAU pre-processing tools and then invoke the compilers using the appropriate method. Successful compilation will produce a binary executable for the program. It is recommended to give your executable a unique name, perhaps by adding a .tau suffix, to distinguish it from a standard executable. Once a program is compiled with TAU instrumentation, the type and nature of the instrumentation is largely set (though some runtime variables exist that alter the instrumentation\u2019s behavior). If you wish to collect a different type of performance data, you must re-compile the application with a different TAU_MAKEFILE.</p>"},{"location":"theta/performance-tools/tau/#c-tips","title":"C Tips","text":"<p>If you receive the following type of error: <pre><code>\"foo.c\", line 368: error: \nexpected an expression for (int i = 0; i &lt; n; i++)\n</code></pre> append <code>-optPdtCOpts=-c99 to TAU_OPTIONS</code></p>"},{"location":"theta/performance-tools/tau/#fortran-tips","title":"Fortran Tips","text":"<ul> <li>Append -optPreProcess to TAU_OPTIONS if pre-process directives are present.</li> <li>Source-based instrumentation will not work for ENTRY points; a workaround is needed.</li> <li>Identify all relevant ENTRY points and exclude the parent function with a selective instrumentation file.</li> <li>Use compiler-based instrumentation instead.</li> <li>For Fortran77 codes, tau_f90.sh called the Fortran90 compiler. Thus, it will be necessary to add \"-qfixed\" to the Fortran compiler flags in your Makefile. If compilation fails with errors referencing syntax errors on lines that are comments, this indicates the use of \"-qfixed.\" Comments using \"C\" in the first column are one instance in which \"-qfixed\" is required.</li> </ul>"},{"location":"theta/performance-tools/tau/#running-with-tau","title":"Running with TAU","text":"<p>Once an application has been built with TAU instrumentation, it is not necessary to do anything special in order to run it. Simply execute the application as usual and TAU will collect data and write it to one or more files. However, it should be noted that in many cases TAU collects a large amount of performance data that have a significant impact on your application's wall-clock time. It is always a good idea to compare the wall-clock from an instrumented binary to that of the pristine (un-instrumented) binary. If you see a large number of function calls, chances are there will be significant overhead. </p> <p>Several runtime environment variables are available that can influence TAU's runtime behavior and limit the imposed overhead: <pre><code>These environment variables are passed to TAU when you job is submitted with cobalt using the --env flag. \n() denotes default.\n\nTAU_VERBOSE=(0) or 1        Stderr contains TAU debugging information. \nTAU_THROTTLE=0 or (1)       Attempts to reduce TAU overhead by turning off instrumentation for frequently called routines.\nTAU_COMPENSATE=(0) or 1     Attempts to approximate and subtract out the instrumentation overhead from the reported metrics.\nTAU_COMM_MATRIX=(0) or 1    Collects details information on point-to-point communication for MPI ranks.\nTAU_TRACE=(0) or 1          Collects tracing information instead of profile information.\nTAU_CALLPATH=(0) or 1       Generates a call path information for profiles. \nTAU_CALLPATH_DEPTH=N(2)     Where N is an positive integer.\nTAU_PROFILE_FORMAT=merged   Will merge all data into a single file in snapshot format: tauprofile.xml. \n                               Recommended using more than 10,000 cores.\nTAU_TRACK_HEAP=(0) or 1     Measures heap on function entry and exit.\nTAU_TRACK_MESSAGE=(0) or 1  Collects detailed information about message sizes.\n</code></pre> The simplest method for managing instrumentation overhead is by using the | <code>TAU_THROTTLE</code> environment variable. <code>TAU_THROTTLE = 0</code> is needed for a full profile of your application. However, it is possible that you will incur a significant amount of overhead. If your application spends a significant percent of its runtime calling small routines repeatedly, e.g., 10 microsecond per call and more 1e5 calls, use either <code>TAU_THROTTLE=0</code> or selective instrumentation to have a flat profile with manageable overhead (&lt;25%).</p> <p>The <code>AU_COMPENSATE</code> setting will approximate the instrumentation overhead and subtract this from the metric reported. Check against timings from a pristine binary. For example, if the total exclusive time reported by TAU and the wall-clock time from the pristine binary are very different from the compensate option; it is not working effectively for your application.</p> <p><code>TAU_COMM_MATRIX</code> collects and writes the communication matrix (columns, actually) for each rank. Different values of <code>TAU_CALLPATH_DEPTH</code> produce different types of information:</p> <ul> <li>0 - communication matrix for application as a whole</li> <li>1 - communication matrix broken down by function</li> <li>2 - same as <code>TAU_CALLPATH_DEPTH=1</code> but also includes parent function</li> </ul>"},{"location":"theta/performance-tools/tau/#analyzing-your-data","title":"Analyzing Your Data","text":"<p>The data collected by TAU will be written to one more file, by default to the application execution directory. This data may be viewed with the TAU command line tool pprof, or the GUI tools ParaProf and PerfExplorer. These tools may be run from the login nodes if you have an X-Windows environment on your local machine and X11 forwarding was set by logging in with \"ssh \u2013X.\" Alternatively, the GUI tools may be installed onto your local machine.</p> <p>ParaProf is primarily for viewing a handful of profiles, while PerfExplorer is analyzing a larger collection of performance data (e.g., for weak scaling, strong scaling, etc.). PerfExplorer is highly recommended when large volumes of performance data are to be collected. In order to use PerfExplorer, first create a PerfDMF database. <pre><code>perfdmf_configure --create-default\n</code></pre> Go to the directory that contains your profiles. There will be a number of files named profile.x.0.0 where x is the MPI rank. The profile files are text files and it is convenient to pack into the native TAU format. <pre><code>paraprof --pack filename.ppk\n</code></pre> SCP filename.ppk to your desktop and open it up in the ParaProf viewer.</p> <p>To analyze a trace file (from TAU_TRACE=1), see the Jumpshot instructions at http://www.cs.uoregon.edu/Research/tau/docs/newguide/bk01ch04s03.html</p>"},{"location":"theta/performance-tools/tau/#advanced-users","title":"Advanced Users","text":""},{"location":"theta/performance-tools/tau/#selective-instrumentation","title":"Selective Instrumentation","text":"<p>Selective instrumentation is enabled by appending -optTauSelectFile= to TAU_OPTIONS. Note that this is incompatible with the compiler-based instrumentation, e.g., -optCompInst. The syntax of the select instrumentation file can be found here: <p>https://www.cs.uoregon.edu/research/tau/docs/newguide/bk01pt01ch01s03.html</p> <p>It is also possible to generate a selective instrumentation file using Paraprof and a full flat profile obtained through automatic instrumentation.</p> <p>http://www.cs.uoregon.edu/Research/tau/docs/newguide/bk03pt02ch16s03.html</p>"},{"location":"theta/performance-tools/tau/#manual-instrumentation","title":"Manual Instrumentation","text":"<p>If automatic instrumentation has too much overhead or too fine-grained details, an option to consider is performing light-weight/coarse-grained instrumentation by adding functional calls to the TAU timers directly in the source code. See the TAU API: </p> <p>http://www.cs.uoregon.edu/Research/tau/docs/newguide/bk03rn01.html</p> <p>The most useful TAU API routines are those for starting and stopping timers. A text token needs to be provided for the timer: <pre><code>Fortran:\n\nCALL TAU_START('dgemm');\nCALL TAU_STOP('dgemm');\nC:\n#include \nTAU_START(\"dgemm\");\nTAU_STOP(\"dgemm\");\n</code></pre></p> <p>Note: External libraries such as BLAS or LAPACK are not automatically instrumented. This must be performed manually or a TAU wrapper library must be created:</p> <p>http://www.cs.uoregon.edu/Research/tau/docs/newguide/re32.html</p>"},{"location":"theta/performance-tools/tau/#resources","title":"Resources","text":"<ul> <li>TAU Performance Analysis</li> </ul>"},{"location":"theta/programming-models/kokkos/","title":"Kokkos","text":""},{"location":"theta/programming-models/kokkos/#overview","title":"Overview","text":"<p>Kokkos implements a programming model in C++ for writing performance portable applications targeting all major HPC platforms. It provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. It can use OpenMP, etc as backend programming model. For more information please visit https://github.com/kokkos/kokkos</p> <p>The Kokkos shared memory programming model is a C++ library, that provides the necessary architecture specific backends (e.g. OpenMP, CUDA, \u2026). To begin with, though, it is important to note that the Kokkos programming model is usable only in C/C++ codes. Hence, for those with Fortran codes, Kokkos must first be encapsulated within C/C++ functions and called from the main Fortran code.</p> <p>The purpose of this document is to provide guidance on using Kokkos on Cooley. Please see the following pages for tutorial and more information on Kokkos: Kokkos GitHub and Kokkos Tutorials. </p>"},{"location":"theta/programming-models/kokkos/#using-kokkos-at-alcf","title":"Using Kokkos at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta/programming-models/kokkos/#building-kokkos-on-theta","title":"Building Kokkos on Theta","text":"<p>Please follow the steps below to build Kokks on Theta.</p> <p>Step 1:  Clone the repository <pre><code>git clone https://github.com/kokkos/kokkos.git\ncd kokkos\nexport KOKKOS_PATH=\u201d${PWD}\u201d\n</code></pre> Note: The Kokkos Project strives to keep the master branch stable.</p> <p>Step 2:  Make sure that a relatively new version of CMake is available <pre><code>module load cmake/3.14.5\n</code></pre> Step 3:  Create a build directory <pre><code>mkdir build &amp;&amp; cd build\n</code></pre> Step 4:  Generate the Makefile <pre><code>cmake ../kokkos \\\n    -DCMAKE_CXX_COMPILER=CC \\\n    -DCMAKE_INSTALL_PREFIX=${PWD}/kokkos-install \\\n    -DKokkos_ENABLE_OPENMP=On \\\n    -DKokkos_ENABLE_HWLOC=On \\\n    -DKokkos_HWLOC_DIR=/usr \\\n    -DHWLOC_LIBRARY=/usr/lib64/libhwloc.so\n</code></pre></p> <p>Note: The default compiler on Theta should be sufficient to build Kokkos.</p> <p>Step 5: Install Kokkos <pre><code>make install\n</code></pre> Note: This will end up installing Kokkos in <code>${KOKKOS_PATH}/build/kokkos-install</code>. If you wish to install it in a different directory, change <code>CMAKE_INSTALL_PREFIX in step 4</code>.</p>"},{"location":"theta/programming-models/mpi-on-theta/","title":"MPI on Theta","text":""},{"location":"theta/programming-models/mpi-on-theta/#what-is-mpi","title":"What is MPI?","text":"<p>MPI is a library specification for message-passing, proposed as a standard by a broadly-based committee of vendors, implementors, and users.</p> <ul> <li>The MPI standard is available.</li> <li>MPI was designed for high performance on both massively parallel machines and on workstation clusters.</li> <li>MPI is widely available, with both free available and vendor-supplied implementations.</li> <li>Test Suites for MPI implementations are available.</li> <li>MPICH is a high performance and widely portable implementation of the Message Passing Interface (MPI) standard.</li> <li>Documentation about MPICH</li> </ul>"},{"location":"theta/programming-models/mpi-on-theta/#mpi-on-theta_1","title":"MPI on Theta","text":"<ul> <li>The default MPI on Theta is Cray MPI which is a proprietary implementation from Cray based on the MPICH distribution from Argonne</li> <li>Supports Fortran, C, C++</li> <li>Integrated within Cray Programming Environment</li> <li>IO, collectives, point-to-point and one-sided operations are optimized for the Cray XC architecture</li> <li>Highly tunable through environment variables</li> <li>Integrated within the Cray programming environment</li> </ul>"},{"location":"theta/programming-models/mpi-on-theta/#references","title":"References","text":"<ul> <li> <p>MPI Tutorials: Programming Models and Languages track at ATPESC</p> </li> <li> <p>Talks: Using MPI effectively on Theta</p> </li> </ul>"},{"location":"theta/programming-models/openmp-theta/","title":"OpenMP on Theta","text":""},{"location":"theta/programming-models/openmp-theta/#overview","title":"Overview","text":"<p>The OpenMP API is an open standard for parallel programming. The specification document can be found here: https://www.openmp.org. </p> <p>The specification describes directives, runtime routines, and environment variables that allow an application developer to express parallelism (e.g. shared memory multiprocessing and device offloading). Many compiler vendors provide implementations of the OpenMP specification https://www.openmp.org/specifications.</p>"},{"location":"theta/programming-models/openmp-theta/#using-openmp-at-alcf","title":"Using OpenMP at ALCF","text":"<p>On Theta, all of the standard programming environments (PrgEnv-cray, PrgEnv-intel, PrgEnv-gnu, PrgEnv-llvm modules) support OpenMP. An application can be compiled with OpenMP support using the flags listed below in the \u201cBuilding on Theta\u201d section for each vendor. At runtime, how OpenMP threads are mapped to physical cores can be controlled with environment variables and the aprun command, as discussed below in the \u201cRunning Jobs with OpenMP on Theta\u201d section.</p> <p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta/programming-models/openmp-theta/#building-on-theta","title":"Building on Theta","text":"<p>To enable OpenMP, use the following flags in your compile line, depending on the programming environment you have loaded:</p>"},{"location":"theta/programming-models/openmp-theta/#compiler-flags","title":"Compiler flags","text":"Vendor Programming Environment Module Flag to enable OpenMP Intel PrgEnv-intel \u201c-qopenmp\" Cray (9.0.2) PrgEnv-cray \u201c-fopenmp\" GNU PrgEnv-gnu \u201c-fopenmp\" LLVM PrgEnv-llvm \u201c-fopenmp\""},{"location":"theta/programming-models/openmp-theta/#running-jobs-with-openmp-on-theta","title":"Running Jobs with OpenMP on Theta","text":"<p>To run jobs on Theta with OpenMP threads, the OpenMP environment variable <code>OMP_NUM_THREADS</code> will need to be set to the desired number of threads per MPI rank, and certain flags in the aprun command will need to be set. Some examples are given below, and more information about running is here: Affinity on Theta.</p>"},{"location":"theta/programming-models/openmp-theta/#source-code-for-xthic","title":"Source code for xthi.c:","text":"<pre><code>#define _GNU_SOURCE\n\n#include \n#include \n#include \n#include \n#include \n#include \n\n/* Borrowed from util-linux-2.13-pre7/schedutils/taskset.c */\nstatic char *cpuset_to_cstr(cpu_set_t *mask, char *str)\n{\n  char *ptr = str;\n  int i, j, entry_made = 0;\n  for (i = 0; i &lt; CPU_SETSIZE; i++) {\n    if (CPU_ISSET(i, mask)) {\n      int run = 0;\n      entry_made = 1;\n      for (j = i + 1; j &lt; CPU_SETSIZE; j++) {\n        if (CPU_ISSET(j, mask)) run++;\n        else break;\n      }\n      if (!run)\n        sprintf(ptr, \"%d,\", i);\n      else if (run == 1) {\n        sprintf(ptr, \"%d,%d,\", i, i + 1);\n        i++;\n      } else {\n        sprintf(ptr, \"%d-%d,\", i, i + run);\n        i += run;\n      }\n      while (*ptr != 0) ptr++;\n    }\n  }\n  ptr -= entry_made;\n  *ptr = 0;\n  return(str);\n}\n\nint main(int argc, char *argv[])\n{\n  int rank, thread;\n  cpu_set_t coremask;\n  char clbuf[7 * CPU_SETSIZE], hnbuf[64];\n\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n  memset(clbuf, 0, sizeof(clbuf));\n  memset(hnbuf, 0, sizeof(hnbuf));\n  (void)gethostname(hnbuf, sizeof(hnbuf));\n#pragma omp parallel private(thread, coremask, clbuf)\n  {\n    thread = omp_get_thread_num();\n    (void)sched_getaffinity(0, sizeof(coremask), &amp;coremask);\n    cpuset_to_cstr(&amp;coremask, clbuf);\n    #pragma omp barrier\n    printf(\"Hello from rank %d, thread %d, on %s. (core affinity = %s)\\n\",\n           rank, thread, hnbuf, clbuf);\n  }\n  MPI_Finalize();\n  return(0);\n}\n</code></pre> <ol> <li> <p>Compile with appropriate flag for the Programming Environment <pre><code>$ cc -qopenmp xthi.c -o xthi # PrgEnv-intel\n</code></pre></p> </li> <li> <p>Run with <code>aprun</code> (either in a batch script that is submitted to the job scheduler a or on the command line as part of an interactive session. See job scheduling for more details about how to run.</p> </li> </ol> <p>Mapping of OpenMP threads to hardware threads on a KNL node can be achieved with the \u201c--cc\u201d option in aprun.</p> <p>One common option described in more detail on Affinity on Theta  is to use <code>--cc depth</code> with the <code>-d</code> and <code>-j</code> flags:</p> <pre><code>$ aprun -n 1 -N 1 -d 8 -j 1 -cc depth -e OMP_NUM_THREADS=8 ./a.out\nHello from rank 0, thread 5, on nid03554. (core affinity = 5)\nHello from rank 0, thread 0, on nid03554. (core affinity = 0)\nHello from rank 0, thread 3, on nid03554. (core affinity = 3)\nHello from rank 0, thread 4, on nid03554. (core affinity = 4)\nHello from rank 0, thread 2, on nid03554. (core affinity = 2)\nHello from rank 0, thread 7, on nid03554. (core affinity = 7)\nHello from rank 0, thread 1, on nid03554. (core affinity = 1)\nHello from rank 0, thread 6, on nid03554. (core affinity = 6)\nApplication 19165961 resources: utime ~1s, stime ~1s, Rss ~6284, inblocks ~0, outblocks ~8\n</code></pre> <p>Another option is to use <code>--cc none</code> with OpenMP affinity controls:</p> <pre><code>$ aprun -n 1 -N 1 -cc none -e OMP_NUM_THREADS=8 -e OMP_PROC_BIND=spread -e OMP_PLACES=cores ./a.out\nHello from rank 0, thread 0, on nid03554. (core affinity = 0,64,128,192)\nHello from rank 0, thread 5, on nid03554. (core affinity = 40,104,168,232)\nHello from rank 0, thread 1, on nid03554. (core affinity = 8,72,136,200)\nHello from rank 0, thread 2, on nid03554. (core affinity = 16,80,144,208)\nHello from rank 0, thread 4, on nid03554. (core affinity = 32,96,160,224)\nHello from rank 0, thread 3, on nid03554. (core affinity = 24,88,152,216)\nHello from rank 0, thread 7, on nid03554. (core affinity = 56,120,184,248)\nHello from rank 0, thread 6, on nid03554. (core affinity = 48,112,176,240)\nApplication 19165962 resources: utime ~0s, stime ~1s, Rss ~6284, inblocks ~0, outblocks ~8\n</code></pre>"},{"location":"theta/programming-models/openmp-theta/#performance-notes","title":"Performance Notes","text":"<p>Affinity of OpenMP threads to hardware threads can have a large impact on performance. Please see Affinity for more information.</p>"},{"location":"theta/programming-models/raja/","title":"Overview","text":"<p>RAJA is a collection of C++ software abstractions, being developed at Lawrence Livermore National Laboratory (LLNL), that enable architecture portability for HPC applications. The overarching goals of RAJA are to:</p> <ul> <li>Make existing (production) applications portable with minimal disruption</li> <li>Provide a model for new applications so that they are portable from inception.</li> </ul> <p>RAJA targets portable, parallel loop execution by providing building blocks that extend the generally-accepted parallel for idiom.</p> <p>Additional information can be found at RAJA User Guide.</p>"},{"location":"theta/programming-models/raja/#using-raja","title":"Using RAJA","text":"<p>RAJA provides a project template for how to use RAJA in an application project that uses CMake or Make. This is located at RAJA Project Template.</p>"},{"location":"theta/programming-models/raja/#how-to-get-the-source-code","title":"How to get the source code","text":"<p>The RAJA source code lives at RAJA github. </p> <p>It can be cloned with <code>git clone --recursive https://github.com/llnl/raja.git</code>. The recursive clone will also clone RAJA's dependencies in the proper locations.</p>"},{"location":"theta/programming-models/raja/#building-on-theta","title":"Building on Theta","text":"<p>RAJA requires a compiler with C++11 support and CMake version 3.9 or greater RAJA includes an example build script for Theta that you can use. A quick example is below.</p> <pre><code>module load cmake/3.14.5\ncd raja\ncp scripts/alcf-builds/theta_intel18.sh .\n./theta_intel18.sh\ncd build_alcf-theta-intel18.0/\nmake -j 4\n</code></pre> <p>The build script makes use of a CMake configuration file <code>raja/host-configs/alcf-builds/theta_intel18_0.cmake</code>. The RAJA and compiler options can be adjusted in this configuration file.</p>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/","title":"Affinity on Theta","text":""},{"location":"theta/queueing-and-running-jobs/affinity-theta/#introduction","title":"Introduction","text":"<p>Each KNL node on Theta has 32 tiles, each with 2 physical cores. Each physical core has 4 hardware threads, for a total of 64 physical cores and 256 hardware threads per KNL node. When a parallel job is run, the job must have some way of mapping specific ranks or threads to each of the 256 hardware threads. Mapping is typically done by an \u201caffinity mask\u201d, which assigns hardware threads to each MPI rank or thread to use.</p> <p>A visual representation of two tiles in a KNL is shown below. Each tile is represented by a large white rectange. The two large grey squares in each tile represent the physical cores. Inside of each core are four hardware threads, represented by splitting the grey box into four quadrants. </p> <p>The numbers inside the quadrants identify the specific hardware threads in the core. That is, hardware threads 0, 64, 128, and 192 are the 4 hardware threads on the first physical core. Hardware threads 1, 65, 129, and 193 are the 4 hardware threads that share the second physical core. Since there are 256 hardware threads (or logical cores), the numbers run from 0 to 255. For i from 0 to 63, hardware threads i, i+64, i+642, and i+643 share a physical core.</p> <p> </p> Visual representation of two tiles in a KNL <p>Using the -j, -d, and --cc arguments to aprun and environment variables, MPI ranks and threads can be assigned to run on specific hardware threads. For more information about the flags to aprun, see  Running Jobs on Theta. Four examples of using aprun are given below followed by descriptions of two methods for displaying the mapping produced.</p> <p>Note: Logical core and hardware thread are used interchangeably below.</p>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/#examples-of-thread-affinity","title":"Examples of Thread Affinity","text":""},{"location":"theta/queueing-and-running-jobs/affinity-theta/#example-1-2-nodes-64-ranksnode-1-threadrank-1-rankcore","title":"Example 1: 2 nodes, 64 ranks/node, 1 thread/rank, 1 rank/core","text":"<p>The following four examples show how the -j and -d aprun arguments can affect where the MPI ranks and OpenMP threads are mapped.</p> <pre><code>aprun -n 128 -N 64 -d 1 -j 1 --cc depth &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The \"-n 128\" argument says to use 128 MPI ranks in total and \"-N 64\" places 64 ranks per node.</li> <li>The \"-d 1\" argument says to use 1 hardware thread for each MPI rank.</li> <li>The \"-j 1\" argument says to use only one hardware thread per physical core.</li> </ul>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/#resulting-mapping","title":"Resulting mapping","text":"<p>MPI ranks 0,1,2,..63 map to hardware threads 0,1,2,...63 on each of the two nodes. Assuming the job was allocated on node 0 and node 1:</p> <ul> <li> <p>MPI rank 0 \u2192 node 0, hardware thread 0</p> </li> <li> <p>MPI rank 1 \u2192 node 0, hardware thread 1</p> </li> <li> <p>MPI rank 2 \u2192 node 0, hardware thread 2</p> </li> </ul> <p>...</p> <ul> <li> <p>MPI rank 64 \u2192 node 1, hardware thread 0</p> </li> <li> <p>MPI rank 65 \u2192 node 1, hardware thread 1</p> </li> <li> <p>MPI rank 66 \u2192 node 1, hardware thread 2</p> </li> </ul> <p>...</p> <p>This is shown below, where the presence of an MPI rank on a hardware thread is shown in blue, and the active hardware threads are numbered.</p> <p> </p> MPI rank on a hardware thread is shown in blue, and the active hardware threads are numbered."},{"location":"theta/queueing-and-running-jobs/affinity-theta/#example-2-2-nodes-32-ranksnode-4-threadsrank-2-threadscore","title":"Example 2: 2 nodes, 32 ranks/node, 4 threads/rank, 2 threads/core","text":"<p>The example below shows the affinity flags in aprun to map MPI ranks and OpenMP threads for an MPI/OpenMP job. <pre><code>aprun -n 64 -N 32 -d 4 -j 2 --cc depth -e OMP_NUM_THREADS=4 &lt;app&gt; &lt;app_args&gt;\n</code></pre></p> <ul> <li>The \"-n 64\" argument says to use 64 MPI ranks in total and \"-N 32\" places 32 ranks per node.</li> <li>The \"-d 4\" argument says to use 4 hardware threads for each MPI rank.</li> <li>The \"-j 2\" argument says to use two hardware threads per physical core.</li> <li>The \"-e OMP_NUM_THREADS=4\" arguments tells the application to spawn 4 OpenMP threads per MPI rank.</li> </ul>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/#resulting-mapping_1","title":"Resulting mapping","text":"<p>Assuming the job was allocated on node 0 and node 1:</p> <ul> <li> <p>MPI rank 0, OpenMP thread 0 \u2192 node 0, hardware thread 0</p> </li> <li> <p>MPI rank 0, OpenMP thread 1 \u2192 node 0, hardware thread 1</p> </li> <li> <p>MPI rank 0, OpenMP thread 2 \u2192 node 0, hardware thread 64</p> </li> <li> <p>MPI rank 0, OpenMP thread 3 \u2192 node 0, hardware thread 65</p> </li> <li> <p>MPI rank 1, OpenMP thread 0 \u2192 node 0, hardware thread 2</p> </li> <li> <p>MPI rank 1, OpenMP thread 1 \u2192 node 0, hardware thread 3</p> </li> </ul> <p>...</p> <ul> <li> <p>MPI rank 32, OpenMP thread 0 \u2192 node 1, hardware thread 0</p> </li> <li> <p>MPI rank 32, OpenMP thread 1 \u2192 node 1, hardware thread 1</p> </li> <li> <p>MPI rank 32, OpenMP thread 2 \u2192 node 1, hardware thread 64</p> </li> </ul> <p>...</p> <p>The mapping is shown below, where the presence of the master thread (thread id 0) associated with an MPI rank on a hardware thread is shown in blue, other active threads are shown in purple, and active hardware threads are numbered.</p> <p> </p> Master thread (thread id 0) associated with an MPI rank on a hardware thread is shown in blue, other active threads are shown in purple, and active hardware threads are numbered."},{"location":"theta/queueing-and-running-jobs/affinity-theta/#example-3-1-node-1-ranknode-64-threadsrank-1-threadcore","title":"Example 3: 1 node, 1 rank/node, 64 threads/rank, 1 thread/core","text":"<p>The example below shows the affinity flags in aprun to map MPI ranks and OpenMP threads for an MPI/OpenMP job.</p> <pre><code>aprun -n 1 -N 1 -d 64 -j 1 -cc depth -e OMP_NUM_THREADS=64 &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The \"-n 1\" argument says to use 1 MPI rank in total and \"-N 1\" places 1 rank per node.</li> <li>The \"-d 64\" argument says to use 64 hardware threads for each MPI rank.</li> <li>The \"-j 1\" argument says to use one hardware thread per physical core.</li> <li>The \"-e OMP_NUM_THREADS=64\" arguments tells the application to spawn 64 OpenMP threads per MPI rank.</li> </ul>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/#resulting-mapping_2","title":"Resulting mapping","text":"<p>64 OpenMP threads are allocated on one node. All 64 physical cores are used and each OpenMP thread is placed on its own physical core, since \"-j 1\" specifies using one hardware thread per core.</p> <ul> <li> <p>OpenMP threads 0 to 63 map to hardware threads 0 through 63:</p> </li> <li> <p>MPI rank 0, OpenMP thread 0 \u2192 node 0, hardware thread 0</p> </li> <li> <p>MPI rank 0, OpenMP thread 1 \u2192 node 0, hardware thread 1</p> </li> <li> <p>MPI rank 0, OpenMP thread 2 \u2192 node 0, hardware thread 2</p> </li> <li> <p>MPI rank 0, OpenMP thread 3 \u2192 node 0, hardware thread 3</p> </li> <li> <p>MPI rank 0, OpenMP thread 4 \u2192 node 0, hardware thread 4</p> </li> <li> <p>MPI rank 0, OpenMP thread 5 \u2192 node 0, hardware thread 5</p> </li> </ul> <p>...</p>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/#example-4-1-node-1-ranknode-64-threadsrank-4-threadscore","title":"Example 4: 1 node, 1 rank/node, 64 threads/rank, 4 threads/core","text":"<p>The example below shows the affinity flags in aprun to map MPI ranks and OpenMP threads for an MPI/OpenMP job.</p> <pre><code>aprun -n 1 -N 1 -d 64 -j 4 -cc depth -e OMP_NUM_THREADS=64 &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The \"-n 1\" argument says to use 1 MPI rank in total and \"-N 1\" places 1 rank per node.</li> <li>The \"-d 64\" argument says to use 64 hardware threads for each MPI rank.</li> <li>The \"-j 4\" argument says to use four hardware threads per physical core.</li> <li>The \"-e OMP_NUM_THREADS=64\" arguments tells the application to spawn 64 OpenMP threads per MPI rank.</li> </ul>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/#resulting-mapping_3","title":"Resulting mapping","text":"<p>64 OpenMP threads are allocated on one node. Only 16 phyiscal cores are used and all OpenMP threads are packed into the first 16 physical cores, since \"-j 4\" specifies using all 4 hardware threads per core.</p> <p>The mapping is shown below where the OpenMP threads given in ranges map to hardware threads given in ranges sequentially:</p> <ul> <li> <p>MPI rank 0, OpenMP threads 0-15 \u2192 node 0, hardware threads 0-15</p> </li> <li> <p>MPI rank 0, OpenMP threads 16-31 \u2192 node 0, hardware threads 64-79</p> </li> <li> <p>MPI rank 0, OpenMP threads 32-47 \u2192 node 0, hardware threads 128-143</p> </li> <li> <p>MPI rank 0, OpenMP threads 48-63 \u2192 node 0, hardware threads 192-207</p> </li> </ul> <p>Since hardware threads 0, 64, 128, and 192 are on same physical core, this means that 4 OpenMP threads (OpenMP threads 0, 16, 32, and 48) share a physical core.</p>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/#methods-to-display-thread-affinity","title":"Methods to display thread affinity:","text":"<p>In the examples below, two methods to display the mapping between MPI rank (or OpenMP thread) to hardware threads are shown. Note that \"logical core\" and \"hardware thread\" are used interchangably below.</p>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/#1-sched_getaffinity-in-the-hello_affinity-program","title":"1. sched_getaffinity in the hello_affinity program","text":"<p>One way to display the mapping of ranks to cores is with the \"hello_affinity\" program on Theta, see: https://github.com/argonne-lcf/GettingStarted/blob/master/Examples/theta/affinity/submit.sh. For each OpenMP thread on each rank, \"hello_affinity\" prints which hardware thread (or logical core) the OpenMP threads map to for the given aprun settings (or environment variables in some cases). One can compare the output for different aprun settings and see how the affinity changes.</p> <p>As an example, 8 MPI ranks are launched on a single node, with each rank on its own physical core. The output shows that rank 0 maps to hardware thread 0, and rank 1 maps to hardware thread 1, and so on. This means that all 8 ranks are on separate physical cores.</p> <pre><code>user@thetalogin5:~/affinty&gt; cat submit.sh\n#!/bin/sh\n#COBALT -n 1\n#COBALT -t 10\n#COBALT -A Performance\n#COBALT -q debug-cache-quad\n#COBALT --attrs mcdram=cache:numa=quad\n\n# 1 node, 8 ranks/node, 1 thread/rank, 1 thread/core\n aprun -n 8 -N 8 --cc depth -d 1 -j 1 ./hello_affinity\n\nuser@thetalogin5:~/affinty&gt; qsub ./submit.sh\nJob routed to queue \"debug-cache-quad\".\nMemory mode set to cache quad for queue debug-cache-quad\n295130\n\nuser@thetalogin5:~/affinty&gt; cat 295130.output\nTo affinity and beyond!! nname= nid03834  rnk= 3  tid= 0: list_cores= (3) \nTo affinity and beyond!! nname= nid03834  rnk= 7  tid= 0: list_cores= (7) \nTo affinity and beyond!! nname= nid03834  rnk= 5  tid= 0: list_cores= (5) \nTo affinity and beyond!! nname= nid03834  rnk= 6  tid= 0: list_cores= (6) \nTo affinity and beyond!! nname= nid03834  rnk= 2  tid= 0: list_cores= (2) \nTo affinity and beyond!! nname= nid03834  rnk= 4  tid= 0: list_cores= (4) \nTo affinity and beyond!! nname= nid03834  rnk= 0  tid= 0: list_cores= (0) \nTo affinity and beyond!! nname= nid03834  rnk= 1  tid= 0: list_cores= (1) \nApplication 9888541 resources: utime ~0s, stime ~1s, Rss ~6648, inblocks ~0, outblocks ~8\n</code></pre> <ul> <li>\"nname\" is the name of the node.</li> <li>\"rnk\" is the MPI rank.</li> <li>\"tid\" is the thread id (For OpenMP codes, tid is the OpenMP thread. For MPI-only codes, tid is 0 for each rank).</li> <li>\"list_cores\" lists the hardware threads (or logical cores) that the MPI rank and thread map to.</li> </ul> <p>Some other examples are in comments in the submission script on Theta, see https://github.com/argonne-lcf/GettingStarted/blob/master/Examples/theta/affinity/submit.sh.</p>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/#2-mpich_cpumask_display","title":"2. MPICH_CPUMASK_DISPLAY","text":"<p>Another way to display the mapping of ranks to cores is to set the environment variable MPICH_CPUMASK_DISPLAY to true in your submission script. An example that corresponds to the hello_affinity example above is shown below. </p> <p>As above, 8 MPI ranks are launched on a single node. For each rank, a cpumask is printed out, showing the mapping of the ranks to logical cores. For each cpumask, the rightmost 64 values correspond to logical cores 0 to 63, the next rightmost are logical cores 64 to 127, the next are 128 to 191, and then 192 to 255. A \"1\" in a spot within the cpumask list means the MPI rank is allowed to use that logical core/hardware thread.</p> <p>In this example, PE_0 corresponds to rank 0 and its cpumask shows that it is mapped to logical core 0. PE_1 corresponds to rank 1 and its cpumask shows that it is mapped to logical core 2. In this case, no ranks share a physical core.</p> <pre><code>user@thetalogin5:~/affinty&gt; cat submit.sh \n#!/bin/sh #COBALT -n 1 \n#COBALT -t 10 \n#COBALT -A Performance \n#COBALT -q debug-cache-quad\n#COBALT --attrs mcdram=cache:numa=quad \nexport MPICH_CPUMASK_DISPLAY=true aprun -n 8 -N 8 --cc depth -d 1 -j 1 ./hello_world user@thetalogin5:~/affinty&gt; qsub ./submit.sh \n\nJob routed to queue \"debug-cache-quad\". Memory mode set to cache quad for queue debug-cache-quad 295129\n\nuser@thetalogin5:~/affinty&gt; cat 295129.error [PE_0]:cpumask set to 1 cpu on nid03834,cpumask = 0000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000001 \n\n[PE_1]:cpumask set to 1 cpu on nid03834,cpumask = 0000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000010 \n\n[PE_2]:cpumask set to 1 cpu on nid03834,cpumask = 0000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000100 \n\n[PE_3]:cpumask set to 1 cpu on nid03834,cpumask = 0000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000001000 \n\n[PE_4]:cpumask set to 1 cpu on nid03834,cpumask = 0000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000010000 \n\n[PE_5]:cpumask set to 1 cpu on nid03834,cpumask = 0000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000100000 \n\n[PE_6]:cpumask set to 1 cpu on nid03834,cpumask = 0000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000001000000 \n\n[PE_7]:cpumask set to 1 cpu on nid03834,cpumask = 0000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000000000000 \\ 0000000000000000000000000000000000000000000000000000000000000000010000000\n</code></pre>"},{"location":"theta/queueing-and-running-jobs/affinity-theta/#other-resources","title":"Other resources","text":"<ul> <li>aprun man page</li> <li>Getting Started</li> <li>Using OpenMP Effectively on Theta</li> </ul>"},{"location":"theta/queueing-and-running-jobs/example-job-scripts/","title":"Example Job Scripts","text":"<p>This page contains a small collection of example job scripts users may find useful for submitting their jobs on Theta. Additional information on Cobalt and how to submit these job scripts is available here. A simple example using a similar script on Polaris is available in the Getting Started Repo.</p>"},{"location":"theta/queueing-and-running-jobs/example-job-scripts/#cpu-mpi-example","title":"CPU MPI Example","text":"<p>The following <code>submit.sh</code> example submits a 2-node job to Theta with 64 MPI ranks per node and 1 MPI rank per core. </p> <pre><code>#!/bin/bash\n#COBALT -n 2 \n#COBALT -t 30 \n#COBALT -A Comp_Perf_Workshop \n#COBALT -q comp_perf_workshop \n#COBALT --attrs mcdram=cache:numa=quad \n#COBALT --attrs filesystems=home,theta-fs0\n\necho \"COBALT_JOBID = \" $COBALT_JOBID\necho \"COBALT_JOBSIZE (nodes) =\" $COBALT_JOBSIZE\necho \"COBALT_PARTNAME = \" $COBALT_PARTNAME\n\nNNODES=${COBALT_JOBSIZE}\nNRANKS_PER_NODE=64\nNTHREADS_PER_CORE=1\nNDEPTH=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\n\n# option  long version            (explanation)\n#\n# -n                              \"PEs\" (ranks)\n# -N      --pes-per-node          ranks per node\n# -d      --cpus-per-pe           hyperthreads per rank\n# -cc     --cpu-binding depth\n# -j                              cpus (hyperthreads) per compute unit (core)\n\naprun -n ${NTOTRANKS} -N ${NRANKS_PER_NODE} -d ${NDEPTH} -j ${NTHREADS_PER_CORE} -cc depth ./hellompi\nstatus=$?\n\necho \"Exit status of aprun is: $status\"\nexit $status\n</code></pre> <p>Each Theta node has 1 KNL CPU with a total of 64 cores and each core supports 4 threads. The process affinity in this example is setup to map each MPI rank to 1 core. In this example, a special ALCF training event queue was used to illustrate the need to specify <code>--attrs mcdram=cache:numa=quad</code> to select the memory mode. Outside of a training event, this small test job would need to be submitted to one of the debug queues (<code>-q debug-cache-quad</code> or <code>-q debug-flat-quad</code>), where the memory mode is implied. Applications must be launched with <code>aprun</code> for them to run on the KNL compute nodes. Information on the use of <code>aprun</code> is available via <code>man aprun</code>. Some notes on the specific options used in the above example follow.</p> <ul> <li><code>-n ${NTOTRANKS}</code> : This is specifying the total number of MPI ranks to start as determined by the total number of nodes allocated to the job (<code>${COBALT_JOBSIZE}</code>) and number of MPI ranks to launch on each node (<code>NRANKS_PER_NODE</code>).</li> <li><code>-N ${NRANKS_PER_NODE}</code> : This is specifying the number of MPI ranks to start on each node.</li> <li><code>-d ${NDEPTH}</code> : This is specifying how many cores/threads to space MPI ranks apart on each node.</li> <li><code>-j ${NTHREADS_PER_CORE}</code> : This is indicating the number of hardware threads that will be active on each core. This value can be 1, 2, or 4.</li> <li><code>-cc depth</code> : This is specifying how to bind processes to cores/threads. The <code>depth</code> option will set affinity such that MPI ranks are spaced apart by the argument to <code>-d</code> iterating over hardware threads specified by <code>-j</code>.</li> </ul>"},{"location":"theta/queueing-and-running-jobs/example-job-scripts/#cpu-mpi-openmp-example","title":"CPU MPI-OpenMP Example","text":"<p>Using the MPI-only job submission example above as a baseline, there are not many additional changes needed to use OpenMP parallelism with an application. In the following 2-node example, 64 MPI ranks will be started on each node. Each MPI rank will be spaced apart by 2 threads as determined by <code>-d 2</code> and achieves the same process affinity as in the earlier MPI-only example because of <code>-j 2</code> in this example. The number of OpenMP threads is specified by the environment variable <code>OMP_NUM_THREADS</code> and passed via the <code>-e</code> option to <code>aprun</code>. Some simple examples using a similar job submission script on Theta is available in the Getting Started Repo.</p> <pre><code>#!/bin/bash\n#COBALT -n 2 \n#COBALT -t 30 \n#COBALT -A Catalyst\n#COBALT -q debug-cache-quad\n#COBALT --attrs filesystems=home,theta-fs0\n\nNNODES=${COBALT_JOBSIZE}\nNRANKS_PER_NODE=64\nNTHREADS_PER_CORE=2\nNDEPTH=2\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\n\naprun -n ${NTOTRANKS} -N ${NRANKS_PER_NODE} -d ${NDEPTH} -j ${NTHREADS_PER_CORE} -cc depth -e OMP_NUM_THREADS=2 ./a.out\n</code></pre> <p>Additional discussion and examples of how to specify process affinity on Theta is available in the Affinity on Theta page.</p>"},{"location":"theta/queueing-and-running-jobs/example-job-scripts/#bundling-multiple-runs-within-a-job","title":"Bundling Multiple Runs Within a Job","text":""},{"location":"theta/queueing-and-running-jobs/example-job-scripts/#running-many-jobs-one-after-another","title":"Running many jobs one after another","text":"<p>The simplest way of bundling many apruns in a script is simply to list one after another. The apruns will run one at a time sequentially. Each aprun can use up to the number of nodes that were requested in the initial qsub. The following script is an example of launching multiple runs within a script, where each aprun requests the same number of nodes.</p> <p><pre><code>#!/bin/bash\n#COBALT -n 2 \n#COBALT -t 30 \n#COBALT -A Catalyst\n#COBALT -q debug-cache-quad\n#COBALT --attrs filesystems=home,theta-fs0\n\nNNODES=${COBALT_JOBSIZE}\nNRANKS_PER_NODE=64\nNTHREADS_PER_CORE=2\nNDEPTH=2\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\n\naprun -n ${NTOTRANKS} -N ${NRANKS_PER_NODE} -d ${NDEPTH} -j ${NTHREADS_PER_CORE} -cc depth -e OMP_NUM_THREADS=2 ./a.out\naprun -n ${NTOTRANKS} -N ${NRANKS_PER_NODE} -d ${NDEPTH} -j ${NTHREADS_PER_CORE} -cc depth -e OMP_NUM_THREADS=2 ./a.out\naprun -n ${NTOTRANKS} -N ${NRANKS_PER_NODE} -d ${NDEPTH} -j ${NTHREADS_PER_CORE} -cc depth -e OMP_NUM_THREADS=2 ./a.out\n</code></pre> The aprun command blocks until task completion, at which point it exits, providing a convenient way to run multiple short jobs together. In addition, if a subset of nodes is requested, aprun will place jobs on nodes in the script\u2019s reservation until the pool of inactive nodes is exhausted. If the number of nodes requested by an aprun exceeds the number of nodes reserved by the batch scheduler for the job (through the qsub command), that aprun will fail to execute and an error will be returned.</p>"},{"location":"theta/queueing-and-running-jobs/example-job-scripts/#running-many-jobs-at-the-same-time","title":"Running many jobs at the same time","text":"<p>Multiple simultaneous apruns can be launched by backgrounding the aprun commands in the script and then waiting for completion. A short sleep between apruns is recommended to avoid a potential race condition during a large number of aprun starts. As an example, the following script will launch 4 simultaneous apruns, which execute on the compute nodes at the same time. The first aprun listed runs on 2 nodes and the others each run on 1 node. Since the apruns are backgrounded (as denoted by the &amp;), the script must have a <code>wait</code> command at the end so that it does not exit before the apruns complete.</p> <p><pre><code>#!/bin/bash\n#COBALT -n 4 \n#COBALT -t 30 \n#COBALT -A Catalyst\n#COBALT -q debug-cache-quad\n#COBALT --attrs filesystems=home,theta-fs0\n\nNTHREADS_PER_CORE=2\nNDEPTH=2\n\naprun -n 128 -N 64 -d ${NDEPTH} -j ${NTHREADS_PER_CORE} -cc depth -e OMP_NUM_THREADS=2 ./a.out &amp;\nsleep 1\n\naprun -n 64 -N 64 -d ${NDEPTH} -j ${NTHREADS_PER_CORE} -cc depth -e OMP_NUM_THREADS=2 ./a.out &amp;\nsleep 1\n\naprun -n 64 -N 64 -d ${NDEPTH} -j ${NTHREADS_PER_CORE} -cc depth -e OMP_NUM_THREADS=2 ./a.out &amp;\nwait\n</code></pre> Each <code>aprun</code> command will launch executables onto separate sets of nodes. It's not currently possible to run multiple <code>aprun</code> instances on the same node at the same time.</p> <p>There is a system limitation of 1,000 simultaneous aprun invocations in a job script. if this limit is hit, you will see the following error. <pre><code>apsched: no more claims allowed for this reservation (max 1000)\n</code></pre></p>"},{"location":"theta/queueing-and-running-jobs/example-job-scripts/#using-a-workflow-manager","title":"Using a Workflow Manager","text":"<p>There are a variety of workflow managers that can assist bundling jobs together. A few are listed below:</p> <ul> <li> <p>Balsam</p> </li> <li> <p>Parsl </p> </li> <li> <p>Scheduler.x </p> </li> <li> <p>Swift </p> </li> </ul>"},{"location":"theta/queueing-and-running-jobs/gronkulator/","title":"The Gronkulator: Job Status Display","text":""},{"location":"theta/queueing-and-running-jobs/gronkulator/#overview","title":"Overview","text":"<p>The Gronkulator, or \u201cThe Gronk\u201d is a web-based tool that provides a pictorial depiction of the jobs running on ALCF supercomputers. The tool also provides information about the job queue and machine reservations. The tool can be accessed via status.alcf.anl.gov.</p> <p>Note: If you are logged into the system, all of the information contained on the Gronkulator is available via the \u201cqstat\u201d command. There are more options in the qstat command that are not available on the Gronkulator. </p> <p>To view machine activity, click on the name of one of our supercomputers. Upon clicking, you will see all the jobs running on the resource. The running jobs are color-coded by the name of the project allocation that the job is being charged to. </p> <p> </p> Theta activity <p>Under the color-coded blocks, the \"Running\" tab will be activated. This tab displays the length of time for the job has been running (i.e. \u201cRunning Time\u201d), the requested \u201cWall Time\u201d, its location on the machine (i.e. the specific rack partitions on which the job is running), the number of compute nodes the job is running, and the mode (i.e. the number of MPI threads that the job is running on, on each compute node).</p> <p>There are three additional tabs: Starting, Queued, and Reservations.</p> <p> </p> Theta queued <p>\"Starting\" shows the jobs that will run next. \"Queued\" shows the number of jobs that are queued on the system, and their score. The higher the score the more likelihood of the job running before those that have a lower score, should an appropriately sized partition become available. The third tab \u201cReservations\u201d, shows if any reservations have been placed. </p> <p> </p> Theta reservations <p>For instance, if you have submitted a fairly large job in the queue (large by way of the number of nodes on which the job will run, and/or the length of time for which it will run), and you notice that other jobs with a lower score are being transitioned from the \u201cQueued Jobs\u201d state to the \u201cRunning Jobs\u201d state, then check if there is an upcoming \u201creservation\u201d that is causing the system to \u201cdrain\u201d. As a note, there is almost always a reservation for \u201cpreventative maintenance\u201d.</p> <p>The layout of Cooley (the visualization system) is a bit different than Theta. The squares under a Rack represents a compute node which has 12 cores.</p> <p> </p> Cooley activity"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/","title":"Job Scheduling and Submission on Theta","text":""},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#job-and-queue-scheduling-for-theta","title":"Job and Queue Scheduling for Theta","text":""},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#general-policy","title":"General Policy","text":"<p>We ask that all users follow good etiquette and be kind to one another.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#job-priority","title":"Job Priority","text":"<p>As with all Argonne Leadership Computing Facility production systems, job priority in the queue is based on several criteria:</p> <ul> <li>positive balance of your project</li> <li>size (in nodes) of the job, larger jobs receive higher priority</li> <li>the type of project (e.g. INCITE, ALCC, or discretionary)</li> <li>job duration - shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible</li> </ul>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#reservations-and-scheduling-policy","title":"Reservations and Scheduling Policy","text":"<p>Some work will require use of Theta that requires deviation from regular policy. On such occasions, normal reservation policy applies. Please send the regular form no fewer than five (5) business days in advance.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#monday-maintenance","title":"Monday Maintenance","text":"<p>When the ALCF is on a regular business schedule, preventitive maintenance is typically scheduled on alternate Mondays. The showres command may be used to view pending and active maintenance reservations.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#big-run-mondays","title":"Big Run Mondays","text":"<p>As part of our regular maintenance procedures on Mondays, we will promote to the highest priority any jobs in the queued state requesting 802 nodes or more (.ie. capability jobs). Promotion is subject to operational discretion.</p> <p>We may also, at our discretion, take the opportunity to promote the priority of capability jobs if the system has been drained of jobs for any other reason.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#incitealcc-overburn-policy","title":"INCITE/ALCC Overburn Policy","text":"<p>If an INCITE or ALCC project has exhausted its allocation in the first 11 months of its allocation year, it is eligible for overburn running. At this point, capability jobs submitted by INCITE and ALCC projects will run in the default queue (instead of backfill) for the first 11 months of the allocation year until 125% of the project allocation has been consumed. Note that non-capability jobs will be routed to backfill queue.</p> <p>INCITE and ALCC projects needing additional overburn hours should e-mail support@alcf.anl.gov with a short description of what they plan to do with the additional hours, highlighting specific goals or milestones and the time expected to accomplish them. This will be reviewed by the scheduling committee, allocations committee, and ALCF management. Requests should be submitted 15 days before the start of the next quarter of the allocation year for full consideration. Non-capability jobs from projects that have exhausted their allocation will continue to run in backfill. </p> <p>To be clear, this policy does not constitute a guarantee of extra time, and we reserve the right to prioritize the scheduling of jobs submitted by projects that have not yet used 100% of their allocations, so the earlier that an INCITE or ALCC project exhausts its allocation, the more likely it is to be able to take full advantage of this policy.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#queues","title":"Queues","text":"<p>Information on ThetaGPU Queues</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#theta-knl-queues","title":"Theta (KNL) Queues","text":"<p>Debugging Queues</p> <ul> <li>There are two 16-node debugging queues: <ul> <li>debug-cache-quad</li> <li>debug-flat-quad</li> </ul> </li> <li>Hardware is dedicated to each queue</li> <li>Nodes are not rebootable to another mode</li> <li>Minimum allocation of 1 node</li> <li>Maximum allocation of 8 nodes</li> <li>Job wall-clock time is limited to 1:00:00 (1 hour).</li> <li>The maximum running job count is one (1) job per user.</li> </ul> <p>Production Queues</p> <ul> <li>There is a single submission queue for the entire system: default</li> <li>Priority is given to jobs using at least 20% of Theta (&gt;=802 nodes) (previously &gt;=648 nodes)</li> <li>There is a global limit of ten (10) jobs running per user</li> <li>There is a global limit of twenty (20) jobs in queue per user</li> <li>There is a minimum job time of thirty (00:30:00) minutes for the default queue</li> <li>Beginning 21 May 2018 there is a minimum allocation of 128 nodes (previously 8 nodes)</li> <li>Jobs smaller than the minimum allocation will be allocated and charged for the minimum allocation of nodes</li> <li>While shorter jobs may accumulate priority faster, all requested wall-clock times (job durations) greater than or equal to 12 hours are treated equivalently.</li> <li>Wall-clock limits are a step-wise function designed to encourage scaling: <ul> <li>node count &gt;= 128 nodes (minimum allocation): maximum 3:00:00 hours</li> <li>node count &gt;= 256 nodes : maximum 6:00:00 hours</li> <li>node count &gt;= 384 nodes : maximum 9:00:00 hours</li> <li>node count &gt;= 640 nodes : maximum 12:00:00 hours</li> <li>node count &gt;= 802 nodes : maximum 24:00:00 hours</li> </ul> </li> <li>There is no default mode nodes may be assumed to be booted into. Failure to specify a mode will result in the assumption of cache-quad.</li> </ul> <p>Note: Jobs for projects with a negative balance (with the exception of capability jobs for projects that have overburn enabled) are automatically routed to the backfill queue. Walltime limit for backfill queue is 20 minutes - 6 hours. Backfill jobs have low priority.</p> <p>ALCF has 260 nodes in the production queue for a research project. The maximum job size is 4,100 nodes.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#running-with-less-than-128-nodes-in-default-queue","title":"Running With Less Than 128 Nodes in Default Queue","text":"<p>It is important to remember that jobs are charged by the number of nodes allocated, not the number of nodes used. For instance, a job requesting 14 nodes, below the minimum of 128, will be allocated 128 nodes and charged for 128 even though only 14 nodes are used.</p> <p>We strongly encourage you to bundle smaller jobs into ensembles, either using Cobalt ensemble scripting or using a workflow system such as Balsam.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#theta-knl-mode-selection-and-charging","title":"Theta (KNL) Mode Selection and Charging","text":"<p>Time spent booting or rebooting nodes to obtain requested modes will not be charged to projects and neither will it count against requested walltime, although it will be included in the total runtime. Please allow up to thirty (30) minutes for rebooting of nodes when submitting jobs.</p> <p>Failure to specify a mode will result in the selection of cache-quad, the equivalent of listing <code>mcdram=cache:numa=quad</code> in your qsub or job script.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#submit-a-job","title":"Submit a Job","text":"<p>The queuing system used at ALCF is Cobalt. On Theta, Cobalt jobs may run either as script jobs or interactive mode jobs.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#script-method","title":"Script Method","text":"<p>In the script method, Cobalt will execute a user-supplied script when running a user\u2019s job. All scripts on Theta must have their execute bit set, and must be able to run on a standard x86_64 architecture. Following are the required flags to qsub, as well as some of the more common options. A complete list of options may be found as a part of the qsub manpage, available on any login node.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#required-flags","title":"Required Flags","text":"<pre><code>-n NN - number of nodes (-n 64 for 64 nodes) \n-t time - running time (-t 5 for 5 minutes, -t 01:10:20 for 1 hr 10 min 20 sec) \n-A Project - project (-A YourProject)\n</code></pre>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#common-options","title":"Common Options","text":"<pre><code>--attrs - you may specify additional attributes for your job. \n          Multiple attribute key-value pairs are colon-delimited. \n          The following are common on the KNL nodes: \n          - location: a comma-separated list of node ids. Ranges may be hyphenated. \n          - mcdram: The desired MCDRAM mode of a job (default: cache) \n          - numa: The desired NUMA mode of a job (default: quad)\n\n          The following are common on the GPU nodes:          \n          - location: a comma-separated list of node names (not IDs). Ranges may NOT be hyphenated on the GPU nodes. \n          - mig-mode: Should the GPUs be put in Multi Instance GPU (MIG) mode (default: False) \n\nSee: [https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html)\n\n--env VAR1=1:VAR2=2:\u2026 - specify required environment variables\n-i file - give a file name to be used for stdin \n-O Name - name your job and stdout/stderr (-O Job1) \n-q queue - queue (only for special cases, such as debug queues (debug-cache-quad, debug-flat-quad))\n</code></pre> <p>Note: Remember to give all options before the executable name.</p> <p>See Submitting to Specific Nodes below to see the difference between KNL and GPU node location specifications. Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation request. ThetaGPU is listed under Theta on the form.</p> <p>Example <pre><code>qsub -A YourProject -n 256  -t 30  \\\n--env MYVAR=value1 -i inputdata -O Project1_out \\\n\u2013attrs mcdram=flat:numa=quad program.exe progarg1\n</code></pre></p> <p>The syntax for Cobalt scripting is slightly different than that of a PBS script. For more information, see Cobalt scripting.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#requesting-ability-to-ssh-into-the-compute-nodes-on-knl-nodes","title":"Requesting Ability to SSH into the Compute Nodes on KNL Nodes","text":"<p>To be able to ssh from the MOM/launch nodes on Theta to the compute nodes, pass enable_ssh=1 as part of the --attrs argument (see the example below).  Once the job begins on the MOM/launch node, you can ssh (or scp, etc.) from the MOM node to the compute nodes.  The compute node name can be found by reading the $COBALT_PARTNAME number, and prepending \"nid\" with the appropriate number of 0s to reach 5 digits.</p> <p>For example, for an interactive job:</p> <pre><code>n@thetalogin4:~/&gt; qsub -I -n 1 -t 20 --attrs enable_ssh=1 -A project -q debug-cache-quad \nConnecting to thetamom1 for interactive qsub... \nJob routed to queue \"debug-cache-quad\". \nMemory mode set to cache quad for queue debug-cache-quad Wait for job 266815 to start... \nOpening interactive session to 3835 n@thetamom1:/gpfs/mira-home/user&gt; echo $COBALT_PARTNAME \n3835 \nn@thetamom1:/gpfs/mira-home/user&gt; \nssh nid03835 n@nid03835:~&gt; hostname \nnid03835\n</code></pre>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#specifying-filesystems","title":"Specifying Filesystems","text":"<p>On Theta and other systems running Cobalt at the ALCF, your job submission should specify which filesystems your job will be using.  In the event that a filesystem becomes unavailable, this information is used to preserve jobs that would use that filesystem while allowing other jobs that are not using an affected filesystem to proceed to run normally. </p> <p>You may specify your filesystem by adding filesystems= to the --attrs argument of qsub in Cobalt. Valid filesystems are home, eagle, grand, and theta-fs0. The list is comma-delimited. For example, to request the home and eagle filesystems for your job you would add filesystems=home,eagle to your qsub command. If this is not specified, a warning will be printed and then the job will be tagged as requesting all filesystems and may be held unnecessarily if a filesystem is not currently available. The warnings are written to stderr of qsub and qalter commands that change the value of the --attrs flag.  Scripts that are parsing stderr from these utilities may encounter errors from the additional warnings if filesystems are not specified in these commands. <p>If a job is submitted while a filesystem it requested is marked down, the job will automatically be placed into a user_hold and a warning message will be printed, but the job will be otherwise queued. The job is also placed into admin_hold by a sysadmin script. Once the affected filesystem has been returned to normal operation, the admin_hold is released. You are responsible for releasing the user_hold once you receive the message that the affected filesystem has been returned to normal operation. The job cannot run until both the holds are released.</p> <p>If a job requesting a filesystem that is marked down is already in the queue, it will be placed on admin_hold and will be released once the filesystem is operational.</p> <p>An example of a job requesting filesystems: <pre><code>qsub -n 128 -t 30 -q default --attrs filesystems=home,grand -A Project ./my_job.sh\n</code></pre></p> <p>To update the filesystems list for your job, use qalter. Note that qalter --attrs is a replace and not an update operation. This means that you should once again specify all the attributes that you had in the original qsub command.</p> <pre><code>qalter --attr filesystems=home,eagle:mcdram=cache:numa=quad &lt;jobid&gt;\n</code></pre> <p>To release user hold: <pre><code>qrls &lt;jobid&gt;\n</code></pre></p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#interactive-method","title":"Interactive Method","text":"<p>To run an \u201cinteractive mode\u201d job on ALCF Cray resources, add the \u201c-I\u201d flag or \u201c--mode interactive\u201d to your qsub line and omit any executable. Your qsub submission will then wait until nodes are allocated to your job and Cobalt will start a shell on a job-launch node on your behalf. You may aprun against your assigned resources and run other interactive commands from this node. It is important to note that your shell is executed from a launch node and not from your compute head-node. Once your allocation ends, all apruns will be terminated, but your shell will remain for any cleanup actions that you choose to take.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#ensemble-jobs","title":"Ensemble Jobs","text":"<p>Users may run an \u201censemble job\u201d and combine runs into a single script. This can provide major enhancements to throughput, especially for large ensemble jobs. Users may run multiple jobs in sequence or may use multiple backgrounded apruns to subset their resources among multiple backend executables. There is a system limitation of 1000 simultaneous apruns per Cobalt script job.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#project-names","title":"Project Names","text":"<p>You can find active project names that your account is associated with by running the command: <pre><code>sbank allocations\n</code></pre> If an account is associated with more than one project, a job must be submitted by using a specific project name using -A or by setting the environment variable COBALT_PROJ.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#submitted-job-with-the-wrong-arguments","title":"Submitted Job with the Wrong Arguments","text":"<p>If you submit a job with the wrong arguments, you can modify without deleting it and resubmitting it. Most settings can be changed using qalter.</p> <p>For Example: <pre><code> Usage: qalter [-d] [-v] -A &lt;project name&gt; -t &lt;time in minutes&gt;\n             -e &lt;error file path&gt; -o &lt;output file path&gt;\n             --dependencies &lt;jobid1&gt;:&lt;jobid2&gt;\n             -n &lt;number nodes of&gt; -h --proccount &lt;processor count&gt;\n             -M &lt;email address&gt; &lt;jobid1&gt; &lt;jobid2&gt;\n</code></pre> Note: To change the queue, use qmove. <pre><code>Usage: qmove &lt;queue name&gt; &lt;jobid&gt; &lt;jobid&gt;\n</code></pre></p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#changing-executable-after-job-submission","title":"Changing Executable after Job Submission","text":"<p>When a job is submitted via qsub, Cobalt records the path to the executable or script, but it does not make a copy. As a result, if the executable or script is modified when there is a deletion or modification, it will affect any jobs already submitted that use that executable. To avoid confusion, it is generally best to avoid making changes after job submission.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#holding-and-releasing-jobs","title":"Holding and Releasing Jobs","text":""},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#user-holds","title":"User Holds","text":"<p>To hold a job (prevent from running), use qhold. This will put the job in the \"user_hold\" state. <pre><code>qhold &lt;jobid&gt;\n</code></pre> To release a job in a user hold (user_hold) state, use qrls. <pre><code>qrls &lt;jobid&gt;\n</code></pre></p> <p>A job may also be put into a user hold immediately upon submission by passing qsub the -h flag. <pre><code>qsub -n 512 -t 120 -A MyProject -h myExe\n</code></pre></p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#dependency-holds","title":"Dependency Holds","text":"<p>For jobs in the dep_hold or dep_fail state, see the section on job dependencies.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#admin-holds","title":"Admin Holds","text":"<p>Jobs in the state admin_hold may be released only by a system administrator.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#maxrun-holds","title":"MaxRun Holds","text":"<p>Jobs may temporarily enter the state maxrun_hold if the user has reached the limit of per-user running jobs in a particular queue. No action is required; as running jobs complete, jobs in the maxrun_hold state will be automatically changed back to queued and eligible to run.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#job-dependencies","title":"Job Dependencies","text":"<p>To submit a job that waits until another job or jobs have completed, use the dependencies argument to qsub. For example, to submit a job that depends on job 12345: <pre><code>qsub -n 512 -t 10 -A yourproject --dependencies 12345 a.out\n</code></pre> For multiple dependencies, list and separate with colons. <pre><code>qsub -n 512 -t 10 -A yourproject --dependencies 12345:12346 a.out\n</code></pre> Jobs submitted with dependencies will remain in the state dep_hold until all the dependencies are fulfilled, then will proceed to the state queued.</p> <p>Note: In the event any of the dependencies do not complete successfully (nonzero exit status), the job will instead go into the state dep_fail. To manually release a job that is in either dep_hold or dep_fail: <pre><code>qrls --dependencies &lt;jobid&gt;\n</code></pre> or alternatively change the job's dependencies setting to \u201cnone\u201d: <pre><code>qalter --dependencies none &lt;jobid&gt;\n</code></pre></p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#customizing-the-output-of-qstat","title":"Customizing the Output of Qstat","text":"<p>Default fields displayed by the qstat command may be changed by setting the QSTAT_HEADER environment variable. <pre><code>export QSTAT_HEADER=\"JobID:JobName:User:WallTime:RunTime:Nodes:State:attrs:Queue\"\"\nqstat\n\n\u200bJobID   JobName                           User      WallTime  RunTime   Nodes  State      attrs             Queue\n     =======================================================================================================================================\n     104927  N/A                               user1     02:00:00  01:20:45  128    running    {'numa': 'quad', 'mcdram': 'cache'}  backfill\n     104941  N/A                               user2     00:20:00  N/A       2048   queued     {'numa': 'quad', 'mcdram': 'flat'}   backfill\n     104934  xxxx.yyyyy                        user3     04:00:00  01:10:12  32     running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104948  Xxx-YY_ZZ                         user4     02:00:00  00:15:03  128    running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104919  aaaaa_0000_bbbb_c                 user5     06:00:00  01:50:21  64     running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104945  aaaa_bb_cccc-d_eee_f.hhhhhh.iiii  user6     06:00:00  00:18:25  100    running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104848  bbbbbb                            user7     01:00:00  N/A       3624   queued     {'numa': 'quad', 'mcdram': 'cache'}  default\n     ....\n</code></pre> One may specify column headers via the --header flag to qstat.</p> <p>Available field names can be seen by entering \"qstat -fl \" for any current jobid."},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#redirecting-standard-input","title":"Redirecting Standard Input","text":"<p>To redirect the standard input to a job, do not use the '&lt;' redirection operator on the qsub command line. This simply redirects standard input to qsub, not the job itself. Instead, use the qsub option \"-i\". <pre><code># WRONG\nqsub -t 10 -n 64 a.out &lt; my_input_file.dat\n\n# RIGHT\nqsub -t 10 -n 64 -i my_input_file.dat a.out\n</code></pre></p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#sbank","title":"Sbank","text":"<p>The sbank database is updated hourly. This means transactions against your account can take up to an hour before they show up.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#submitting-into-backfill-nodes","title":"Submitting into Backfill Nodes","text":"<p>Sometimes the scheduler will try to clear up room for a large job. During these times, although not many jobs may be running, new jobs are not being scheduled as expected.</p> <p>At such times, backfill nodes may be available. While nodes are being drained for a larger job, other user jobs may be backfilled onto these resources, provided that their requested wall time is less than the remaining drain time of the set of resources. For instance, suppose that 16 nodes are being drained to allow a 16-node job to run. Of the 16 nodes, perhaps eight are empty and the other eight are running an eight-node job that has 2 hours of wall time left. This allows the opportunity to run a 2-hour, eight-node job in the backfill here.</p> <p>To discover available backfill, run the nodelist command.</p> <p>For example: <pre><code>nodelist\nNode_id  Name         Queues     Status                 MCDRAM  NUMA    Backfill\n================================================================================\n{...]\n20       c0-0c0s5n0   default     cleanup-pending       flat    quad    4:59:44\n21       c0-0c0s5n1   default     cleanup-pending       flat    quad    4:59:44\n22       c0-0c0s5n2   default     busy                  flat    quad    4:59:44\n24       c0-0c0s6n0   default     busy                  flat    quad    4:59:44\n25       c0-0c0s6n1   default     busy                  flat    quad    4:59:44\n26       c0-0c0s6n2   default     busy                  flat    quad    4:59:44\n27       c0-0c0s6n3   default     busy                  flat    quad    4:59:44\n28       c0-0c0s7n0   default     idle                  flat    quad    4:59:44\n29       c0-0c0s7n1   default     idle                  flat    quad    4:59:44\n30       c0-0c0s7n2   default     idle                  flat    quad    4:59:44\n31       c0-0c0s7n3   default     idle                  flat    quad    4:59:44\n32       c0-0c0s8n0   default     idle                  flat    quad    4:59:44\n33       c0-0c0s8n1   default     idle                  flat    quad    4:59:44\n34       c0-0c0s8n2   default     idle                  flat    quad    4:59:44\n\n[...]\n</code></pre> In this example, a four-node job with a maximum wall time of 4 hours and 59 minutes can be run during this backfill. The backfill times will not always be identical and will depend on the mix of jobs on the partitions that are being drained.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#submitting-to-specific-nodes","title":"Submitting to Specific Nodes","text":"<p>In rare cases, there may be a need to target specific hardware. This may be accomplished using \"--attrs location=\".</p> <p>For example:</p> <p><pre><code>qsub -t 10 -n 16 --attrs location=0-7,100-107 myprogram.exe\n</code></pre> This will force the job to run on those specific nodes. Should that location become unschedulable, for instance, due to a failed node, the job will not be allowed to run anywhere else, without resetting the location attribute. If more nodes are specified in the location field than are required to fill a job\u2019s requested node count, then the first n nodes available in the location set will be used.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#running-with-a-group-of-users","title":"Running with a Group of Users","text":"<p>Sometimes it is useful to allow other users to run Cobalt commands on a given job such as qhold, qrls, or qdel. A list of users can be allowed to run commands on your job by submitting a list of users to qsub, cqsub, or qalter using the flag --run_users. Specified users need not be in the same project under which the job was submitted.</p> <p>For example: <pre><code>qsub -A FellowShipOTR -n 512 -t 1:00 --run_users frodo:sam:pippin ./council (KNL)\n</code></pre> As a convenience, all users belonging to the project under which a job was submitted can be added to a list of users that may control a job by using the --run_project flag.</p> <p>Users who have been added to the list can run any command that the job-submitter could run on a job. This includes qhold, qrls, qalter, and qdel.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#group-running-and-file-system-groups","title":"Group Running and File System Groups","text":"<p>While setting this list of users allows any of the listed users to run Cobalt commands on a job, it does not do anything about the permissions of any files involved with the job. Those must be handled by the user(s) setting appropriate permissions on their directories to allow users in their group to read and write files as appropriate. If your project needs a group on the file system to share files or a user needs to be added, email User Support.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#more-information","title":"More Information","text":"<p>For more information on Cobalt commands, their options, consult the manpages on the system. The same information may be found online in the Cobalt Command Reference.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#using-the-job-resource-manager-commands-options-and-examples","title":"Using the Job Resource Manager: Commands, Options, and Examples","text":"<p>This document provides examples of how to submit jobs on our systems. It also provides examples of commands that can be used to query the status of jobs, what partitions are available, etc. </p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#submit-a-job-request","title":"Submit a Job Request","text":"<p>Use qsub to submit a job. (Unlike jobs on the ALCF BlueGene systems, all jobs on Theta are either script or interactive.)</p> <p>Run the script jobscript.sh with 10 nodes for a maximum of 15 minutes: <pre><code>qsub -n 10 -t 15 jobscript.sh\n</code></pre> To submit jobs to a particular queue, use qsub -q . <p>To run jobscript.sh with 10 nodes for a maximum of 30 minutes in the debug queue for flat memory mode and quad numa mode: <pre><code>qsub -q debug-flat-quad -n 10 -t 30 jobscript.sh\n</code></pre></p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#charge-a-job-to-a-project","title":"Charge a Job to a Project","text":"<p>Use qsub -A  to charge a job to a particular project. <p>To run jobscript.sh with 10 nodes for a maximum of 15 minutes and charge the job to MyProject: <pre><code>qsub -n 10 -t 15 -A MyProject jobscript.sh\n</code></pre> To see which projects you are a member of:</p> <pre><code>projects\n</code></pre> <p>You can use the environment variable \u201cCOBALT_PROJ\u201d to set your default project. qsub -A takes precedence over COBALT_PROJ.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#delete-a-job-from-the-queue","title":"Delete a Job from the Queue","text":"<p>To delete a job from the queue, use the qdel command. For example for job with ID of 34586 <pre><code>qdel 34586\n</code></pre></p> <p>Depending on the stage of a job\u2019s lifetime, qdel may not complete immediately, especially if the delete is issued during startup on a job that is changing memory modes and rebooting a node. If the job does not ultimately terminate, contact support@alcf.anl.gov with the jobid so that an administrator can take appropriate cleanup actions and administratively terminate the job.</p>"},{"location":"theta/queueing-and-running-jobs/job-and-queue-scheduling/#query-partition-availability","title":"Query Partition Availability","text":"<p>To determine which partitions are currently available to the scheduler, use the nodelist command. This command provides a list of node ids, names, queue, and state as well as any backfill windows.</p> <p>For example: <pre><code>% nodelist Node_id  Name         Queues Status           MCDRAM NUMA Backfill\n ===============================================================================\n[...] \n20       c0-0c0s5n0 default cleanup-pending flat quad 4:59:44 \n21       c0-0c0s5n1 default cleanup-pending flat quad 4:59:44 \n22       c0-0c0s5n2 default busy flat quad 4:59:44 \n24       c0-0c0s6n0 default busy flat quad 4:59:44 \n25       c0-0c0s6n1 default busy flat quad 4:59:44 \n26       c0-0c0s6n2 default busy flat quad 4:59:44 \n27       c0-0c0s6n3 default busy flat quad 4:59:44 \n28       c0-0c0s7n0 default idle flat quad 4:59:44 \n29       c0-0c0s7n1 default idle flat quad 4:59:44 \n30       c0-0c0s7n2 default idle flat quad 4:59:44 \n31       c0-0c0s7n3 default idle flat quad 4:59:44 \n32       c0-0c0s8n0 default idle flat quad 4:59:44 \n33       c0-0c0s8n1 default idle flat quad 4:59:44 \n34       c0-0c0s8n2 default idle flat quad 4:59:44 \n[...]\n</code></pre></p>"},{"location":"theta/queueing-and-running-jobs/machine-reservations/","title":"Machine Reservations on Theta","text":"<p>To get a reservation, you must first demonstrate a need to run outside of the normal queueing policies. Reservations are available only to projects with a positive allocation. Lead time for approval is 5 business days. If approved, scheduling is contingent on machine availability.</p> <p>Disclaimer: Approval for reservation requests are subject to their appropriateness and machine availability. Not all requests will be approved. It is particularly difficult to accommodate reservation requests during busy times of the year, e.g. Supercomputing, end of the ALCC and INCITE allocation cycles.</p> <p>To request a reservation, e-mail support@alcf.anl.gov with the requested information below.</p> <ol> <li>RESERVATION REQUEST FOR ALL SYSTEMS (including vis clusters) AT ALCF Machine name:</li> <li>Project for reservation:</li> <li>ALCF account username(s) (NOT the user's legal name) for reservation:</li> <li>Length of reservation:</li> <li>Earliest date you could start:</li> <li>Deadline for the run(s),</li> <li>Details on the Run: Can it run anytime, day or night?</li> <li>Your local time zone (e.g., US/Central):</li> <li>Total number of jobs to be run:</li> <li>Total amount of data generated during reservation:</li> <li>For each job, indicate: Node count (Note: not processor count)<ol> <li>Run time whether this job depends on any other jobs to finish before it can start</li> <li>Briefly describe the goals for this run: (Example: We are doing a scaling run of code XXXX to determine YYY)</li> <li>Please provide a detailed explanation of why this workload cannot be accomplished with the existing queues: (Requests omitting this response will be not be processed)</li> <li>After a reservation is granted, you will receive a reservation name by e-mail. Use the command \u201cshowres\u201d to verify the reservation attributes.</li> </ol> </li> </ol> <p>For example:</p> <pre><code>showres\nReservation  Queue     User   Start                                 Duration  Passthrough  Partitions             \n==========================================================================================================\nsmith        R.smith   smith  Mon Aug  18 09:00:00 2013 -0500 (CDT)  24:00     Allowed      MIR-00000-73FF1-16384  \n\nqsub -q R.smith -t 60 -n 1024 myprog.exe\n</code></pre> <p>Once the reservation is set up, jobs can be submitted to the reservation queue prior to the reservation start time.</p> <p>For jobs using 33 percent or more of a system, place your job in the queue at least 12 hours prior to the start of the reservation or your reservation may be canceled. The machine will start to drain for your reservation, and it is important that your job is ready to run.</p> <p>You can also move jobs from the regular queue to the reservation queue at any time using the \u201cqmove\u201d command. Keep in mind that a job won't start unless enough time is left in the reservation. There is a 10-minute pad at the end. (That is, if you have 60 minutes left, do not try to run more than 50.)</p> <p>If you have finished running your jobs before your reservation has ended, use the command \u201cuserres\u201d to release it for other users. The argument to userres is the reservation name shown in the first column of the showres output (in the example above, smith, not R.smith). <pre><code>userres reservation_name\n</code></pre></p>"},{"location":"theta-gpu/getting-started/","title":"Getting Started on ThetaGPU","text":""},{"location":"theta-gpu/getting-started/#references","title":"References","text":"<p>In addition to the content below, here is a getting started video covering the basics of using ThetaGPU and a  related video on Lustre File Striping Basics. This should help you get up and running quickly on the GPU nodes.</p> <ul> <li>Video on Getting Started on ThetaGPU </li> <li>Lustre File Striping Basics</li> </ul>"},{"location":"theta-gpu/getting-started/#login-to-thetagpu","title":"Login to ThetaGPU","text":"<p><pre><code>ssh -A username@theta.alcf.anl.gov\n</code></pre> Replace the username with your ALCF username. You will prompted to type in your MFA password. Note: In order to log in to ALCF systems, you need to have an active ALCF account.</p>"},{"location":"theta-gpu/getting-started/#setup-thetagpu-environment","title":"Setup ThetaGPU environment","text":"<p>Once logged in, you land on theta login nodes (thetalogin1 - thetalogin6). </p> <p>You can set an environment variable to control which instance the default commands (qsub, qstat, etc) will interact with. The primary use case here will be users who only use GPU nodes, but are working from the Theta login nodes.  To do so, you may do: <pre><code>module load cobalt/cobalt-gpu\n</code></pre> To switch back you may do <code>module load cobalt/cobalt-knl</code> which would make cobalt commands interact with the original Cobalt instance and launch jobs on the KNL nodes.</p> <p>Alternatively, If you are on a GPU node, for instance, the service nodes (thetagpusn1-2), then commands will default to the GPU instance. To head to a service node from the theta login nodes use: <pre><code>ssh thetagpusnX\n</code></pre> you can also set <code>COBALT_CONFIG_FILES=&lt;path to cobalt config&gt;</code> </p> <ul> <li>knl config: /etc/cobalt.knl.conf</li> <li>gpu config: /etc/cobalt.gpu.conf</li> </ul> <p>You can use suffixed commands to explicitly control which instance you are interacting with. If you regularly use both types of nodes, this is the recommended path to avoid confusion and to prevent launching jobs on the wrong architecture.</p> <p>All the commands you are used to are there, they take the same command line parameters, etc., they just have either -knl or a -gpu suffix on them. For instance:</p> <ul> <li>qsub-knl  would submit a job to the KNL nodes <li>qstat-gpu would check the queue status for the GPU nodes</li> <p>For all the build and development please use ThetaGPU compute nodes. Please avoid using the service nodes thetagpusn[1,2] as they have not been set up for development. </p> <p>Using \"qstat -Q\" to see all available queues. You can submit your job to a specific queue (as long as you are part of that queue) using \"qsub -q queue_name\".</p> <ul> <li>For more information on all ThetaGPU queues visit: Queue Policy on ThetaGPU</li> <li>For more information on submitting a job visit: Submit a job on ThetaGPU</li> </ul>"},{"location":"theta-gpu/getting-started/#project-space-and-home","title":"Project Space and Home:","text":"<p>Every user has a home directory located at /home/username.</p> <p>The project folder is located at: <pre><code>/grand/project_name or /eagle/project_name\n/lus/grand/projects/project_name or /lus/eagle/projects/project_name\n</code></pre> Please use the project folder when building and running on ThetaGPUcompute nodes</p> <p>/grand is on an HDR network directly connected to ThetaGPU</p> <p>/home is on an FDR network which is up-linked to the HDR network via a straw, heavy use of this file system will result in Bad Things\u2122 again.</p> <p>For more information on all available file systems visit: File Systems </p>"},{"location":"theta-gpu/getting-started/#software","title":"Software","text":"<p>ThetaGPU is new, so it has limited ALCF provided software. ThetaGPU compute nodes are setup with CUDA11</p> <p>Default Nvidia installed software will just be in your PATH</p> <p><code>which nvcc</code></p> <p>To see the available software via modules</p> <p><code>module avail</code></p> <p>Other ThetaGPU software can be found in</p> <p><code>/soft/thetagpu</code></p> <p>Theta software can be found in  <code>/soft</code>   \u2013 Anything that is not compute specific will be useable on the AMD host CPUs   \u2013 cmake is good example of something that can be used</p> <p>For more information on compiling and linking on ThetaGPU visit: Compiling and Linking on ThetaGPU</p>"},{"location":"theta-gpu/getting-started/#nvidia-hpc-sdk","title":"NVIDIA HPC SDK","text":"<p><code>module use /soft/thetagpu/hpc-sdk/modulefiles</code></p> <p>\u2013 Adds more modules for Nvidia SDK</p> <p><code>module avail</code></p> <p>\u2013 Shows you the new modules you have available \u2013 20.9 version will be loaded by default \u2013 21.2 version available using CUDA11 driver \u2013 21.3 version available using CUDA11 driver</p> <p><code>nvhpc</code></p> <p>\u2013 Loads the SDK and sets various compiler environment variables so that build tools will likely pick up the compilers by default \u2013 MPI wrappers disabled</p> <p><code>nvhpc-byo-compiler</code></p> <p>\u2013 Identical to nvhpc but doesn\u2019t set compiler environment variables</p> <p><code>nvhpc-nompi</code></p> <p>\u2013 Excludes MPI libraries</p>"},{"location":"theta-gpu/getting-started/#proxy","title":"Proxy","text":"<p>If the node you are on doesn\u2019t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host <pre><code># proxy settings\nexport HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128\nexport http_proxy=http://theta-proxy.tmi.alcf.anl.gov:3128\nexport https_proxy=http://theta-proxy.tmi.alcf.anl.gov:3128\n</code></pre></p>"},{"location":"theta-gpu/getting-started/#io","title":"I/O","text":"<p>/grand is a Lustre file system. Default stripe size is 1MiB and stripe count is 1. If you have a large file to read or write with high performance (in parallel) \u2013 Set the stripe count higher than 1 \u2013 Use a specific directory for these files <pre><code>mkdir big_files\nlfs setstripe -c 8 big_files\n/raid/scratch for local disk in RAID-0 config\n</code></pre></p>"},{"location":"theta-gpu/getting-started/#mpi","title":"MPI","text":"<p>ALCF provides a few MPI package built specifically for ThetaGPU \u2013 UCX is enabled</p> <p><code>module load openmpi</code></p> <p>\u2013 Default module is openmpi/openmpi-4.1.0</p> <p><code>module av openmpi</code></p> <p>List of possible openmpi modules</p>"},{"location":"theta-gpu/applications-and-libraries/applications/gromacs/","title":"Gromacs on ThetaGPU","text":""},{"location":"theta-gpu/applications-and-libraries/applications/gromacs/#what-is-gromacs","title":"What is Gromacs?","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p>"},{"location":"theta-gpu/applications-and-libraries/applications/gromacs/#using-gromacs-at-alcf","title":"Using GROMACS at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for GROMACS. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta-gpu/applications-and-libraries/applications/gromacs/#building-gromacs","title":"Building Gromacs","text":"<ol> <li>Download latest source code: http://manual.gromacs.org/documentation/2022.1/download.html</li> <li><code>tar -xzf gromacs-2022.1.tar.gz</code></li> <li>Submit an interactive job to a ThetaGPU compute node from Theta login node: <pre><code>user@thetalogin4:~&gt;module load cobalt/cobalt-gpu\nuser@thetalogin4:~&gt;qsub -I -n 1 -t 60 -q single-gpu -A PROJECT --attrs filesystems=home\nJob routed to queue \"single-gpu\".\nWait for job 10108666 to start...\nOpening interactive session to thetagpu06-gpu0\n...\nuser@thetagpu06:~$ \n</code></pre></li> <li><code>cd gromacs-2022.1</code></li> <li><code>mkdir build</code></li> <li><code>module load cmake</code></li> <li><pre><code>cmake -DCMAKE_C_COMPILER=mpicc -DCMAKE_CXX_COMPILER=mpicxx \\\n      -DBUILD_SHARED_LIBS=OFF -DGMX_BUILD_OWN_FFTW=ON \\\n      -DCMAKE_INSTALL_PREFIX=/path-to/gromacs-2022.1/build \\\n      -DGMX_MPI=ON -DGMX_OPENMP=ON -DGMX_GPU=CUDA \\\n      -DCUDA_TOOLKIT_ROOT_DIR=/user/local/cuda-11.4\n</code></pre></li> <li><code>make \u2013j 16</code></li> <li><code>make install</code></li> <li>The installed binary is <code>build/bin/gmx_mpi</code>.</li> </ol>"},{"location":"theta-gpu/applications-and-libraries/applications/gromacs/#running-gromacs-on-thetagpu","title":"Running Gromacs on ThetaGPU","text":"<p>Prebuilt Gromacs binaries can be found in the directory <code>/soft/applications/gromacs/gromacs_cuda</code>.</p> <p>A sample qsub script follows that will run GROMACS on a full node using all eight GPUs available.</p> <pre><code>#!/bin/bash -l\n#COBALT -n 1\n#COBALT -t 30 \n#COBALT -q full-node \n#COBALT -project catalyst \n#COBALT --attrs filesystems=home,theta-fs0\n\nNODES=`cat $COBALT_NODEFILE | wc -l`\n\nmpirun -hostfile $COBALT_NODEFILE --np 8 \\\n      /soft/applications/gromacs/gromacs_cuda/gmx_mpi.2022.1 \\\n      mdrun -ntomp 8 -gputasks 01234567 -nb gpu -pme gpu -npme 1 \\\n      -dlb yes -resethway -pin on -v deffnm step5_1 -g test.log\n</code></pre> <p>We strongly suggest that users try combinations of different numbers of nodes, MPI ranks per node, number of GPU tasks/devices, GPU task decomposition between nonbonded and PME kernels, and OMP threads per rank to find the optimal throughput for their particular workload.</p> <p>The following is a representative benchmark for a system with 30,000 atoms generated on a single ThetaGPU node with above example.</p> Core time(sec) Wall time(sec) (%) Time 691.769 10.810 6399.6 ns/day hour/ns Performance 399.661 0.060"},{"location":"theta-gpu/compiling-and-linking/compiling-and-linking-overview/","title":"Compiling and Linking on ThetaGPU","text":""},{"location":"theta-gpu/compiling-and-linking/compiling-and-linking-overview/#overview","title":"Overview","text":"<p>ThetaGPU has AMD processors on the service nodes (thetagpusn1,2) and AMD processors and NVIDIA A100 GPUs on the compute nodes [see overview page]. The service nodes can be used to create containers and launch jobs.</p> <p>Note: Until the cross-compiling environment is set up or dedicated build nodes get added, the compute nodes will have to be used for compiling. Do not compile codes on service nodes (thetagpusn1,2).</p> <p>The default programming environment on the ThetaGPU compute nodes is the GNU compiler tools coupled with NVIDIA\u2019s CUDA toolkit. </p> <p>Note: Symlinks to the project directories are not available on the compute nodes. Use the full path (eg: /lus/theta-fs0/projects/) to access the project directory. <p>For non-GPU codes:   - gcc \u2013 for C compiler   - g++ \u2013 for C++   - gfortran \u2013 for Fortran</p> <p>For CUDA codes, please note that there is a new driver(v470) and default cuda toolkit(v11.4); the old toolkit is still available on the compute nodes at /usrlocal/cuda-11.3   - nvcc</p> <p>For MPI, the latest MPI is in /lus/theta-fs0/software/thetagpu/openmpi-4.0.5.   - mpicc   - mpicxx   - mpif77/mpif90 not configured yet</p> <p>mpirun is a wrapper in /usr/local/bin that sets the appropriate options and uses the mpirun in the MPI directory above.</p> <p>On the service nodes, GNU compilers are available.</p>"},{"location":"theta-gpu/compiling-and-linking/compiling-and-linking-overview/#modules-on-thetagpu","title":"Modules on ThetaGPU","text":"<p>Available modules can be listed (on thetagpusn1,2) via the command: <pre><code>user@thetagpusn1:~$ module avail\n\n----------------------- /usr/local/lmod/lmod/modulefiles -----------------------\n\n   Core/lmod    Core/settarg\n\n-------- /lus/theta-fs0/software/environment/thetagpu/lmod/modulefiles ---------\n\n   Core/StdEnv                 (L,D)    conda/tensorflow/2021-01-08\n\n   aocl/blis-3.0                        conda/tensorflow/2021-03-02      (D)\n\n   conda/pytorch/2020-11-25             nccl/nccl-v2.8.4-1_CUDA11\n\n   conda/pytorch/2021-03-02    (D)      openmpi/openmpi-4.0.5            (L)\n\n   conda/tensorflow/2020-11-11          openmpi/openmpi-4.1.0_ucx-1.10.0\n\n   conda/tensorflow/2020-12-17          openmpi/openmpi-4.1.0            (D)\n\n   conda/tensorflow/2020-12-23\n\n-- /lus/theta-fs0/software/spack/share/spack/modules/linux-ubuntu18.04-x86_64 --\n\n   autoconf-2.69-gcc-7.5.0-wmttzuv\n\n   autoconf-archive-2019.01.06-gcc-7.5.0-bdyarrk\n\n....\n</code></pre> Loaded modules in your environment can be listed (on thetagpusn1,2) via the command:</p> <p><pre><code>user@thetagpusn1:~$ module list\n\nCurrently Loaded Modules:\n\n1) openmpi/openmpi-4.0.5   2) Core/StdEnv\n</code></pre> To load new modules use: <pre><code>user@thetagpusn1:~$ module load &lt;module_name&gt;\n</code></pre> There are few modules available at this time, but the number will grow as more packages become available.</p> <p>Usage: csh and zsh users do not have to do anything special to their environments. bash users, however, will need to add the following to any job scripts: <pre><code>#!/bin/bash\n. /etc/profile\n</code></pre> bash users are also encouraged to modify their ~/.bashrc to ensure the ubuntu system /etc/bash.bashrc file is sourced properly: <pre><code># Source global definitions\nif [ -f /etc/bashrc ]\nthen\n    . /etc/bashrc\nelif [ -f /etc/bash.bashrc ]\nthen\n    . /etc/bash.bashrc\nfi\n</code></pre></p>"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/","title":"Continuous Integration on ThetaGPU","text":""},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#overview","title":"Overview","text":"<p>Continuous Integration (CI) in software development is the practice of committing code changes regularly to a version control system and having automated processes perform build, test, package, and deploy activities. The key concepts of CI include high frequency, repeatability, and automation in order to realize increased quality and ease of delivery. The main goal CI aims to achieve is the elimination of build and deployment issues, which in turn improves development cycles, provides a timely feedback loop with developers, and results in higher quality deliverables with reduced development time.</p> <p>CI usually describes the work that is done by a deployment or operations team to build and deploy code throughout an environment and make it available to the different interested teams involved in the SDLC. The steps that make up this process are referred to as a workflow or pipeline, which, when combined with automation, provides the mechanism for Continuous Integration.</p> <p>Today it is a common practice to use a CI tool for defining pipelines and executing the tasks required to take code from a source stored in a version control system to compiled and packaged artifacts executing in production. Two excellent examples of CI tools are Jenkins and GitLab.</p>"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#ci-tools-at-alcf","title":"CI Tools at ALCF","text":"<p>The ALCF provides a tool for implementing CI processes named Jenkins. Using the Jenkins tool, ALCF projects can make use of CI functionality. The Jenkins CI tool enables projects to auto-compile their custom software code, automate testing cycles, provide a feedback loop, and submit jobs to HPC resources. The custom pipelines needed for each project can be defined in Jenkins by project users, and execution can be controlled through triggers.</p>"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#jenkins","title":"Jenkins","text":"<p>Jenkins \"is a self-contained, open-source automation server which can be used to automate all sorts of tasks relating to building, testing, and delivering or deploying software.\" Jenkins is the tool that provides CI/CD functionality for ALCF resources. Most importantly, it provides the mechanisms required for DevOps automation.</p> <p>Additional information, technical and user documentation, and community support can be found on the Jenkin's project website.</p>"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#alcf-jenkins","title":"ALCF Jenkins","text":"<p>Log in to the ALCF Jenkins web portal using your ALCF credentials (ALCF username and cryptocard token password).</p>"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#projects-using-ci","title":"Projects Using CI","text":"<p>Enabling a project to use CI requires some additional steps and configuration to get started. Once enabled for a project, users can access the Jenkins CI environment and configure CI jobs or pipelines for building and testing their project code.</p>"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#on-boarding-with-ci","title":"On-Boarding with CI","text":"<p>To enable CI for your project, send an email to the ALCF Service Desk requesting CI functionality for your project and include the ALCF project shortname and the PI\u2019s name with the request.</p> <p>The project\u2019s PI will get an email with details and a new CI account associated with the project. This is a service account that the Jenkins CI tool will use when executing tasks associated with your project. The CI account will be listed as a project member and added to the project\u2019s group for access controls.</p>"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#folders","title":"Folders","text":"<p>Each CI project will have a top-level \u2018folder\u2019 created with the project\u2019s name. Please do not delete the project folder: it is used for organization in the multi-project environment and is required for implementing the needed level of security. The project folder is where all of the project objects are stored, you can additionally create any subfolders, jobs, pipelines, etc. within your project folder to meet your CI needs.</p> <p>In the example below, we have a project named \u2018TestFromJanet2\u2019 with an associated folder.</p> <p> </p> CI folders screenshot"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#nodes","title":"Nodes","text":"<p>Each CI project will have an assigned node for execution. Nodes execute jobs defined within a project, typically on the target system\u2019s login node. Currently there are CI nodes configured for HPC systems Theta and Cooley, as well as non-HPC nodes with 32 cores (Intel Xeon Processor E5-2683 v4) and 128 GB RAM for generic x86 processing with access to the Mira shared filesystems.</p> <p>In the example below, the node for this project is named \u2018TestFromJanet2-Theta. Jobs and pipeline steps triggered from Jenkins will execute on the TestFromJanet2-Theta node which has been configured to use host: thetalogin1 and will use the project\u2019s CI user ID (provided during on-boarding) to execute scripts or code just as if the end user had logged into the thetalogin1 node and executed the same set of actions manually from the command line.</p> <p> </p> CI folders screenshot"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#job-configuration","title":"Job Configuration","text":"<p>When configuring any new job within a project there are some guidelines to follow for setting permissions and nodes. Project data is kept secure by setting up permissions at the project level and node selection controls where the job will execute.</p> <p>When creating jobs, enable project-based security, set the inheritance strategy, and add your project\u2019s group name to the permission matrix table. The example below has enabled project-based security, set the inheritance strategy to Do not inherit permission grants from other ACLs, and added the project\u2019s group name \u2018\u2018TestFromJanet2\u2019 to the permission matrix granting all rights to the group.</p> <p> </p> CI folders screenshot <p>To assign the node that the project will use to execute jobs, select the option Restrict where this project can be run and enter the project\u2019s assigned node. The example below has assigned the jobs to node: TestFromJanet2-Theta so that any time the job is executed, it runs on host: thetalogin1.</p> <p> </p> Execute"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#common-jenkins-features","title":"Common Jenkins Features","text":""},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#version-control-features","title":"Version Control Features","text":"<p>Jenkins can connect to most common version control systems (VCS), including git/svn. The ALCF Jenkins instance can connect with local VCS hosted at at ANL as well as with external VCS, such as that hosted at Github.</p> <p>On the job configuration page, look for the section Source Code Management (SCM). If it is there already, add it to the job. The required fields for SCM are Repository URL and Credentials. The example below shows a connection to the ALCF internal Gitlab VCS and uses previously setup credentials.</p> <p> </p> Repository access <p>To use the new connection to the Git repository interactively, configure the job to be parameterized and add a a Git Parameter to the job. The example below shows the configuration to select a branch at build time.</p> <p> </p> Git Parameter <p>On the build screen, select from the drop-down menu the branch to be referenced during this job execution. The example below shows the list of available branches from the configured repository. It is automatically populated duing the Git connector configuration of the preceding steps. If a new branch is added to the Git repository, it will display in the populated list of avialable branches when the job runs in Jenkins.</p> <p> </p> Select branch"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#build-steps","title":"Build Steps","text":"<p>Build steps are where users define executable tasks and jobs do something interesting within an envionment. A core component of Jenkens, build steps can take a few different forms and are morst commonly configured to call remote scripts for code building and deployment. A build step can even contain the shell script contents to execute on the remote machine.</p> <p> </p> Select branch <p>The example below uses the Execute Shell build step type and codes the shell logic within the Jenkins portal.</p> <p> </p> Execute shell"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#pipelines","title":"Pipelines","text":"<p>Pipelines in Jenkins allow for more advanced execution logic and are written in Groovy. A pipeline can be added directly to your project as an object using the New Item link. More commonly, they are defined in a \"Jenkinsfile\" and stored in VCS along with the project code. The Jenkinsfile can be created and edited outside of the Jenkins system using any text editor.</p> <p>To add a pipeline manually, select Pipeline from the new New Item dialog box.</p> <p> </p> New Item dialog box <p>The pipeline can then be configured and edited from the project folder in the same way as jobs, as shown in the example below.</p> <p> </p> Pipeline configuration <p>To add a pipeline using a Jenkinsfile in SCM, add the pipeline object as shown below. On the pipeline configuration page, select Pipeline script from SCM and provide the SCM connection details along with the Script Path. The Script Path is the path-to and filename where the Jenkinsfile is located within the SCM repository. The example below uses a Jenkinsfile stored in the project source code from the ALCF Git repository, and the Jenkinsfile containing the Groovy code pipeline definition is located at scripts/Jenkinsfile from the repository root.</p> <p> </p> Pipeline script path"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#triggers","title":"Triggers","text":"<p>Triggers are events that intitiate tasks in Jenkins. Triggers can be called a few different ways, including directly by a user via the Build Now action (a time-based trigger similar to a Cron system), or based on commits made to source control.</p> <p>The example below shows a time-based configuration to run the job on a regular schedule. Details on the scheduling syntax can be found by clicking the blue question mark to the right of the Schedule field.</p> <p> </p> Build Triggers"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#console-output","title":"Console Output","text":"<p>Jenkins provides console output and saves this history for each job run. During job execution you can view the live output from the tasks in a display similar to what would be seen if the commands were run directly in an interactive console.</p> <p> </p> Console output"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#credentials","title":"Credentials","text":"<p>Credentials are stored in Jenkins and used when connecting to remote resources that require authentication in a non-interactive manner. Once defined, credentials can be used throughout the Jenkins system when configuring jobs, SCM connections, SSH connections, etc.</p> <p>To add a set of credentials, click on Credentials from the available options on the left-hand navigation menu. Then select System and click on the link for Global credentials.</p> <p> </p> Credentials <p>Click Add Credentials from the left-hand navigation menu and provide the required information. The example below configures a new credential set of type \"SSH Username with private key.\" Make sure Scope is set to \"Global.\" Provide the username, private key (copy and paste), and key passphrase, and then give a pertinent ID and detailed description to help identify and organize stored credentials in the system.</p> <p> </p> Add credentials"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#faqs","title":"FAQS","text":"<p>Why does my project's execution node say it is offline? Node services for executing project tasks are inititated when there is demand for the node. The process of starting the node services acan take up to one minute; the status change is displayed in the Jenkins web portal. When there is no longer demand for the node, the services will stop again after one minute of idle time.</p> <p>Why is my shell environment different when executing tasks on a Jenkins node? Since Jenkins uses SSH with no tty, any shell scripts need to have this at the top so that login scripts are run against the session:</p> <p><code>#!/bin/bash -1</code></p>"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#glossary","title":"Glossary","text":"<p>Continuous Integration (CI) - The process of automating the build and testing of code every time developers commit changes to version control.</p> <p>Pipeline - A CI pipeline is a list of tasks or jobs that are defined and executed as a procedure within a project. Pipeline is analogous to workflow.</p> <p>Source Control Management (SCM) - A term used in Jenkins to describe objects related to version control.</p> <p>Version Control System (VCS) - Software that manages access, storage, and revision history for a code respository.</p>"},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#appendix","title":"Appendix","text":""},{"location":"theta-gpu/compiling-and-linking/continuous-integration/#abbreviated-setup","title":"Abbreviated Setup","text":"<ul> <li>Request CI capabilities for your project by emailing the ALCF Service Desk.</li> <li>Add jobs and pipelines to the project folder space to handle code compiling and testing.</li> <li>Configure jobs with credentials, SCM integrations, and trigger components depending on the intended behavior for your project.</li> <li>Execute jobs and pipelines by invoking the configured triggers.</li> </ul>"},{"location":"theta-gpu/data-science-workflows/building-python-packages/","title":"Building Python Packages","text":"<p>To build Python packages for ThetaGPU, there are two options: build on top of a bare-metal build or build on top of (and within) a singularity container. Additionally, you can build a new container from NVIDIA's docker images.</p>"},{"location":"theta-gpu/data-science-workflows/building-python-packages/#build-on-thetagpu-compute-using-conda","title":"Build on ThetaGPU compute using Conda","text":"<p>To build on ThetaGPU compute and install your own packages, login to theta and then submit an interactive job to log on to ThetaGPU compute node. </p> <p>Please see Running PyTorch with Conda or Running TensorFlow with Conda for more information.</p>"},{"location":"theta-gpu/data-science-workflows/building-python-packages/#building-on-top-of-a-container","title":"Building on top of a container","text":"<p>At the moment, you will need two shells to do this: have one open on a login node (for example, <code>thetaloginN</code>, and one open on a compute node (<code>thetagpuN</code>). First, start the container in interactive mode: <pre><code>singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash\n</code></pre> From here, you can create a virtual env for installation: <pre><code>export VENV_LOCATION=/path/to/virtualenv # replace this with your path! \npython -m venv --system-site-packages $VENV_LOCATION\n</code></pre> Note: sometimes, the venv package is available and if not, you can try <code>python -m virtualenv</code>. If neither are available, you can install it in your user directory: <pre><code>pip install --user virtualenv\n</code></pre> and it should work.</p> <p>Next time you log in, you'll have to start the container, and then run source <code>$VENV_LOCATION/bin/activate</code> to re-enable your installed packages.</p>"},{"location":"theta-gpu/data-science-workflows/building-python-packages/#reaching-the-outside-world-for-pip-packages","title":"Reaching the outside world for pip packages","text":"<p>You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: <pre><code>export HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128\nexport HTTPS_PROXY=https://theta-proxy.tmi.alcf.anl.gov:3128\n</code></pre> Now, you can pip install your favorite packages: <code>pip install mpi4py</code></p>"},{"location":"theta-gpu/data-science-workflows/building-python-packages/#building-custom-packages","title":"Building custom packages","text":"<p>Most packages (HDF5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful.</p>"},{"location":"theta-gpu/data-science-workflows/building-python-packages/#hdf5","title":"HDF5","text":"<p>You can find the source code for HDF5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code. When downloaded and un-tarred, cdto the directory and run: <pre><code>./configure --prefix=$VENV_LOCATION # Add any other configuration arguments \nmake -j 64 \nmake install\n</code></pre> This should get you HDF5! For example, after this: <pre><code>(pytorch_20.08) Singularity&gt; which h5cc \n/home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success!\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/building-python-packages/#horovod","title":"Horovod","text":"<p>Horovod is useful for distributed training. To use it, you need it enabled within the container. <pre><code>git clone https://github.com/horovod/horovod.git \ncd horovod \ngit submodule update --init \npython setup.py build \npython setup.py install\n</code></pre> This should install Horovod within your container.</p>"},{"location":"theta-gpu/data-science-workflows/data-science-software-availability/","title":"Data Science Software Availability","text":"<p>On ThetaGPU, currently we support the major deep learning frameworks through two paths: Singularity containers, based off of NVIDIA's Docker containers, and through bare-metal source builds. The bare-metal builds are for TensorFlow 2.X PyTorch. TensorFlow 1.X is supported only via NVIDIA's containers at this time.</p>"},{"location":"theta-gpu/data-science-workflows/data-science-software-availability/#containers","title":"Containers","text":"<p>As of now, the NVIDIA containers with TensorFlow 1, 2 and PyTorch built against <code>cuda11</code>, <code>cudnn8</code> are available in singularity format here: <pre><code>$ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ \npytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif\n</code></pre></p> <p>Execute a container interactively like this: <pre><code>$ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/gpu-monitoring/","title":"GPU Monitoring","text":"<p>Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command <code>nvidia-smi</code>.</p> <p>Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero.</p> <p>You can target a specific GPU with <code>nvidia-smi -i 0</code> for the first GPU, for example.</p>"},{"location":"theta-gpu/data-science-workflows/gpu-monitoring/#gpu-selection","title":"GPU Selection","text":"<p>In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with:</p> <pre><code># Specify to run only on GPU 4: \nexport CUDA_VISIBLE_DEVICES=4 \n\n# Let your application see GPUS 0, 1, and 7: \nexport CUDA_VISIBLE_DEVICES=\"0,1,7\"\n</code></pre> <p>In these cases, the GPU orderings will appear as a consecutive list starting with 0.</p> <p>From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch:</p> <ul> <li>TensorFlow</li> <li>PyTorch</li> </ul>"},{"location":"theta-gpu/data-science-workflows/gpu-node-queue-and-policy/","title":"GPU Node Queue and Policy","text":"<p>Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation Request.</p> <p>ThetaGPU is listed under Theta on the form.</p> <p>The GPU nodes are new and we expect the workload to be significantly different than it is on the KNL nodes. This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary.</p>"},{"location":"theta-gpu/data-science-workflows/gpu-node-queue-and-policy/#nodes-vs-queue-vs-mig-mode","title":"Nodes vs Queue vs MIG mode","text":"<p>The GPU nodes are NVidia DGX A100 nodes and each node contains eight (8) A100 GPUs.</p> <p>You may request either entire nodes, or individual GPUs based on your job needs. What you will get is determined by the queue you submit to: - If it has node in the name, you will get nodes. - If it has GPU in the name, you will get GPUs.</p> <p>Note: The <code>-n</code> parameter in your <code>qsub</code> will match the resource type in the queue (<code>-n 2</code> in node queue will get you two full nodes, <code>-n 2</code> in a GPU queue will get you two GPUs). </p> <p>Additionally, the Nvidia A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode. This allows a single GPU to be shared by up to 7 different processes. We do not schedule at this level, but you may pass <code>\u2013attrs mig-mode=True</code> in with your qsub and we will set the node to MIG mode and you may take advantage of it in your job script.</p>"},{"location":"theta-gpu/data-science-workflows/gpu-node-queue-and-policy/#queues","title":"Queues","text":"<p>There will be two primary queues: - full-node: This is the general production queue for jobs that require full nodes. - single-gpu: This is the general production queue for jobs that operate best on individual GPUs.</p> <p>And two debug queues: - debug-node: Submit to this queue if you need an entire node for your testing (for instance you are utilizing the NVLink) - debug-gpu: Submit to this queue if you need GPUs.</p> <p>Initially, we are relaxing our node restrictions to encourage early users. Please be courteous to your fellow users and do not monopolize the machine. We will tighten restrictions as required to manage the demonstrated workload. </p> <p>Here are the initial queue limits: - MinTime is 5 minutes - MaxTime is 12 hours - MaxRunning will be 2 full nodes or 16 individual GPUs</p>"},{"location":"theta-gpu/data-science-workflows/gpu-node-queue-and-policy/#queue-restrictions","title":"Queue Restrictions","text":"<ul> <li>MaxQueued will be 100 jobs</li> <li>You may have at most 1152 node-hours or 9216 GPU hours in the queue at any time.</li> <li>You may not violate either of these policies.</li> <li>You could not submit (1000) 1 node-hour jobs because that would violate the MaxQueued of 100 jobs, nor could you submit (2) 1000 node-hour jobs because that would violate the MaxNodeHours limit.</li> <li>The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.</li> </ul>"},{"location":"theta-gpu/data-science-workflows/pythonc-code-interoperability/","title":"Python/C++ Code Interoperability","text":"<p>These are the steps to build code that has Python/C++ code interoperability. </p>"},{"location":"theta-gpu/data-science-workflows/pythonc-code-interoperability/#login-to-a-thetagpu-head-node","title":"Login to a ThetaGPU head node","text":"<pre><code>ssh thetagpusn1\n</code></pre>"},{"location":"theta-gpu/data-science-workflows/pythonc-code-interoperability/#1-request-an-interactive-session-on-an-a100-gpu","title":"1. Request an interactive session on an A100 GPU","text":"<pre><code>qsub -n 1 -q default -A datascience -I -t 1:00:00\n</code></pre> <p>Following this, we need to execute a few commands to get setup with an appropriately optimized TensorFlow. These are: 3. Activate the TensorFlow 2.2 Singularity container: <pre><code>singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/pythonc-code-interoperability/#2-setup-access-to-the-internet","title":"2. Setup access to the internet","text":"<pre><code>export HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128 \nexport HTTPS_PROXY=https://theta-proxy.tmi.alcf.anl.gov:3128\n</code></pre> <p>Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time). <pre><code>python -m pip install --user virtualenv \nexport VENV_LOCATION=/home/rmaulik/THETAGPU_TF_ENV # Add your path here \npython -m virtualenv --system-site-packages $VENV_LOCATION \nsource $VENV_LOCATION/bin/activate \npython -m pip install cmake \npython -m pip install matplotlib \npython -m pip install sklearn\n</code></pre></p> <p><code>cmake</code> is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example <code>MakeLists.txt</code> file for building with Python/C interoperability with examples can be found here.</p>"},{"location":"theta-gpu/data-science-workflows/containers/containers/","title":"Containers on Theta(GPU)","text":"<p>On Theta(GPU), container creation can be achieved by using Docker on your local machine as mentioned in Example <code>Dockerfile</code>, or using a Singularity recipe file and building on a Theta(GPU) worker node.</p>"},{"location":"theta-gpu/data-science-workflows/containers/containers/#building-using-docker","title":"Building using Docker","text":"<p>If you followed the <code>Dockerfile</code> instructions, using the Theta(GPU) specific <code>Dockerfile_thetagpu</code> you can build your container for theta gpu using: <pre><code>singularity build &lt;image_name&gt; docker://&lt;username&gt;/&lt;repo_name&gt;:&lt;tag&gt;\n# using tutorial example\nsingularity build my_image.simg docker://jtchilders/alcf_cwp_example:thetagpu\n</code></pre></p> <p></p> <p>Then you can submit a job to Theta(GPU) using the job submission script</p> <pre><code>module load cobalt/cobalt-gpu\nqsub -A &lt;project-name&gt; job_submission_thetagpu.sh ./my_image.simg\n</code></pre> <p>The output should look like this: <pre><code>C++ MPI\nHello world from processor thetagpu12, rank 4 out of 16 processors\nHello world from processor thetagpu12, rank 7 out of 16 processors\nHello world from processor thetagpu12, rank 1 out of 16 processors\nHello world from processor thetagpu12, rank 5 out of 16 processors\nHello world from processor thetagpu12, rank 6 out of 16 processors\nHello world from processor thetagpu12, rank 0 out of 16 processors\nHello world from processor thetagpu12, rank 2 out of 16 processors\nHello world from processor thetagpu12, rank 3 out of 16 processors\nHello world from processor thetagpu18, rank 14 out of 16 processors\nHello world from processor thetagpu18, rank 15 out of 16 processors\nHello world from processor thetagpu18, rank 13 out of 16 processors\nHello world from processor thetagpu18, rank 8 out of 16 processors\nHello world from processor thetagpu18, rank 9 out of 16 processors\nHello world from processor thetagpu18, rank 11 out of 16 processors\nHello world from processor thetagpu18, rank 12 out of 16 processors\nHello world from processor thetagpu18, rank 10 out of 16 processors\nPython MPI\nHello world from processor thetagpu18, rank 13 out of 16 processors\nHello world from processor thetagpu18, rank 8 out of 16 processors\nHello world from processor thetagpu18, rank 9 out of 16 processors\nHello world from processor thetagpu18, rank 14 out of 16 processors\nHello world from processor thetagpu18, rank 15 out of 16 processors\nHello world from processor thetagpu18, rank 11 out of 16 processors\nHello world from processor thetagpu18, rank 10 out of 16 processors\nHello world from processor thetagpu18, rank 12 out of 16 processors\nHello world from processor thetagpu12, rank 2 out of 16 processors\nHello world from processor thetagpu12, rank 5 out of 16 processors\nHello world from processor thetagpu12, rank 0 out of 16 processors\nHello world from processor thetagpu12, rank 6 out of 16 processors\nHello world from processor thetagpu12, rank 4 out of 16 processors\nHello world from processor thetagpu12, rank 1 out of 16 processors\nHello world from processor thetagpu12, rank 7 out of 16 processors\nHello world from processor thetagpu12, rank 3 out of 16 processors\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/containers/containers/#building-using-singularity-recipes","title":"Building using Singularity Recipes","text":"<p>While building using Docker on your local machine tends to be the easier method. There are sometimes reasons to build in the environment of the supercomputer. In this case, one can build a singularity container on ThetaGPU in an interactive session on a compute (or worker) node. First a recipe file is needed, below is an example singularity definition file which can also be found here. </p> <p>Detailed directions for recipe construction are available on the Singularity Recipe Page.</p>"},{"location":"theta-gpu/data-science-workflows/containers/containers/#example-singularity-definition-file","title":"Example Singularity definition file","text":"<p>Here we have defined the base image from which to <code>bootstrap</code> our container. We are using an image from Docker Hub, <code>ubuntu:20.04</code>.</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n</code></pre> <p>The <code>%files</code> section lists files to copy from the host system (left path) to the container filesystem (right path)prior to build time.</p> <pre><code>%files\n../Local/source/* /usr/source/\n    ../Local/submit.sh /usr/\n</code></pre> <p>The <code>%environment</code> section defines environment variables that will be available to the container at runtime.</p> <pre><code>%environment\nexport PATH=$PATH:/mpich/install/bin\n    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/mpich/install/lib\n</code></pre> <p>The <code>%post</code> section executes within the container at build time on top of our <code>ubuntu:20.04</code> operating system. The <code>%post</code> section is therefore the place to perform installations of custom apps with syntax similar to BASH.</p> <pre><code>%post\n#### INSTALL BASE PACKAGES NEEDED FOR MPI APPLICATIONS AND PYTHON3 ####\nDEBIAN_FRONTEND=noninteractive\n    apt-get update -y \\\n&amp;&amp; DEBIAN_FRONTEND=noninteractive \\\n&amp;&amp; apt-get install -y build-essential libfabric-dev libibverbs-dev gfortran wget \\\n&amp;&amp; apt-get install -y python3 python3-distutils python3-pip gcc\n\n#### DOWNLOAD AND INSTALL MPICH AND MPI4PY ####\n# Source is available at http://www.mpich.org/static/downloads/\n# See installation guide of target MPICH version\n# Ex: https://www.mpich.org/static/downloads/4.0.2/mpich-4.0.2-installguide.pdf\n# These options are passed to the steps below\nOPENMPI_VERSION_A=\"4.0\"\nOPENMPI_VERSION_B=\"4.0.5\"\nOPENMPI_CONFIGURE_OPTIONS=\"--prefix=/openmpi/install --disable-wrapper-rpath --disable-wrapper-runpath\"\nOPENMPI_MAKE_OPTIONS=\"-j\"\nmkdir -p openmpi\n    cd /openmpi\n    wget https://download.open-mpi.org/release/open-mpi/v${OPENMPI_VERSION_A}/openmpi-${OPENMPI_VERSION_B}.tar.gz\n    tar xfz openmpi-${OPENMPI_VERSION_B}.tar.gz  --strip-components=1\n./configure ${OPENMPI_CONFIGURE_OPTIONS}\nmake install ${OPENMPI_MAKE_OPTIONS}\nexport PATH=$PATH:/openmpi/install/bin\n   export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/openmpi/install/lib\n\npip install mpi4py\n\n#### BUILD FILES ####\nchmod +x /usr/submit.sh\n    mpicc -o /usr/source/mpi_hello_world /usr/source/mpi_hello_world.c\n</code></pre> <p>The <code>%runscript</code> section defines actions for the container to take when it is executed using <code>singularity run &lt;container_name&gt;</code>.</p> <pre><code>%runscript\nexec /usr/submit.sh \"$@\"\n</code></pre> <p>The <code>%labels</code> section allows for custom metadata to be added to the container.</p> <pre><code>%labels\nMAINTAINER Aditya atanikanti@anl.gov\n</code></pre> <p>The <code>%help</code> section can be used to define how to build and run the container.</p> <pre><code>%help\nThis is container is used to illustrate a mpi based def file to build a container running python and c programs. To build the container use singularity build --fakeroot mpi.sif mpi.def\n</code></pre>"},{"location":"theta-gpu/data-science-workflows/containers/containers/#build-singularity-container-on-thetagpu-compute","title":"Build Singularity container on ThetaGPU compute","text":"<p>After logging on to Theta login nodes, launch an interactive job using the attrs <code>fakeroot=true</code>, <code>pubnet=true</code> and specifying the filesystems <code>filesystems=home,theta-fs0</code>.</p> <pre><code># on Theta login node, must load cobalt-gpu module to submit jobs to ThetaGPU\nmodule load cobalt/cobalt-gpu\nqsub -I -n 1 -t 01:00:00 -q single-gpu -A &lt;project_name&gt; --attrs fakeroot=true:pubnet=true:filesystems=home,theta-fs0\n</code></pre> <p>Before building the container make sure the ThetaGPU compute nodes have access to external resources, this is achieved by setting the <code>http_proxy</code> and <code>https_proxy</code> variables <pre><code># setup network proxy to reach outside world\nexport http_proxy=http://proxy.tmi.alcf.anl.gov:3128\nexport https_proxy=http://proxy.tmi.alcf.anl.gov:3128\n</code></pre></p> <p>Now build the container using <code>--fakeroot</code> where <code>&lt;def_filename&gt;.def</code> is the definition file we have defined in the example above and <code>&lt;image_name&gt;.sif</code> is the user defined image file name Using mpi.def example <pre><code># important you run this in the proper path because the file copies in\n# the `%files` section of the recipe uses relative paths on the host.\ncd singularity build --fakeroot &lt;image_name&gt;.sif &lt;def_filename&gt;.def </code></pre></p>"},{"location":"theta-gpu/data-science-workflows/containers/containers/#run-singularity-container-on-thetagpu-compute","title":"Run Singularity container on ThetaGPU compute","text":"<p>An example job submission script is here: job_submission_thetagpu.sh.</p> <p>First we define our job and our script takes the container name as an input parameter.</p> <pre><code>#!/bin/bash -l\n#COBALT -n 1\n#COBALT -t 00:10:00\n#COBALT -q single-gpu\n#COBALT --attrs filesystems=home,theta-fs0:pubnet=true\nCONTAINER=$1\n</code></pre> <p>Enable network access at run time by setting the proxy.</p> <pre><code>export http_proxy=http://proxy.tmi.alcf.anl.gov:3128\nexport https_proxy=http://proxy.tmi.alcf.anl.gov:3128\n</code></pre> <p>Setup our MPI settings, figure out number of nodes <code>NODES</code> and fix number of process per node <code>PPN</code> and multiply to get total MPI ranks <code>PROCS</code>.</p> <pre><code>NODES=`cat $COBALT_NODEFILE | wc -l`\nPPN=8 # GPUs per NODE\nPROCS=$((NODES * PPN))\necho NODES=$NODES  PPN=$PPN  PROCS=$PROCS\n</code></pre> <p>The OpenMPI installed on ThetaGPU must be used for MPI to properly run across nodes. Here the library path is added to <code>SINGULARITYENV_LD_LIBRARY_PATH</code>, which will be used by Singularity to set the container's <code>LD_LIBRARY_PATH</code> and therefore tell our executables where to find the MPI libraries.</p> <pre><code>MPI_BASE=/lus/theta-fs0/software/thetagpu/openmpi-4.0.5/\nexport LD_LIBRARY_PATH=$MPI_BASE/lib:$LD_LIBRARY_PATH\nexport SINGULARITYENV_LD_LIBRARY_PATH=$LD_LIBRARY_PATH\necho mpirun=$(which mpirun)\n</code></pre> <p>Finally the exectuable is launched. Notice on NVidia systems that the <code>singularity exec</code> or <code>singularity run</code> commands must use the <code>--nv</code> flag to pass important libraries/drivers from the host to the container environment.</p> <p><pre><code>mpirun -hostfile $COBALT_NODEFILE -n $PROCS -npernode $PPN singularity exec --nv -B $MPI_BASE $CONTAINER /usr/source/mpi_hello_world\n</code></pre> The job can be submitted using: <pre><code>qsub -A &lt;project-name&gt; job_submission_thetagpu.sh /path/to/my_image.sif\n</code></pre></p> <p>The output should look like this: <pre><code>C++ MPI\nHello world from processor thetagpu02, rank 12 out of 16 processors\nHello world from processor thetagpu02, rank 8 out of 16 processors\nHello world from processor thetagpu02, rank 10 out of 16 processors\nHello world from processor thetagpu02, rank 11 out of 16 processors\nHello world from processor thetagpu02, rank 13 out of 16 processors\nHello world from processor thetagpu02, rank 9 out of 16 processors\nHello world from processor thetagpu02, rank 14 out of 16 processors\nHello world from processor thetagpu02, rank 15 out of 16 processors\nHello world from processor thetagpu01, rank 0 out of 16 processors\nHello world from processor thetagpu01, rank 1 out of 16 processors\nHello world from processor thetagpu01, rank 2 out of 16 processors\nHello world from processor thetagpu01, rank 3 out of 16 processors\nHello world from processor thetagpu01, rank 4 out of 16 processors\nHello world from processor thetagpu01, rank 5 out of 16 processors\nHello world from processor thetagpu01, rank 6 out of 16 processors\nHello world from processor thetagpu01, rank 7 out of 16 processors\nPython MPI\nHello world from processor thetagpu02, rank 9 out of 16 processors\nHello world from processor thetagpu02, rank 10 out of 16 processors\nHello world from processor thetagpu02, rank 11 out of 16 processors\nHello world from processor thetagpu02, rank 15 out of 16 processors\nHello world from processor thetagpu02, rank 13 out of 16 processors\nHello world from processor thetagpu02, rank 8 out of 16 processors\nHello world from processor thetagpu02, rank 12 out of 16 processors\nHello world from processor thetagpu02, rank 14 out of 16 processors\nHello world from processor thetagpu01, rank 7 out of 16 processors\nHello world from processor thetagpu01, rank 3 out of 16 processors\nHello world from processor thetagpu01, rank 1 out of 16 processors\nHello world from processor thetagpu01, rank 4 out of 16 processors\nHello world from processor thetagpu01, rank 5 out of 16 processors\nHello world from processor thetagpu01, rank 6 out of 16 processors\nHello world from processor thetagpu01, rank 0 out of 16 processors\nHello world from processor thetagpu01, rank 2 out of 16 processors\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/containers/containers/#pre-existing-images-for-deep-learning","title":"Pre-existing Images for Deep Learning","text":"<p>There are several containers on ThetaGPU that will help you get started with deep learning experiments that can efficiently use the A100 GPUs. We have different optimized container for DL here <code>ls /lus/theta-fs0/software/thetagpu/nvidia-containers/</code></p> <p>The bootstap.def gives an example of how these containers were created.</p> <p>The image is bootstrapped from an NVidia image, in this case from a PyTorch build. One can also use the TensorFlow build. At the time of this writing, the latest tag for the PyTorch image was <code>22.04-py3</code>, but users should select the version that best suits their needs.</p> <p><pre><code>Bootstrap: docker\nFrom: nvcr.io/nvidia/pytorch:22.04-py3\n</code></pre> Next we need to install MPI support for cross-node parallel training.</p> <p><pre><code>%post\n# Install mpi4py\nCC=$(which mpicc) CXX=$(which mpicxx) pip install --no-cache-dir mpi4py\n\n# Install horovod\nCC=$(which mpicc) CXX=$(which mpicxx) HOROVOD_WITH_TORCH=1 pip install --no-cache-dir horovod\n</code></pre> Next build the container on a ThetaGPU compute node, following the instructions in the previous section. Then an example job submission script is here: job_submission_thetagpudl.sh.</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/deepspeed/","title":"DeepSpeed","text":"<p>The base <code>conda</code> environment on ThetaGPU comes with Microsoft's DeepSpeed pre-installed. Instructions for using / cloning the base environment can be found here.</p> <p>We describe below the steps needed to get started with DeepSpeed on ThetaGPU.</p> <p>We focus on the <code>cifar</code> example provided in the DeepSpeedExamples repository, though this approach should be generally applicable for running any model with DeepSpeed support.</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/deepspeed/#running-deepspeed-on-thetagpu","title":"Running DeepSpeed on ThetaGPU","text":"<p>Note</p> <p>The instructions below should be ran directly from a compute node. Explicitly, to request an interactive job (from <code>thetalogin</code>):</p> <pre><code>qsub-gpu -A &lt;project&gt; -n 2 -t 01:00 -q full-node \\\n--attrs=\"filesystems=home,grand,eagle,theta-fs0:ssds=required\" \\\n-I\n</code></pre> <p>Refer to GPU Node Queue and Policy.</p> <ol> <li> <p>Load <code>conda</code> module and activate base environment:     <pre><code>module load conda ; conda activate base\n</code></pre></p> </li> <li> <p>Clone    microsoft/DeepSpeedExamples    and navigate into the directory:     <pre><code>git clone https://github.com/microsoft/DeepSpeedExamples.git\ncd DeepSpeedExamples/cifar\n</code></pre></p> </li> <li> <p>Our newer conda environments should come with DeepSpeed pre-installed, but    in the event your environment has no <code>deepspeed</code>, it can be    installed<sup>2</sup> with <code>pip</code>:     <pre><code>$ which deepspeed\ndeepspeed not found\n$ python3 -m pip install --upgrade pip setuptools wheel\n$ DS_BUILD_OPS=1 python3 -m pip install </code></pre></p> </li> </ol> <p>Launching DeepSpeed</p> Launching with OpenMPILaunching with DeepSpeed <ol> <li> <p>Get total number of available GPUs:</p> <ol> <li>Count number of lines in <code>$COBALT_NODEFILE</code> (1 host per line)</li> <li>Count number of GPUs available on current host</li> <li><code>NGPUS = $((${NHOSTS}*${NGPU_PER_HOST}))</code> <pre><code>NHOSTS=$(wc -l &lt; \"${COBALT_NODEFILE}\")\nNGPU_PER_HOST=$(nvidia-smi -L | wc -l)\nNGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n</code></pre></li> </ol> </li> <li> <p>Launch with <code>mpirun</code><sup>1</sup>: <pre><code>mpirun \\\n-n \"${NGPUS}\" \\\n-npernode \"${NGPU_PER_HOST}\" \\\n--hostfile \"${COBALT_NODEFILE}\" \\\n-x PATH \\\n-x LD_LIBRARY_PATH \\\n-x PYTHONUSERBASE \\\n-x http_proxy \\\n-x https_proxy\n    python3 cifar10_deepspeed.py \\\n--deepspeed_config ds_config-1.json\n</code></pre></p> </li> </ol> <ol> <li> <p>Create a DeepSpeed compliant <code>hostfile</code>, specifying the hostname and    number of GPUs (<code>slots</code>) for each of our available workers: <pre><code>cat $COBALT_NODEFILE &gt; hostfile\nsed -e 's/$/ slots=4/' -i hostfile\n</code></pre></p> </li> <li> <p>Create a <code>.deepspeed_env</code> containing the environment variables our    workers will need access to: <pre><code>echo \"PATH=${PATH}\" &gt;&gt; .deepspeed_env\necho \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" &gt;&gt; .deepspeed_env\necho \"http_proxy=${http_proxy}\" &gt;&gt; .deepspeed_env\necho \"https_proxy=${https_proxy}\" &gt;&gt; .deepspeed_env\n</code></pre></p> </li> </ol> <p>Warning</p> <p>The <code>.deepspeed_env</code> file expects each line to be of the form <code>KEY=VALUE</code>. Each of these will then be set as environment variables on each available worker specified in our <code>hostfile</code>.</p> <p>We can then run the <code>cifar10_deepspeed.py</code> module using DeepSpeed: Launch with DeepSpeed<pre><code>deepspeed --hostfile=hostfile cifar10_deepspeed.py \\\n--deepspeed \\\n--deepspeed_config ds_config.json\n</code></pre></p> <code>AssertionError: Micro batch sizer per gpu: 0 has to be greater than 0</code> <p>Depending on the details of your specific job, it may be necessary to modify the provided <code>ds_config.json</code>.</p> <p>If you encounter an error: <pre><code>thetagpu23: AssertionError: Micro batch size per gpu: 0 has to be greater than 0\n</code></pre> you can modify the <code>\"train_batch_size\": 16</code> variable in the provided <code>ds_config.json</code> to the (total) number of available GPUs, and explicitly set <code>\"gradient_accumulation_steps\": 1</code>, as shown below. <pre><code>$ export NHOSTS=$(wc -l &lt; \"${COBALT_NODEFILE}\")\n$ export NGPU_PER_HOST=$(nvidia-smi -L | wc -l)\n$ export NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n$ echo $NHOSTS $NGPU_PER_HOST $NGPUS\n2 8 16\n$ # replace \"train_batch_size\" with $NGPUS in ds_config.json\n$ # and write to `ds_config-polaris.json`\n$ sed \\\n\"s/$(cat ds_config.json| grep batch | cut -d ':' -f 2)/ ${NGPUS},/\" \\\nds_config.json \\\n&gt; ds_config-polaris.json\n$ cat ds_config-polaris.json\n{\n\"train_batch_size\": 16,\n    \"gradient_accumulation_steps\": 1,\n    ...\n}\n</code></pre></p> <ol> <li> <p>The flag <code>-x ENVIRONMENT_VARIABLE</code> ensures the <code>$ENVIRONMENT_VARIABLE</code> will be set in the launched processes.\u00a0\u21a9</p> </li> <li> <p>Additional details for installing DeepSpeed can be found int their docs   from: Installation   Details \u21a9</p> </li> </ol>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/","title":"Distributed Training on ThetaGPU Using Data Parallelism","text":"<p>There are two schemes for distributed learning:</p> <ol> <li> <p>Model parallelization: in this scheme, disjoint subsets of a neural network are assigned to different devices. Therefore, all the computations associated to the subsets are distributed. Communication happens between devices whenever there is dataflow between two subsets. Model parallelization is suitable when the model is too large to be fitted into a single device (CPU/GPU) because of the memory capacity. However, partitioning the model into different subsets is not an easy task, and there might potentially introduce load imbalance issues limiting the scaling efficiency.\u202f </p> </li> <li> <p>Data parallelization: in this scheme, all the workers own a replica of the model. The global batch of data is split into multiple minibatches,\u202fand processed by different workers. Each worker computes the corresponding loss and gradients with respect to the data it posseses. Before the updating of the parameters at each epoch, the loss and gradients are averaged among all the workers through a collective operation. This scheme is relatively simple to implement. MPI_Allreduce is the only commu</p> </li> </ol> <p>Our recent presentation about the data parallel training can be found here: https://youtu.be/930yrXjNkgM</p> <p>In this documentation, we would like to show how to do data parallel training on ThetaGPU. </p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#software-environment-setup","title":"Software environment setup","text":"<p>We are still in the process of setting up the software stacks on ThetaGPU. Currently, one can get TensorFlow, PyTorch, and Horovod with the following setup script. <pre><code>source /lus/theta-fs0/software/datascience/thetagpu/anaconda3/setup.sh\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#tensorflow-with-horovod","title":"TensorFlow with Horovod","text":""},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#1-initialize-horovod","title":"1. Initialize Horovod","text":"<p><pre><code>import horovod.tensorflow as hvd hvd.init()\n</code></pre> After this initialization, the rank ID and the number of processes can be refered as <code>hvd.rank()</code> and <code>hvd.size()</code>. Besides, one can also call <code>hvd.local_rank()</code> to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank.</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#2-assign-gpu-to-each-rank","title":"2. Assign GPU to each rank","text":"<p><pre><code>gpus = tf.config.experimental.list_physical_devices('GPU') \nfor gpu in gpus: \n    tf.config.experimental.set_memory_growth(gpu, True) \nif gpus: \n    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n</code></pre> In this case, we set one GPU per process: <code>ID=hvd.local_rank()</code></p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#3-scale-the-learning-rate","title":"3. Scale the learning rate","text":"<p>Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01). <pre><code>opt = tf.train.AdagradOptimizer(0.01*hvd.size())\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#4-wrap-the-optimizer-with-distributed-optimizer","title":"4. Wrap the optimizer with Distributed Optimizer","text":"<pre><code>opt = hvd.DistributedOptimizer(opt)\n</code></pre>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#5-broadcast-the-model-from-rank-0","title":"5. Broadcast the model from rank 0","text":"<p>This is to make sure that all the workers will have the same starting point. <pre><code>hooks = [hvd.BroadcastGlobalVariablesHook(0)]\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#6-loading-data-according-to-rank-id","title":"6. Loading data according to rank ID","text":"<p>TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader. </p> <p>In general, one has two ways to deal with the data loading: </p> <ol> <li> <p>Each worker randomly select one batch of data from the dataset at each step. In such case, each worker can see the entire dataset. It is important to make sure that the different worker have different random seeds so that they will get different data at each step.</p> </li> <li> <p>Each worker accesses a subset of dataset. One manually partition the entire dataset into different partions, and each rank access one of the partions. </p> </li> </ol> <p>In both cases, the total number of steps per epoch is <code>nsamples / hvd.size()</code>.</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#7-checkpointing-on-root-rank","title":"7. Checkpointing on root rank","text":"<p>It is important to let only one process to do the checkpointing I/O lest perhaps the file been corrupted. </p> <pre><code>if hvd.rank() == 0: \n   checkpoint.save(checkpoint_dir)\n</code></pre>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#8-average-metric-across-all-the-workers","title":"8. Average metric across all the workers","text":"<p>Notice that in the distributed training, any tensor are local to each worker. In order to get the global averaged value, one can use Horovod allreduce. Below is an example on how to do the average.</p> <p><pre><code>def tensor_average(val, name): \n     tensor = torch.tensor(val) \n     if (with_hvd): \n         avg_tensor = hvd.allreduce(tensor, name=name) \n     else: \n         avg_tensor = tensor \n   return avg_tensor.item()\n</code></pre> We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/tensorflow2_mnist.py</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#pytorch-with-ddp","title":"PyTorch with DDP","text":"<p>PyTorch has its own native parallelization library called DDP. We will provide more details on how to run this on ThetaGPU. The current PyTorch on ThetaGPU does not have DDP built in. We will update to our users once we have DDP. </p> <p>For now, please refer to https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#mpi-profiling-for-data-parallel-training","title":"MPI Profiling for data parallel training","text":"<p>We support two ways for profling the performance of data parallel training. </p> <ol> <li>mpitrace library MPI trace allows us to get a flat profiling of all the MPI function calls involved during the training. To enable this, one can set the environment variable <pre><code>export LD_PRELOAD=/lus/theta-fs0/software/datascience/thetagpu/hpctw/lib/libmpitrace.so\n</code></pre> Then run the application as usual. MPI profiling results will be generated after the run finishes <code>mpi_profile.XXXX.[rank_id]</code>.</li> </ol> <p>Below is an example output: <pre><code>Data for MPI rank 0 of 8: \nTimes and statistics from MPI_Init() to MPI_Finalize(). \n----------------------------------------------------------------------- \nMPI Routine #calls avg. bytes time(sec) \n----------------------------------------------------------------------- \nMPI_Comm_rank 3 0.0 0.000 \nMPI_Comm_size 3 0.0 0.000 \nMPI_Bcast 520 197140.6 0.518 \nMPI_Allreduce 24561 208138.3 162.080 \nMPI_Gather 126 4.0 0.363 \nMPI_Gatherv 126 0.0 0.434 \nMPI_Allgather 2 4.0 0.000 \n----------------------------------------------------------------- \nMPI task 0 of 8 had the maximum communication time. \ntotal communication time = 163.396 seconds. \ntotal elapsed time = 187.298 seconds. \nuser cpu time = 4127.728 seconds. \nsystem time = 728.100 seconds. \nmax resident set size = 8403.938 MBytes. \n\nRank 0 reported the largest memory utilization : 8403.94 MBytes \nRank 0 reported the largest elapsed time : 187.30 sec \n----------------------------------------------------------------- \nMessage size distributions: \n                       MPI_Bcast      #calls   avg. bytes       time(sec) \n                                         126          4.0          0.008 \n                                           1          8.0          0.000 \n                                         121         25.0          0.006 \n                                          30        251.5          0.002 \n                                          32        512.0          0.002 \n                                          64       1024.0          0.005 \n                                          44       2048.0          0.003 \n                                          29       4092.8          0.003 \n                                          16       8192.0          0.032 \n\n                       MPI_Allreduce   #calls  avg. bytes       time(sec) \n                                        19780         8.0         90.822 \n                                         4576        24.0         18.239 \n                                           43      4004.0          0.295 \n                                            5   2780979.2          0.469 \n                                           50   8160289.2         20.893 \n                                            9  11803392.0          0.964 \n                                           48  28060640.0          3.293 \n                                           50  64731668.5         27.105 \n\n                       MPI_Gather      #calls   avg. bytes      time(sec) \n                                          126         4.0          0.363\n</code></pre> The useful information here is the message size distribution. </p> <ol> <li>Horovod Timeline To perform Horovod timeline analysis, one has to set the environment variable HOROVOD_TIMELINE which specifies the file for the output. export HOROVOD_TIMELINE=timeline.json This file is only recorded on rank 0, but it contains information about activity of all workers. You can then open the timeline file using the chrome://tracing facility of the Chrome browser.</li> </ol> <p>More details: https://horovod.readthedocs.io/en/stable/timeline_include.html</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/running-pytorch-conda/","title":"Running PyTorch with Conda","text":"<p>Be aware that these builds use CUDA and will not work on login nodes, which do not have CUDA installed as there are no GPUs.</p> <p>One can test these software packages in an interactive session: <pre><code>qsub -I -q single-gpu -n 1 -t 30 -A &lt;project-name&gt; --attrs filesystems=&lt;list of filesystems&gt;\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/running-pytorch-conda/#pytorch-master-build","title":"PyTorch (master build)","text":"<p>Users can find the latest builds via the <code>module avail</code> conda command, which will list available builds such as <code>conda/2021-06-26</code> which is a module that was built on 2021-06-26. Use <code>module show conda/2021-06-26</code> or <code>module help conda/2021-06-26</code> to get high level info on which versions of the key packages and libraries that this particular module contains. </p> <p>This version can be used by: <pre><code>module load conda/2021-06-26 # loads conda into your environment, sets up appropriate CUDA libraries and environment variables\nconda activate # add entries to PATH for the environment and run any activation scripts that the environment may contain\n</code></pre></p> <p>This will setup a conda environment with the \"from scratch\" build of PyTorch.</p> <p>This package will also include builds of TensorFlow and Horovod tagged releases.</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/running-pytorch-conda/#installing-packages","title":"Installing Packages","text":""},{"location":"theta-gpu/data-science-workflows/dl-frameworks/running-pytorch-conda/#using-pip-install-user-not-recommended","title":"Using <code>pip install --user</code> (not recommended)","text":"<p>With the conda environment setup, one can install common Python modules using <code>pip install --users &lt;module-name&gt;</code> which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>. The <code>$PYTHONUSERBASE</code> environment variable is automatically set when you load the base conda module, and is typically equal to <code>/home/$USER/.local/conda/YYYY-MM-DD</code> or  <code>/home/$USER/.local/thetagpu/conda/YYYY-MM-DD</code>, depending on the date of the module. </p> <p>Note, Python modules installed this way that contain command line binaries will not have those binaries automatically added to the shell's <code>$PATH</code>. To manually add the path: <pre><code>export PATH=$PYTHONUSERBASE/bin:$PATH\"\n</code></pre> Be sure to remove this location from <code>$PATH</code> if you deactivate the base Anaconda environment or unload the module. </p> <p>Cloning the Anaconda environment, or using <code>venv</code> are both more flexible and transparent when compared to <code>--user</code> installs. </p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/running-pytorch-conda/#using-conda-environments","title":"Using Conda Environments","text":"<p>If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via <code>conda install &lt;module&gt;</code> or <code>pip install &lt;module&gt;</code>.</p> <ol> <li>Setup the conda environment you want to use as instructed above.</li> <li>Create/edit your <code>$HOME/.condarc</code> file to include this these lines, replacing <code>&lt;project-name&gt;</code> with your project name. <code>&lt;path-to-your-project&gt;</code> is the path to the file system your project is on (e.g. <code>/lus/theta-fs0</code> or <code>/grand</code>or <code>/eagle</code>). By default, Conda will your <code>$HOME/.conda/*</code> area for caching files. </li> </ol> <p>Note: Since home directories are limited to 100GB, this fills up quickly. This addition tells Conda to use your project space for cache storage instead.</p> <pre><code>pkgs_dirs: \n  - &lt;path-to-your-project&gt;/&lt;project-name&gt;/conda/pkgs \nenvs_dirs: \n  - &lt;path-to-your-project&gt;/&lt;project-name&gt;/conda/envs\n</code></pre> <ol> <li>Clone the environment into a local path to which you have write access <pre><code>conda create --clone $CONDA_PREFIX -p &lt;path/to/env&gt;\n</code></pre></li> <li>Activate that environment: <pre><code>conda activate &lt;path/to/env&gt;\n</code></pre></li> </ol> <p>One should then be able to install modules natively.</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/running-tensorflow-conda/","title":"Running TensorFlow with Conda","text":"<p>Be aware that these builds use CUDA and will not work on login nodes, which do not have CUDA installed as there are no GPUs.</p> <p>One can test these software packages in an interactive session: <pre><code>qsub -I -q single-gpu -n 1 -t 30 -A &lt;project-name&gt; --attrs filesystems=&lt;list of filesystems&gt;\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/running-tensorflow-conda/#tensorflow-master-build","title":"TensorFlow (master build)","text":"<p>Users can find the latest builds via the <code>module avail</code> conda command, which will list available builds such as <code>conda/2021-06-26</code> which is a module that was built on 2021-06-26. Use <code>module show conda/2021-06-26</code> or <code>module help conda/2021-06-26</code> to get high level info on which versions of the key packages and libraries that this particular module contains. </p> <p>This version can be used by: <pre><code>module load conda/2021-06-26 # loads conda into your environment, sets up appropriate CUDA libraries and environment variables\nconda activate # add entries to PATH for the environment and run any activation scripts that the environment may contain\n</code></pre> This will setup a conda environment with the \"from scratch\" build of TensorFlow.</p> <p>This package will also include builds of PyTorch and Horovod tagged releases.</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/running-tensorflow-conda/#installing-packages","title":"Installing Packages","text":""},{"location":"theta-gpu/data-science-workflows/dl-frameworks/running-tensorflow-conda/#using-pip-install-user-not-recommended","title":"Using <code>pip install --user</code> (not recommended)","text":"<p>With the conda environment setup, one can install common Python modules using <code>pip install --users &lt;module-name&gt;</code> which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>. The <code>$PYTHONUSERBASE</code> environment variable is automatically set when you load the base conda module, and is typically equal to <code>/home/$USER/.local/conda/YYYY-MM-DD</code> or  <code>/home/$USER/.local/thetagpu/conda/YYYY-MM-DD</code>, depending on the date of the module. </p> <p>Note, Python modules installed this way that contain command line binaries will not have those binaries automatically added to the shell's <code>$PATH</code>. To manually add the path: <pre><code>export PATH=$PYTHONUSERBASE/bin:$PATH\"\n</code></pre> Be sure to remove this location from <code>$PATH</code> if you deactivate the base Anaconda environment or unload the module. </p> <p>Cloning the Anaconda environment, or using <code>venv</code> are both more flexible and transparent when compared to <code>--user</code> installs. </p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/running-tensorflow-conda/#using-conda-environments","title":"Using Conda Environments","text":"<p>If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via <code>conda install &lt;module&gt;</code> or <code>pip install &lt;module&gt;</code>.</p> <ol> <li> <p>Setup the conda environment you want to use as instructed above.</p> </li> <li> <p>Create/edit your <code>$HOME/.condarc</code> file to include this these lines, replacing <code>&lt;project-name&gt;</code> with your project name. <code>&lt;path-to-your-project&gt;</code> is the path to the file system your project is on (e.g. <code>/lus/theta-fs0</code> or <code>/grand</code>or <code>/eagle</code>). By default, Conda will your <code>$HOME/.conda/*</code> area for caching files. </p> </li> </ol> <p>Note: Since home directories are limited to 100GB, this fills up quickly. This addition tells Conda to use your project space for cache storage instead.</p> <p><pre><code>pkgs_dirs: \n   - /lus/theta-fs0/projects/&lt;project-name&gt;/conda/pkgs \nenvs_dirs: \n   - /lus/theta-fs0/projects/&lt;project-name&gt;/conda/envs\n</code></pre> 3. Clone the environment into a local path to which you have write access. <pre><code>conda create --clone $CONDA_PREFIX -p &lt;path/to/env&gt;\n</code></pre> 4. Activate that environment. <pre><code>conda activate &lt;path/to/env&gt;\n</code></pre></p> <p>One should then be able to install modules natively.</p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/tensorboard-instructions/","title":"TensorBoard Instructions","text":"<p>If you are able to install TensorBoard on your local machine, it is often easiest to copy the requisite files from ALCF file systems (via <code>sftp</code>, <code>scp</code>, Globus, etc.) to your local machine and run a TensorBoard there. </p> <p>However, if that is not possible, or if you have many and/or large files that TensorBoard needs to process located on ALCF file systems, there are several ways to run a TensorBoard server remotely. </p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/tensorboard-instructions/#tensorboard-server-on-a-thetagpu-compute-node","title":"TensorBoard server on a ThetaGPU compute node","text":"<p>This approach can be useful to have TensorBoard analyze live training progress. After you have logged into ThetaGPU, and have an interactive job running, you'll need to know the name of one of your worker nodes so you can SSH to it. <pre><code>PORT0=9991 \nPORT1=9992 \nPORT3=9993 \n# Select a theta login node N where N=[1-6] ssh -L $PORT0:localhost:$PORT1 $USER@thetaloginN.alcf.anl.gov \n\n# after reaching thetaloginN \n\n# Replace NN with your thetagpu worker node ssh -L $PORT1:thetagpuNN:$PORT3 $USER@thetagpusn1 \n# after reaching thetagpusn1 \n\n# login to worker node \nssh thetagpuNN \n\n# now setup your tensorflow environment \n# for instance run the conda setup.sh script created during the install_tensorflow.sh script \n\n# now run tensorboard \ntensorboard --logdir &lt;/path/to/logs&gt; --port $PORT3 --bind_all\n</code></pre></p>"},{"location":"theta-gpu/data-science-workflows/dl-frameworks/tensorboard-instructions/#tensorboard-server-on-a-thetaknl-login-node","title":"TensorBoard server on a ThetaKNL login node","text":"<p>If you do not require the use of a GPU during analysis while TensorBoard runs, and you do not require a cutting-edge version of TensorBoard (this will load version 2.6.0), you can avoid additional SSH tunnel hops by running the TensorBoard server on a ThetaKNL login node: <pre><code>ssh -D &lt;some-port-number&gt;  theta.alcf.anl.gov\n\nmodule load conda/2021-09-22\nexport LD_LIBRARY_PATH=/soft/thetagpu/cuda/cuda_11.3.0_465.19.01_linux/lib64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/soft/thetagpu/cuda/cuda_11.3.0_465.19.01_linux/extras/CUPTI/lib64/:$LD_LIBRARY_PATH\n</code></pre></p>"},{"location":"theta-gpu/hardware-overview/theta-gpu-machine-overview/","title":"ThetaGPU Machine Overview","text":"<p>ThetaGPU is an extension of Theta and is comprised of 24 NVIDIA DGX A100 nodes. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs and two AMD Rome CPUs that provide 22 with 320 GB of GPU memory and two nodes with 640 GB of GPU memory (8320 GB aggregately) of GPU memory for training artificial intelligence (AI) datasets, while also enabling GPU-specific and -enhanced high-performance computing (HPC) applications for modeling and simulation</p> <p>The DGX A100\u2019s integration into Theta is achieved via the ALCF\u2019s Cobalt HPC scheduler and shared access to a 10-petabyte Lustre filesystem. Fixed ALCF user accounts ensure a smooth onboarding process for the expanded system.</p> <p>A 15-terabyte solid-state drive offers up to 25 gigabits per second in bandwidth. The dedicated compute fabric comprises 20 Mellanox QM9700 HDR200 40-port switches wired in a fat-tree topology. ThetaGPU cannot utilize the Aries interconnect.</p>"},{"location":"theta-gpu/hardware-overview/theta-gpu-machine-overview/#table-1-summarizes-the-capabilities-of-a-thetagpu-compute-node","title":"Table 1 summarizes the capabilities of a ThetaGPU compute node.","text":"COMPONENT COMPONENT AGGREGATE AMD Rome 64-core CPU 2 48 DDR4 Memory 1 TB on 320 GB &amp; 2 TB on 640 GB 26 TB NVIDIA A100 GPU 8 192 GPU Memory 22 nodes w/ 320 GB &amp; 2 nodes w/ 640 GB 8,320 GB HDR200 Compute Ports 8 192 HDR200 Storage Ports 2 48 100GbE Ports 2 48 3.84 TB Gen4 NVME drives 4 96"},{"location":"theta-gpu/hardware-overview/theta-gpu-machine-overview/#thetagpu-login-nodes","title":"ThetaGPU Login Nodes","text":"<p>The Theta login nodes (see above) will be the intended method to access ThetaGPU.  At first, Cobalt jobs cannot be submitted from the Theta login nodes to run on the GPU nodes; until that is supported, users will need to login in to the ThetaGPU service nodes (thetagpusn1 or thetagpusn2) from the Theta login nodes, and from there Cobalt jobs can be submitted to run on the GPU nodes.</p>"},{"location":"theta-gpu/hardware-overview/theta-gpu-machine-overview/#references","title":"References","text":"<ul> <li>Cray DVS</li> <li>Cray Aries</li> <li>Lustre</li> <li>IBM GPFS</li> </ul>"},{"location":"theta-gpu/performance-tools/darshan/","title":"Darshan on ThetaGPU","text":""},{"location":"theta-gpu/performance-tools/darshan/#overview","title":"Overview","text":"<p>Darshan instrumentation on ThetaGPU is not automatically included into the binary like Theta. The user must set the <code>LD_PRELOAD</code> variable as part of running the job.</p> <p>Logs will be generated in the directory: /lus/grand/logs/darshan/thetagpu/// <p>In order to view a log, use the darshan-parser utility. <pre><code>module load darshan \nmpirun ... -x LD_PRELOAD=$DARSHAN_PRELOAD\n</code></pre></p>"},{"location":"theta-gpu/performance-tools/darshan/#more-information","title":"More information","text":"<p>See the Theta documentation for more ALCF specific Darshan usage.  </p>"},{"location":"theta-gpu/performance-tools/nvidia-nsight/","title":"NVIDIA Nsight","text":""},{"location":"theta-gpu/performance-tools/nvidia-nsight/#references","title":"References","text":"<ul> <li>NVIDIA Nsight Systems Documentation</li> <li>NVIDIA Nsight Compute Documentation</li> </ul>"},{"location":"theta-gpu/performance-tools/nvidia-nsight/#introduction","title":"Introduction","text":"<p>NVIDIA\u00ae Nsight\u2122 Systems provides developers a system-wide visualization of an applications performance. Developers can optimize bottlenecks to scale efficiently across any number or size of CPUs and GPUs on ThetaGPU. For further optimizations to compute kernels developers should use Nsight Compute.</p> <p>The NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. </p> <p>In addition, the baseline feature of this tool allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface,  metric collection, and can be extended with analysis scripts for post-processing results.</p>"},{"location":"theta-gpu/performance-tools/nvidia-nsight/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"theta-gpu/performance-tools/nvidia-nsight/#1-common-part-on-thetagpu","title":"1. Common part on thetaGPU","text":"<p>Build your application for ThetaGPU and submit your job script to ThetaGPU or start an interactive job mode on ThetaGPU as follows: <pre><code>$ module load cobalt/cobalt-gpu\n$ qsub -I -n 1 -t 30 -q full-node -A {your_project}\n</code></pre></p>"},{"location":"theta-gpu/performance-tools/nvidia-nsight/#2-nsight-systems","title":"2. Nsight Systems","text":"<p>Run your application with Nsight Systems as follows: <pre><code>$ nsys profile -o {output_filename} --stats=true ./{your_application}\n</code></pre></p>"},{"location":"theta-gpu/performance-tools/nvidia-nsight/#3-nsight-compute","title":"3. Nsight Compute","text":"<p>Run your application with Nsight Compute. <pre><code>$ ncu --set detailed -k {kernel_name} -o {output_filename} ./{your_application}\n</code></pre> Note: Without -o option, Nsight Compute provides performance data as a standard output</p>"},{"location":"theta-gpu/performance-tools/nvidia-nsight/#4-post-processing-the-profiled-data","title":"4. Post-processing the profiled data","text":""},{"location":"theta-gpu/performance-tools/nvidia-nsight/#post-processing-via-cli","title":"Post-processing via CLI.","text":"<pre><code>$ nsys stats {output_filename}.qdrep\n$ ncu -i {output_filename}.ncu-rep\n</code></pre>"},{"location":"theta-gpu/performance-tools/nvidia-nsight/#post-processing-on-your-local-system-via-gui","title":"Post-processing on your local system via GUI","text":"<ul> <li>Install NVIDIA Nsight Systems and NVIDIA Nsight Compute after downloading both of them from the  NVIDIA Developer Zone.</li> <li>Download nsys output files (i.e., ending with .qdrep and . sqlite) to your local system, and then open them with NVIDIA Nsight Systems on your local system.</li> <li>Download ncu output files (i.e., ending with .ncu-rep) to your local system, and then open them with NVIDIA Nsight Compute on your local system.</li> </ul> <p>For more options for performance analysis with Nsight Systems and Nsight Compute: <pre><code>$ nsys --help\n$ ncu --help\n</code></pre></p>"},{"location":"theta-gpu/performance-tools/nvidia-nsight/#a-quick-example","title":"A quick example","text":""},{"location":"theta-gpu/performance-tools/nvidia-nsight/#nsight-systems","title":"Nsight Systems","text":""},{"location":"theta-gpu/performance-tools/nvidia-nsight/#running-a-stream-benchmark-with-nsight-systems","title":"Running a stream benchmark with Nsight Systems","text":"<pre><code>jkwack@thetagpu18:~/HPC_benchmarks/BabelStream/JK_thetaGPU$ nsys profile -o JKreport-nsys-BableStream --stats=true ./cuda-stream\n\nWarning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n\nCollecting data...\n\nBabelStream\n\nVersion: 3.4\n\nImplementation: CUDA\n\nRunning kernels 100 times\n\nPrecision: double\n\nArray size: 268.4 MB (=0.3 GB)\n\nTotal size: 805.3 MB (=0.8 GB)\n\nUsing CUDA device A100-SXM4-40GB\n\nDriver: 11000\n\nFunction    MBytes/sec  Min (sec)   Max         Average     \n\nCopy        1381210.283 0.00039     0.00040     0.00039     \n\nMul         1339635.322 0.00040     0.00041     0.00040     \n\nAdd         1357739.235 0.00059     0.00061     0.00060     \n\nTriad       1366533.461 0.00059     0.00061     0.00060     \n\nDot         1210611.093 0.00044     0.00047     0.00046     \n\nProcessing events...\n\nCapturing symbol files...\n\nSaving temporary \"/tmp/nsys-report-b948-7122-9b9a-feb1.qdstrm\" file to disk...\n\nCreating final output files...\n\nProcessing [==============================================================100%]\n\nSaved report file to \"/tmp/nsys-report-b948-7122-9b9a-feb1.qdrep\"\n\nExporting 7098 events: [==================================================100%]\n\nExported successfully to\n\n/tmp/nsys-report-b948-7122-9b9a-feb1.sqlite\n\nGenerating CUDA API Statistics...\n\nCUDA API Statistics (nanoseconds)\n\nTime(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n\n   44.8       280504347           4      70126086.8         1050249       276881346  cudaMalloc                                                                      \n\n   31.4       196878210         401        490968.1          381542          600948  cudaDeviceSynchronize                                                           \n\n   22.4       140280462         103       1361946.2          436597        32339232  cudaMemcpy                                                                      \n\n    1.0         6263864           4       1565966.0         1236542         1884610  cudaFree                                                                        \n\n    0.4         2729558         501          5448.2            4970           36269  cudaLaunchKernel                                                                \n\nGenerating CUDA Kernel Statistics...\n\nCUDA Kernel Statistics (nanoseconds)\n\nTime(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n\n   24.7        58518170         100        585181.7          580347          594395  void add_kernel&lt;double&gt;(double const*, double const*, double*)                                                                                                                                                                                                                                                                               \n\n   24.6        58312184         100        583121.8          576987          595067  void triad_kernel&lt;double&gt;(double*, double const*, double const*)                                                                                                                                                                                                                                                                             \n\n   18.1        42942748         100        429427.5          419548          438333  void dot_kernel&lt;double&gt;(double const*, double const*, double*, int)                                                                                                                                                                                                                                                                          \n\n   16.5        39062588         100        390625.9          388733          392125  void mul_kernel&lt;double&gt;(double*, double const*)                                                                                                                                                                                                                                                                                              \n\n   16.0        37980930         100        379809.3          376541          392925  void copy_kernel&lt;double&gt;(double const*, double*)                                                                                                                                                                                                                                                                                             \n\n    0.2          521628           1        521628.0          521628          521628  void init_kernel&lt;double&gt;(double*, double*, double*, double, double, double)                                                                                                                                                                                                                                                                  \n\nGenerating CUDA Memory Operation Statistics...\n\nCUDA Memory Operation Statistics (nanoseconds)\n\nTime(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n\n  100.0        94988808         103        922221.4            2335        32089877  [CUDA memcpy DtoH]                                                              \n\nCUDA Memory Operation Statistics (KiB)\n\n              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n\n-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n\n         786632.000             103             7637.204              2.000           262144.000  [CUDA memcpy DtoH]                                                              \n\nGenerating Operating System Runtime API Statistics...\n\nOperating System Runtime API Statistics (nanoseconds)\n\nTime(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n\n   47.6      1184528854          22      53842220.6           10240       100064607  sem_timedwait                                                                   \n\n   36.2       901118663          20      45055933.1           16792       100119932  poll                                                                            \n\n   15.9       395394013        1456        271561.8            1012        17141907  ioctl                                                                           \n\n    0.2         4052477         105         38595.0            2064          111321  open64                                                                          \n\n    0.1         1716108          86         19954.7            1042          509715  mmap                                                                            \n\n    0.0          963824          51         18898.5            1363          771371  fopen                                                                           \n\n    0.0          208937           4         52234.3           42771           62549  pthread_create                                                                  \n\n    0.0          141128           3         47042.7           41178           58621  fgets                                                                           \n\n    0.0           52102          11          4736.5            1824           18145  munmap                                                                          \n\n    0.0           41950           6          6991.7            1783           19146  putc                                                                            \n\n    0.0           37641           5          7528.2            2444           13065  open                                                                            \n\n    0.0           32953          11          2995.7            1422            6402  write                                                                           \n\n    0.0           31581          23          1373.1            1042            1823  fclose                                                                          \n\n    0.0           16470           2          8235.0            1412           15058  sched_yield                                                                     \n\n    0.0            8927           2          4463.5            3437            5490  socket                                                                          \n\n    0.0            8586           1          8586.0            8586            8586  pipe2                                                                           \n\n    0.0            7324           3          2441.3            1593            3627  fwrite                                                                          \n\n    0.0            5782           2          2891.0            1844            3938  fread                                                                           \n\n    0.0            5751           1          5751.0            5751            5751  connect                                                                         \n\n    0.0            4369           2          2184.5            1714            2655  read                                                                            \n\n    0.0            3998           3          1332.7            1082            1603  fcntl                                                                           \n\n    0.0            1433           1          1433.0            1433            1433  fflush                                                                          \n\n    0.0            1252           1          1252.0            1252            1252  bind                                                                            \n\nGenerating NVTX Push-Pop Range Statistics...\n\nNVTX Push-Pop Range Statistics (nanoseconds)\n\nReport file moved to \"/gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/JKreport-nsys-BableStream.qdrep\"\n\nReport file moved to \"/gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/JKreport-nsys-BableStream.sqlite\"\n</code></pre>"},{"location":"theta-gpu/performance-tools/nvidia-nsight/#nsight-compute","title":"Nsight Compute","text":""},{"location":"theta-gpu/performance-tools/nvidia-nsight/#running-a-stream-benchmark-with-nsight-compute-for-triad_kernel","title":"Running a stream benchmark with Nsight Compute for triad_kernel","text":"<pre><code>jkwack@thetagpu18:~/HPC_benchmarks/BabelStream/JK_thetaGPU$ ncu --set detailed -k triad_kernel -o JKreport-ncu_detailed-triad_kernel-BableStream ./cuda-stream\n\nBabelStream\n\nVersion: 3.4\n\nImplementation: CUDA\n\nRunning kernels 100 times\n\nPrecision: double\n\nArray size: 268.4 MB (=0.3 GB)\n\nTotal size: 805.3 MB (=0.8 GB)\n\n==PROF== Connected to process 166971 (/gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/cuda-stream)\n\nUsing CUDA device A100-SXM4-40GB\n\nDriver: 11000\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\nFunction    MBytes/sec  Min (sec)   Max         Average     \n\nCopy        1336793.345 0.00040     0.00042     0.00041     \n\nMul         1307948.274 0.00041     0.00043     0.00042     \n\nAdd         1335561.797 0.00060     0.00062     0.00062     \n\nTriad       976.089     0.82503     1.00961     0.87930     \n\nDot         1081921.148 0.00050     0.00055     0.00053     \n\n==PROF== Disconnected from process 166971\n\n==PROF== Report: /gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/JKreport-ncu_detailed-triad_kernel-BableStream.ncu-rep\n</code></pre>"},{"location":"theta-gpu/programming-models/kokkos/","title":"Kokkos on ThetaGPU","text":""},{"location":"theta-gpu/programming-models/kokkos/#overview","title":"Overview","text":"<p>Kokkos implements a programming model in C++ for writing performance portable applications targeting all major HPC platforms. It provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. It can use OpenMP, etc as a backend programming model. For more information please visit https://github.com/kokkos/kokkos</p> <p>The Kokkos shared memory programming model is a C++ library, that provides the necessary architecture specific backends (e.g. OpenMP, CUDA, \u2026). To begin with, though, it is important to note that the Kokkos programming model is usable only in C/C++ codes. Hence, for those with Fortran codes, Kokkos must first be encapsulated within C/C++ functions and called from the main Fortran code.</p> <p>The purpose of this document is to provide guidance on using Kokkos on Cooley. Please see the following pages for tutorials and more information on Kokkos: Kokkos GitHub and Kokkos Tutorials. </p>"},{"location":"theta-gpu/programming-models/kokkos/#using-kokkos-at-alcf","title":"Using Kokkos at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"theta-gpu/programming-models/kokkos/#building-kokkos-on-thetagpu","title":"Building Kokkos on ThetaGPU","text":"<p>Please follow the steps below to build Kokks on ThetaGPU.</p>"},{"location":"theta-gpu/programming-models/kokkos/#step-1-clone-the-repository","title":"Step 1:  Clone the repository","text":"<p><pre><code>git clone https://github.com/kokkos/kokkos.git \ncd kokkos \nexport KOKKOS_PATH=\u201d${PWD}\u201d\n</code></pre> Note:  the Kokkos Project strives to keep the master branch stable.</p>"},{"location":"theta-gpu/programming-models/kokkos/#step-2-make-sure-that-a-relatively-new-version-of-cmake-is-available","title":"Step 2:  Make sure that a relatively new version of CMake is available","text":"<pre><code>module load cmake/3.14.5\n</code></pre>"},{"location":"theta-gpu/programming-models/kokkos/#step-3-create-a-build-directory","title":"Step 3:  Create a build directory","text":"<pre><code>mkdir build &amp;&amp; cd build\n</code></pre>"},{"location":"theta-gpu/programming-models/kokkos/#step-4-generate-the-makefile","title":"Step 4:  Generate the Makefile","text":"<p><pre><code>cmake ../kokkos \\\n    -DCMAKE_CXX_COMPILER=CC \\\n    -DCMAKE_INSTALL_PREFIX=${PWD}/kokkos-install \\\n    -DKokkos_ENABLE_OPENMP=On \\\n    -DKokkos_ENABLE_HWLOC=On \\\n    -DKokkos_HWLOC_DIR=/usr \\\n    -DHWLOC_LIBRARY=/usr/lib64/libhwloc.so\n</code></pre> Note:  the default compiler on Theta should be sufficient to build Kokkos.</p>"},{"location":"theta-gpu/programming-models/kokkos/#step-5-install-kokkos","title":"Step 5: Install Kokkos","text":"<p><pre><code>make install\n</code></pre> Note: This will end up installing Kokkos in <code>${KOKKOS_PATH}/build/kokkos-install</code>. </p> <p>If you wish to install it in a different directory, change <code>CMAKE_INSTALL_PREFIX</code> in step 4.</p>"},{"location":"theta-gpu/programming-models/openmp/","title":"OpenMP on ThetaGPU","text":""},{"location":"theta-gpu/programming-models/openmp/#openmp-threading-on-cpu","title":"OpenMP threading on CPU","text":"<p>All the compilers available on ThetaGPU supports OpenMP threading.</p>"},{"location":"theta-gpu/programming-models/openmp/#openmp-offload-on-a100-gpu","title":"OpenMP offload on A100 GPU","text":"<p>A few compilers which support OpenMP offload are accessible on ThetaGPU. They are made available via modules.</p> <pre><code>$ module avail llvm\n\n--------- /lus/theta-fs0/software/environment/thetagpu/lmod/modulefiles ----------\n   llvm/main-20210112    llvm/release-12.0.0 (D)\n\n$ module avail nvhpc\n\n--------- /lus/theta-fs0/software/environment/thetagpu/lmod/modulefiles ----------\n   nvhpc-byo-compiler/20.9 (D)    nvhpc-nompi/20.9 (D)    nvhpc/20.9 (D)\n   nvhpc-byo-compiler/21.2        nvhpc-nompi/21.2        nvhpc/21.2\n   nvhpc-byo-compiler/21.3        nvhpc-nompi/21.3        nvhpc/21.3\n</code></pre>"},{"location":"theta-gpu/programming-models/openmp/#llvm-clang-for-cc","title":"LLVM Clang for C/C++","text":"<ul> <li>Clang OpenMP offload features </li> <li>More details about the OpenMP runtime</li> </ul> <p>If there is an issue with the compiler, feel free to contact openmp-dev@lists.llvm.org</p>"},{"location":"theta-gpu/programming-models/openmp/#warning-message","title":"Warning message","text":"<pre><code>clang-12 warning: Unknown CUDA version. version.txt: 11.0.205. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\n</code></pre> <p>Means CUDA 11 language features are not supported. As long as these features are not used, the full CUDA 11.x toolchain works. This warning can be ignored or suppressed by -Wno-unknown-cuda-version compiler option.</p>"},{"location":"theta-gpu/programming-models/openmp/#compiling-example","title":"Compiling example","text":"<pre><code>module load llvm/release-12.0.0\nclang++ -fopenmp -fopenmp-targets=nvptx64 your_source.cpp\n</code></pre>"},{"location":"theta-gpu/programming-models/openmp/#nvidia-hpc-sdk-for-ccfortran","title":"NVIDIA HPC SDK for C/C++/Fortran","text":"<p>OpenMP documentation</p> <p>For compiler bugs, please file bug reports at https://developer.nvidia.com after login</p>"},{"location":"theta-gpu/programming-models/openmp/#compiling-example_1","title":"Compiling example","text":"<pre><code>module load nvhpc-sdk/nvhpc/21.3\nnvfortran -mp=gpu -gpu=cc80 your_source.f90\n</code></pre>"},{"location":"theta-gpu/programming-models/raja/","title":"RAJA ThetaGPU","text":""},{"location":"theta-gpu/programming-models/raja/#overview","title":"Overview","text":"<p>RAJA is a collection of C++ software abstractions, being developed at Lawrence Livermore National Laboratory (LLNL), that enable architecture portability for HPC applications. The overarching goals of RAJA are to: - Make existing (production) applications portable with minimal disruption - Provide a model for new applications so that they are portable from inception.</p> <p>RAJA targets portable, parallel loop execution by providing building blocks that extend the generally-accepted parallel for idiom.</p> <p>Additional information can be found at RAJA User Guide.</p>"},{"location":"theta-gpu/programming-models/raja/#using-raja","title":"Using RAJA","text":"<p>RAJA provides a project template for how to use RAJA in an application project that uses CMake or Make. This is located at RAJA Project Template.</p>"},{"location":"theta-gpu/programming-models/raja/#how-to-get-the-source-code","title":"How to get the source code","text":"<p>The RAJA source code lives at RAJA github. </p> <p>It can be cloned with git clone <code>--recursive https://github.com/llnl/raja.git</code>. The recursive clone will also clone RAJA's dependencies in the proper locations.</p>"},{"location":"theta-gpu/programming-models/raja/#add-a-cmake-configuration-for-thetagpu","title":"Add a cmake configuration for ThetaGPU","text":"<pre><code>raja/host-configs/alcf-builds/thetagpu.cmake\n</code></pre> <pre><code>set(RAJA_COMPILER \"RAJA_COMPILER_GNU\" CACHE STRING \"\")\n\nset(CMAKE_CXX_COMPILER \"g++\" CACHE PATH \"\")\nset(CMAKE_C_COMPILER \"gcc\" CACHE PATH \"\")\n\nset(CMAKE_CXX_FLAGS_RELEASE \"-O3\" CACHE STRING \"\")\nset(CMAKE_CXX_FLAGS_RELWITHDEBINFO \"-O3\" CACHE STRING \"\")\nset(CMAKE_CXX_FLAGS_DEBUG \"-O0 -g\" CACHE STRING \"\")\n\nset(CUDA_COMMON_OPT_FLAGS -restrict; --gpu-architecture sm_80; -std c++11; --expt-extended-lambda)\nset(CUDA_COMMON_DEBUG_FLAGS -restrict; --gpu-architecture sm_80; -std c++11; --expt-extended-lambda)\n\nset(HOST_OPT_FLAGS -Xcompiler -O3 -Xcompiler -fopenmp)\n\nif(CMAKE_BUILD_TYPE MATCHES Release)\n  set(RAJA_NVCC_FLAGS -O3; ${CUDA_COMMON_OPT_FLAGS}; -ccbin; ${CMAKE_CXX_COMPILER} ; ${HOST_OPT_FLAGS} CACHE LIST \"\")\nelseif(CMAKE_BUILD_TYPE MATCHES RelWithDebInfo)\n  set(RAJA_NVCC_FLAGS -g; -G; -O3; ${CUDA_COMMON_OPT_FLAGS}; -ccbin; ${CMAKE_CXX_COMPILER} ; ${HOST_OPT_FLAGS} CACHE LIST \"\")\nelseif(CMAKE_BUILD_TYPE MATCHES Debug)\n  set(RAJA_NVCC_FLAGS -g; -G; -O0; ${CUDA_COMMON_DEBUG_FLAGS}; -ccbin; ${CMAKE_CXX_COMPILER} ; -Xcompiler -fopenmp CACHE LIST \"\")\nendif()\n\nset(RAJA_RANGE_ALIGN 4 CACHE INT \"\")\nset(RAJA_RANGE_MIN_LENGTH 32 CACHE INT \"\")\nset(RAJA_DATA_ALIGN 64 CACHE INT \"\")\n\nset(RAJA_HOST_CONFIG_LOADED On CACHE Bool \"\")\n</code></pre>"},{"location":"theta-gpu/programming-models/raja/#now-build-on-the-thetagpu-compute-node","title":"Now build on the ThetaGPU compute node","text":"<pre><code>git clone --recursive https://github.com/llnl/raja.git\n\nmkdir build_alcf-thetagpu &amp;&amp; cd build_alcf-thetagpu\ncmake \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -C ../host-configs/alcf-builds/thetagpu.cmake \\\n  -DENABLE_OPENMP=On \\\n  -DENABLE_CUDA=On \\\n  -DCUDA_ARCH=sm_80 \\\n  -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda \\\n  -DCMAKE_INSTALL_PREFIX=../install_${BUILD_SUFFIX} \\\n  \"$@\" \\\n\n  ..\n</code></pre>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/","title":"Job and Queue Scheduling on ThetaGPU","text":""},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#queues-and-job-scheduling","title":"Queues and Job Scheduling","text":""},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#general-policy","title":"General Policy","text":"<p>We ask that all users follow good etiquette and be kind to one another.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#job-priority","title":"Job Priority","text":"<p>As with all Argonne Leadership Computing Facility production systems, job priority in the queue is based on several criteria:</p> <ul> <li>positive balance of your project</li> <li>size (in nodes) of the job, larger jobs receive higher priority</li> <li>the type of project (e.g. INCITE, ALCC, or discretionary)</li> <li>job duration - shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible</li> </ul>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#reservations-and-scheduling-policy","title":"Reservations and Scheduling Policy","text":"<p>Some work will require use of Theta that requires deviation from regular policy. On such occasions, normal reservation policy applies. Please send the regular form no fewer than five (5) business days in advance.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#monday-maintenance","title":"Monday Maintenance","text":"<p>When the ALCF is on a regular business schedule, preventitive maintenance is typically scheduled on alternate Mondays. The showres command may be used to view pending and active maintenance reservations.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#big-run-mondays","title":"Big Run Mondays","text":"<p>As part of our regular maintenance procedures on Mondays, we will promote to the highest priority any jobs in the queued state requesting 802 nodes or more (.ie. capability jobs). Promotion is subject to operational discretion.</p> <p>We may also, at our discretion, take the opportunity to promote the priority of capability jobs if the system has been drained of jobs for any other reason.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#incitealcc-overburn-policy","title":"INCITE/ALCC Overburn Policy","text":"<p>If an INCITE or ALCC project has exhausted its allocation in the first 11 months of its allocation year, it is eligible for overburn running. At this point, capability jobs submitted by INCITE and ALCC projects will run in the default queue (instead of backfill) for the first 11 months of the allocation year until 125% of the project allocation has been consumed. Note that non-capability jobs will be routed to backfill queue.</p> <p>INCITE and ALCC projects needing additional overburn hours should e-mail support@alcf.anl.gov with a short description of what they plan to do with the additional hours, highlighting specific goals or milestones and the time expected to accomplish them. This will be reviewed by the scheduling committee, allocations committee, and ALCF management. Requests should be submitted 15 days before the start of the next quarter of the allocation year for full consideration. Non-capability jobs from projects that have exhausted their allocation will continue to run in backfill. </p> <p>To be clear, this policy does not constitute a guarantee of extra time, and we reserve the right to prioritize the scheduling of jobs submitted by projects that have not yet used 100% of their allocations, so the earlier that an INCITE or ALCC project exhausts its allocation, the more likely it is to be able to take full advantage of this policy.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#thetagpu-node-queues","title":"ThetaGPU Node Queues","text":"<p>Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation request. ThetaGPU is listed under Theta on the form.</p> <p>The GPU nodes are new and we expect the workload to be significantly different than it is on the KNL nodes.  This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#nodes-vs-queue-vs-mig-mode","title":"Nodes vs Queue vs MIG mode","text":"<p>The GPU nodes are NVidia DGX A100 nodes and each node contains eight (8) A100 GPUs.  You may request either entire nodes, or a single GPU based on your job needs.  What you will get is determined by the queue you submit to (See Queues section below).  If it has node in the name, you will get nodes.  If it has GPU in the name, you will get a single GPU. Note that if you need more than a single GPU, you should submit to the full-node queue.</p> <p>Additionally, the Nvidia A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode.  This allows a single GPU to be shared by up to 7 different processes.  We do not schedule at this level, but to use MIG capabilities, you may pass <code>\u2013attrs mig-mode=True</code> in with your qsub and use the <code>nvidia-smi_mig</code> command (note the UNDERSCORE) just as you would calling <code>nvidia-smi mig ...</code> directly. Attempts to call <code>nvidia-smi mig ...</code>(no underscore) directly will result in an error message. The single-gpu host will, by default, not create an MIG instance and users will have direct access to the gpu. If you are not using MIG mode, your session will appear as if it were a normal full-node system with only one gpu. Note that as of 12/13/21, MIG mode is unavailable for full-node jobs.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#queues","title":"Queues","text":"<p>There are three primary queues:   - full-node: This is the general production queue for jobs that require full nodes. The -n parameter in your qsub will match the resource type in this queue i.e. -n 2 in node queue will get you two full nodes.   - bigmem -  2 of the nodes have 640 GB of memory compared to the other 22 nodes with 320 GB. Use this queue to access these 2 nodes by specifying <code>-q bigmem</code> in your script. A max of 2 nodes (-n 2) can be requested in this queue.   - single-gpu: This is the general production queue for jobs that operate best on a single GPUs. The -n parameter in your qsub should always be 1 as you can only submit to a single gpu. If you need more than 1 gpu, use the full-node queue.</p> <p>Here are the initial queue limits. You may not violate either of these policies.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#full-node-queue","title":"full-node queue:","text":"<ul> <li>MinTime is 5 minutes</li> <li>MaxTime is 12 hours</li> <li>MaxQueued will be 20 jobs</li> <li>MaxRunning will be 10 jobs</li> </ul>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#bigmem-queue","title":"bigmem queue:","text":"<ul> <li>MinTime is 5 minutes</li> <li>MaxTime is 12 hours</li> <li>MaxQueued is 2 jobs</li> <li>MaxRunning is 1 job</li> </ul>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#single-gpu-queue","title":"single-gpu queue:","text":"<ul> <li>MinTime is 5 minutes</li> <li>MaxTime is 1 hour</li> <li>MaxQueued is 1 job</li> <li>MaxRunning is 1 job</li> </ul> <p>The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#running-jobs-on-thetagpu","title":"Running Jobs On ThetaGPU","text":"<p>Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation request. ThetaGPU is listed under Theta on the form.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#running-on-multiple-gpu-nodes","title":"Running on multiple GPU nodes","text":"<p>Until there is tighter integration of Cobalt and mpirun on GPU nodes, the user will have to identify the nodes Cobalt assigned to their job and pass them as options to mpirun along with some other mpirun options.  The following shows 2 different code snippets on how to get the hosts allocated to the job and pass them to mpirun.</p> <p>option 1 - simplest</p> <ul> <li>mpirun -hostfile $COBALT_NODEFILE -n 16 -npernode 8 mpi-example-code</li> <li>where $COBALT_NODEFILE is a file that the -hostfile option can use.</li> </ul> <p>option 2 - little more complicated</p> <ul> <li>HOSTS=$(cat $COBALT_NODEFILE | sed ':a;N;$!ba;s/\\n/,/g')</li> <li>mpirun  --np 16 --host $HOSTS --oversubscribe ./mpi-example-code</li> </ul> <p>To specifically see how the MPI ranks were assigned, one could add --display-map --display-allocation to the mpirun options.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#controlling-which-cobalt-instance-gpu-for-my-commands","title":"Controlling which Cobalt instance (GPU) for my commands","text":"<p>IF YOU ARE ONLY USING KNL NODES NOTHING CHANGES AND YOU CAN IGNORE THIS</p> <p>Because of the difference in architectures and limitations in Cobalt V1, we are running two Cobalt instances, the existing one for the KNL nodes, which remains as is, and a second one for the GPU nodes.  You need to be able to control which instance you are interacting with and there are several ways to do so.   - As was true in the past, if you do nothing, the commands will default to the architecture associated with the host you are on when you issue it   - If you are on the Theta login nodes, commands will default to the KNL instance.   - If you are on a GPU node, for instance the build nodes, then commands will default to the GPU instance.   - You can set an environment variable to control which instance the default commands (qsub, qstat, etc) will interact with. The primary use case here will be users who only use GPU nodes, but are working from the Theta login nodes.  To do so, you may:     - <code>module load cobalt/cobalt-knl</code> which would make cobalt commands interact with the original Cobalt instance and launch jobs on the KNL nodes     - <code>module load cobalt/cobalt-gpu</code> which would make Cobalt commands interact with the new Cobalt instance and launch jobs on the GPU nodes     - you can also set COBALT_CONFIG_FILES=       - knl config: /etc/cobalt.knl.conf       - gpu config: /etc/cobalt.gpu.conf <p>You can use suffixed commands to explicitly control which instance you are interacting with. If you regularly use both types of nodes, this is the recommended path to avoid confusion and to prevent launching jobs on the wrong architecture.</p> <p>All the commands you are used to are there, they take the same command line parameters, etc., they just have either -knl or a -gpu suffix on them. For instance:   - qsub-knl  would submit a job to the KNL nodes   - qstat-gpu would check the queue status for the GPU nodes. <p>Requesting DGX nodes or individual GPUs The DGX nodes, which contain (8) A100 GPUs, are extremely powerful and it can be very difficult for a single job to efficiently use an entire node.  For this reason, you may request either full nodes (all 8 GPUS) or individual GPUs.  What you are assigned (a node or a GPU) is dependent on the queue you submit to:   - If the queue name ends in -node, you will get full nodes (8 A100 GPUs)    - If the queue name ends in -gpu, you will get an individual GPU   - The -n parameter on the qsub is the number of resources of the type in that queue. So, for example:     - <code>qsub -n 2 -q full-node &lt;rest of command line&gt;</code> would get you two full DGX nodes, which would be a total of (16) A100 GPUs     - <code>qsub -n 2 -q single-gpu &lt;rest of command line&gt;</code> would get you two A100 GPUs</p> <p>For reservations, you can only have one queue, and the resources in the queue need to be consistent, so your entire reservation must be in nodes or GPUs. If you need both, you will need two reservations, one for each type of resource.</p> <pre><code>- Node names are of the form thetagpu## where ## ranges from 01 to 24.  This is an entire node (8 GPUs)\n- GPU names are of the form thetagpu##-gpu# where the GPU numbers range from 0-7.\n</code></pre>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#multi-instance-gpu-mig-mode","title":"Multi-Instance GPU (MIG) mode","text":"<p>The A100 GPUs have a capability known as Multi-Instance GPU (MIG). This allows you a single A100 GPU to be reconfigured at a hardware level down to a maximum of 7 instances. The valid configuration are shown in a table on the MIG page referenced above. These instances appear as a GPU to the application. In order to use this feature, the GPU must be put into MIG mode and this requires a reset of the GPU.  At the current time, we are not supporting scheduling at the MIG level.  However, a user can request that their GPU be put in MIG mode and then they can reconfigure the GPU into a supported configuration from their job script. </p> <p>If you wish to have the resources you have requested put into MIG mode you can add either of these to your qsub command line: <code>--attrs mig-mode=True</code>.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#details-of-a-job-submission","title":"Details of a job submission","text":"<p>Details of the job submission are recorded in the <code>&lt;jobid&gt;.cobaltlog</code>. This file contains the qsub command and environment variables. The location of this file can be controlled with the <code>qsub --debuglog &lt;path&gt;</code> that defaults to the same place as the <code>.output</code> and <code>.error</code> files.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#jobs-stuck-in-starting-state","title":"Jobs stuck in \"starting\" state","text":"<p>If you submit a job and qstat shows it in \"starting\" state for 5 minutes or more, most likely your memory/numa mode selection requires rebooting some or all of the nodes your job was assigned. This process takes about 15 minutes, during which your job appears to be in the \"starting\" phase. When no reboots are required, the \"starting\" phase only lasts a matter of seconds.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#utime-and-stime","title":"\"utime\" and \"stime\"","text":"<p>At the bottom of a .ouput file, there is usually a line like: <pre><code>Application 3373484 resources: utime ~6s, stime ~6s, Rss ~5036, inblocks ~0, outblocks ~8\n</code></pre> The \"utime\" and \"stime\" values are user CPU time and system CPU time from the aprun and getrusage commands. They are rounded aggregate numbers scaled by the number of resources used, and are approximate. The aprun man page has more information about them."},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#cobalt-directives-on-the-second-line-of-job-script","title":"<code>#COBALT</code> directives on the second line of job script","text":"<p>If <code>#COBALT</code> directives are used inside a job submission script, then they must appear at the topmost lines of the script. <code>#COBALT</code> directives following a blank line will be ignored. Attempting to qsub the following example script will lead to the error message below.</p> <p><pre><code>&gt; cat submit.csh #!/bin/csh #COBALT -n 2 -t 2:00:00 -q full-node mpirun -np 20 -npernode 10 ./my_app &gt; qsub submit.csh Usage: qsub.py [options] [] Refer to man pages for JOBID EXPANSION and SCRIPT JOB DIRECTIVES. No required options provided\n</code></pre> A correct submission script would look like the following with the blank line removed. <pre><code>&gt; cat submit.csh\n\n#!/bin/csh\n\n#COBALT -n 2 -t 2:00:00 -q full-node\n\nmpirun -np 20 -npernode 10 ./my_app\n\n&gt; qsub submit.csh\n\n12345\n</code></pre></p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#job-submission-on-thetagpu","title":"Job Submission on ThetaGPU","text":"<p>The queuing system used on ThetaGPU is Cobalt. On ThetaGPU, Cobalt jobs may run either as script jobs or interactive mode jobs.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#script-method","title":"Script Method","text":"<p>In the script method, Cobalt will execute a user-supplied script when running a user\u2019s job. Following are the required flags to <code>qsub</code>, as well as some of the more common options. A complete list of options may be found as a part of the <code>qsub</code> manpage, available on any login node.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#required-flags","title":"Required Flags","text":"<pre><code>-n NN - number of nodes (-n 16 for 16 nodes)\n-t time - running time (-t 30 for 30 minutes, -t 01:10:20 for 1 hr 10 min 20 sec) \n-A Project - project (-A YourProject)\n</code></pre>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#common-options","title":"Common options","text":"<p><pre><code>--attrs - you may specify additional attributes for your job. \n          Multiple attribute key-value pairs are colon-delimited. \n          The following are common on the KNL nodes: \n          - filesystem: a comma-separated list of filesystems used while running your job\n          - location: a comma-separated list of node ids. Ranges may be hyphenated. \n          - mcdram: The desired MCDRAM mode of a job (default: cache) \n          - numa: The desired NUMA mode of a job (default: quad)\n\n          The following are common on the GPU nodes:          \n          - location: a comma-separated list of node names (not IDs). Ranges may NOT be hyphenated on the GPU nodes. \n          - mig-mode: Should the GPUs be put in Multi Instance GPU (MIG) mode (default: False) \n          - pubnet: Enable public network connectivity from compute nodes\n\nSee: https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html\n\n--env VAR1=1:VAR2=2:\u2026 - specify required environment variables\n-i file - give a file name to be used for stdin \n-O Name - name your job and stdout/stderr (-O Job1) \n-q queue - queue name (full-node, single-gpu, bigmem) (default: full-node)\n</code></pre> Note: Remember to give all options before the executable name.</p> <p>Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation on ThetaGPU bthrough our Director's Discretionary award. ThetaGPU is listed under Theta on the form.</p> <p>Users will need to load the <code>cobalt/cobalt-gpu</code> module before issuing a <code>qsub</code> command to access ThetaGPU.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#example-script","title":"Example Script","text":"<p><pre><code>module load cobalt/cobalt-gpu\n\nqsub -A YourProject -n 4 -t 30 -q full-node \\\n--env MYVAR=value1 -i inputdata -O Project1_out \\\n\u2013attrs filesystems=home,eagle \\\nprogram.exe progarg1\n</code></pre> The syntax for Cobalt scripting is slightly different than that of a PBS script. For more information, see Cobalt scripting.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#interactive-method","title":"Interactive Method","text":"<p>To run an \u201cinteractive mode\u201d job on ALCF Cray resources, add the \u201c-I\u201d (uppercase \"i\", not a lowercase \"L\") flag or \u201c--mode interactive\u201d to your qsub line and omit any executable. Your qsub submission will then wait until nodes are allocated to your job and Cobalt will start a shell on a job-launch node on your behalf. You may aprun against your assigned resources and run other interactive commands from this node. It is important to note that your shell is executed from a launch node and not from your compute head-node. Once your allocation ends, all apruns will be terminated, but your shell will remain for any cleanup actions that you choose to take.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#ensemble-jobs","title":"Ensemble Jobs","text":"<p>Users may run an \u201censemble job\u201d and combine runs into a single script. This can provide major enhancements to throughput, especially for large ensemble jobs. Users may run multiple jobs in sequence or may use multiple backgrounded apruns to subset their resources among multiple backend executables. There is a system limitation of 1000 simultaneous apruns per Cobalt script job.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#submitted-job-with-the-wrong-arguments","title":"Submitted Job with the Wrong Arguments","text":"<p>If you submit a job with the wrong arguments, you can modify without deleting it and resubmitting it. Most settings can be changed using <code>qalter</code>.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#example","title":"Example","text":"<p><pre><code> Usage: qalter [-d] [-v] -A &lt;project name&gt; -t &lt;time in minutes&gt;\n             --attrs filesystems=&lt;filesystem&gt;\n             -e &lt;error file path&gt; -o &lt;output file path&gt;\n             --dependencies &lt;jobid1&gt;:&lt;jobid2&gt;\n             -n &lt;number nodes of&gt; -h --proccount &lt;processor count&gt;\n             -M &lt;email address&gt; &lt;jobid1&gt; &lt;jobid2&gt;\n</code></pre> Note: To change the queue, use <code>qmove</code>. <pre><code>Usage: qmove &lt;queue name&gt; &lt;jobid&gt; &lt;jobid&gt;\n</code></pre></p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#changing-executable-after-job-submission","title":"Changing Executable after Job Submission","text":"<p>When a job is submitted via qsub, Cobalt records the path to the executable or script, but it does not make a copy. As a result, if the executable or script is modified when there is a deletion or modification, it will affect any jobs already submitted that use that executable. To avoid confusion, it is generally best to avoid making changes after job submission.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#holding-and-releasing-jobs","title":"Holding and Releasing Jobs","text":""},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#user-holds","title":"User Holds","text":"<p>To hold a job (prevent from running), use <code>qhold</code>. This will put the job in the <code>user_hold</code> state. <pre><code>qhold &lt;jobid&gt;\n</code></pre> To release a job in a user hold (<code>user_hold</code>) state, use <code>qrls</code>. <pre><code>qrls &lt;jobid&gt;\n</code></pre> A job may also be put into a user hold immediately upon submission by passing <code>qsub</code> the <code>-h</code> flag. <pre><code>qsub -n 8 -t 60 --attrs filesystems=home,grand -A MyProject -h myExe\n</code></pre></p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#dependency-holds","title":"Dependency Holds","text":"<p>For jobs in the <code>dep_hold</code> or <code>dep_fail</code> state, see the section on job dependencies.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#admin-holds","title":"Admin Holds","text":"<p>Jobs in the state <code>admin_hold</code> may be released only by a system administrator.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#maxrun-holds","title":"MaxRun Holds","text":"<p>Jobs may temporarily enter the state <code>maxrun_hold</code> if the user has reached the limit of per-user running jobs in a particular queue. No action is required; as running jobs complete, jobs in the <code>maxrun_hold</code> state will be automatically changed back to queued and eligible to run.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#job-dependencies","title":"Job Dependencies","text":"<p>To submit a job that waits until another job or jobs have completed, use the dependencies argument to qsub. For example, to submit a job that depends on job 12345: <pre><code>qsub -n 2 -t 10 --attrs filesystems=theta-fs0,grand,home -A yourproject --dependencies 12345 a.out\n</code></pre> For multiple dependencies, list and separate with colons. <pre><code>qsub -n 2 -t 30 -A yourproject --attrs filesystems=theta-fs0,grand,home --dependencies 12345:12346 a.out\n</code></pre> Jobs submitted with dependencies will remain in the state <code>dep_hold</code> until all the dependencies are fulfilled, then will proceed to the state queued.</p> <p>Note: In the event any of the dependencies do not complete successfully (nonzero exit status), the job will instead go into the state <code>dep_fail</code>. To manually release a job that is in either <code>dep_hold</code> or <code>dep_fail</code>: <pre><code>qrls --dependencies &lt;jobid&gt;\n</code></pre> or alternatively change the job's dependencies setting to \"none\": <pre><code>qalter --dependencies none &lt;jobid&gt;\n</code></pre></p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#customizing-the-output-of-qstat","title":"Customizing the Output of <code>qstat</code>","text":"<p>Default fields displayed by the <code>qstat</code> command may be changed by setting the <code>QSTAT_HEADER</code> environment variable. <pre><code>export QSTAT_HEADER=\"JobID:JobName:User:WallTime:RunTime:Nodes:State:attrs:Queue\"\"\nqstat\n\nJobID   JobName                           User      WallTime  RunTime   Nodes  State      attrs             Queue\n     =======================================================================================================================================\n     104927  N/A                               user1     02:00:00  01:20:45  128    running    {'numa': 'quad', 'mcdram': 'cache'}  backfill\n     104941  N/A                               user2     00:20:00  N/A       2048   queued     {'numa': 'quad', 'mcdram': 'flat'}   backfill\n     104934  xxxx.yyyyy                        user3     04:00:00  01:10:12  32     running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104948  Xxx-YY_ZZ                         user4     02:00:00  00:15:03  128    running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104919  aaaaa_0000_bbbb_c                 user5     06:00:00  01:50:21  64     running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104945  aaaa_bb_cccc-d_eee_f.hhhhhh.iiii  user6     06:00:00  00:18:25  100    running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104848  bbbbbb                            user7     01:00:00  N/A       3624   queued     {'numa': 'quad', 'mcdram': 'cache'}  default\n     ....\n</code></pre> One may specify column headers via the <code>--header</code> flag to <code>qstat</code>.</p> <p>Available field names can be seen by entering <code>qstat -fl &lt;jobid&gt;</code> for any current jobid.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#redirecting-standard-input","title":"Redirecting Standard Input","text":"<p>To redirect the standard input to a job, do not use the <code>&lt;</code> redirection operator on the <code>qsub</code> command line. This simply redirects standard input to <code>qsub</code>, not the job itself. Instead, use the qsub option <code>-i</code>. <pre><code># WRONG\nqsub -t 10 -n 1 --attrs filesystems=home a.out &lt; my_input_file.dat\n\n# RIGHT\nqsub -t 10 -n 1 --attrs filesystems=home -i my_input_file.dat a.out\n</code></pre></p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#project-names","title":"Project Names","text":"<p>You can find active project names that your account is associated with by running the command: <pre><code>sbank allocations\n</code></pre> If an account is associated with more than one project, a job must be submitted by using a specific project name using <code>-A</code> or by setting the environment variable <code>COBALT_PROJ</code>.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#sbank","title":"Sbank","text":"<p>The sbank database is updated hourly. This means transactions against your account can take up to an hour before they show up.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#submitting-into-backfill-nodes","title":"Submitting into Backfill Nodes","text":"<p>Sometimes the scheduler will try to clear up room for a large job. During these times, although not many jobs may be running, new jobs are not being scheduled as expected.</p> <p>At such times, backfill nodes may be available. While nodes are being drained for a larger job, other user jobs may be backfilled onto these resources, provided that their requested wall time is less than the remaining drain time of the set of resources. For instance, suppose that 16 nodes are being drained to allow a 16-node job to run. Of the 16 nodes, perhaps eight are empty and the other eight are running an eight-node job that has 2 hours of wall time left. This allows the opportunity to run a 2-hour, eight-node job in the backfill here.</p> <p>To discover available backfill, run the nodelist command. This command can only be run on the service nodes (<code>thetagpusn1-2</code>), it cannot be run on the compute nodes.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#example_1","title":"Example","text":"<p><pre><code>nodelist\nHost             Queue                                       State      Backfill\n==================================================================================\n[...]\nthetagpu16       CompBioAffin:backfill:full-node             down       -       \nthetagpu16-gpu0  single-gpu                                  idle       -       \nthetagpu16-gpu1  single-gpu                                  allocated  -       \nthetagpu16-gpu2  single-gpu                                  allocated  -       \nthetagpu16-gpu3  single-gpu                                  allocated  -       \nthetagpu16-gpu4  single-gpu                                  idle       -       \nthetagpu16-gpu5  single-gpu                                  idle       -       \nthetagpu16-gpu6  single-gpu                                  idle       -       \nthetagpu16-gpu7  single-gpu                                  idle       -       \nthetagpu17       CompBioAffin:backfill:full-node             allocated  -       \nthetagpu17-gpu0  single-gpu                                  down       -       \nthetagpu17-gpu1  single-gpu                                  down       -       \nthetagpu17-gpu2  single-gpu                                  down       -       \nthetagpu17-gpu3  single-gpu                                  down       -       \nthetagpu17-gpu4  single-gpu                                  down       -       \nthetagpu17-gpu5  single-gpu                                  down       -       \nthetagpu17-gpu6  single-gpu                                  down       -       \nthetagpu17-gpu7  single-gpu                                  down       -\n[...]\n</code></pre> In this example, a four-node job with a maximum wall time of 4 hours and 59 minutes can be run during this backfill. The backfill times will not always be identical and will depend on the mix of jobs on the partitions that are being drained.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#submitting-to-specific-nodes","title":"Submitting to Specific Nodes","text":"<p>In rare cases, there may be a need to target specific hardware. This may be accomplished using <code>--attrs location=</code>.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#example_2","title":"Example","text":"<p><pre><code>qsub -t 10 -n 2 --attrs filesystems=grand location=thetagpu01:thetagpu02 myprog.exe \n</code></pre> This will force the job to run on those specific nodes. Should that location become unschedulable, for instance, due to a failed node, the job will not be allowed to run anywhere else, without resetting the location attribute. If more nodes are specified in the location field than are required to fill a job\u2019s requested node count, then the first n nodes available in the location set will be used.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#running-with-a-group-of-users","title":"Running with a Group of Users","text":"<p>Sometimes it is useful to allow other users to run Cobalt commands on a given job such as <code>qhold</code>, <code>qrls</code>, or <code>qdel</code>. A list of users can be allowed to run commands on your job by submitting a list of users to <code>qsub</code>, <code>cqsub</code>, or <code>qalter</code> using the flag <code>--run_users</code>. Specified users need not be in the same project under which the job was submitted.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#example_3","title":"Example","text":"<p><pre><code>qsub -t 10 -n 16 --attrs filesystems=grand,eagle,home location=thetagpu01:thetagpu02 --run_users frodo:sam:pippin myprog.exe\n</code></pre> As a convenience, all users belonging to the project under which a job was submitted can be added to a list of users that may control a job by using the <code>--run_project</code> flag.</p> <p>Users who have been added to the list can run any command that the job-submitter could run on a job. This includes <code>qhold</code>, <code>qrls</code>, <code>qalter</code>, and <code>qdel</code>.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#group-running-and-file-system-groups","title":"Group Running and File System Groups","text":"<p>While setting this list of users allows any of the listed users to run Cobalt commands on a job, it does not do anything about the permissions of any files involved with the job. Those must be handled by the user(s) setting appropriate permissions on their directories to allow users in their group to read and write files as appropriate. If your project needs a group on the file system to share files or a user needs to be added, email User Support.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#more-information","title":"More Information","text":"<p>For more information on Cobalt commands, their options, consult the manpages on the system. The same information may be found online in the Cobalt Command Reference.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#using-the-job-resource-manager-commands-options-and-examples","title":"Using the Job Resource Manager: Commands, Options, and Examples","text":"<p>This document provides examples of how to submit jobs on our systems. It also provides examples of commands that can be used to query the status of jobs, what partitions are available, etc. For an introduction to using the job resource manager and running jobs on ThetaGPU, see Running Jobs on ThetaGPU. </p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#submit-a-job-request","title":"Submit a Job Request","text":"<p>Use <code>qsub</code> to submit a job. Unlike jobs on the ALCF Blue Gene systems, all jobs on ThetaGPU are either script or interactive.</p> <p>Run the script <code>jobscript.sh</code> with 2 nodes for a maximum of 15 minutes: <pre><code>qsub -n 2 -t 15 --attrs filesystems=theta-fs0 jobscript.sh\n</code></pre> To submit jobs to a particular queue, use <code>qsub -q &lt;queue_name&gt;</code>.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#charge-a-job-to-a-project","title":"Charge a Job to a Project","text":"<p>Use <code>qsub -A &lt;project_name&gt;</code> to charge a job to a particular project.</p> <p>To run <code>jobscript.sh</code> with 2 nodes for a maximum of 15 minutes and charge the job to MyProject: <pre><code>qsub -n 2 -t 15 --attrs filesystems=grand,home -A MyProject jobscript.sh\n</code></pre> To see which projects you are a member of: <pre><code>projects\n</code></pre> You can use the environment variable <code>COBALT_PROJ</code> to set your default project. qsub -A takes precedence over <code>COBALT_PROJ</code>.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#delete-a-job-from-the-queue","title":"Delete a Job from the Queue","text":"<p>To delete a job from the queue, use the qdel command. For example for job with ID of 34586. <pre><code>qdel 34586\n</code></pre> Depending on the stage of a job\u2019s lifetime, qdel may not complete immediately, especially if the delete is issued during startup on a job that is changing memory modes and rebooting a node. If the job does not ultimately terminate, contact support@alcf.anl.gov with the jobid so that an administrator can take appropriate cleanup actions and administratively terminate the job.</p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#query-partition-availability","title":"Query Partition Availability","text":"<p>To determine which partitions are currently available to the scheduler, use the nodelist command. This command provides a list of node ids, names, queue, and state as well as any backfill windows. This command can only be run on the service nodes (<code>thetagpusn1-2</code>), it cannot be run on the compute nodes. </p> <p>For example on <code>thetagpusnX</code> it displays: <pre><code>Host             Queue                                       State      Backfill\n================================================================================== \n[...]\nthetagpu16       CompBioAffin:backfill:full-node             down       -       \nthetagpu16-gpu0  single-gpu                                  idle       -       \nthetagpu16-gpu1  single-gpu                                  allocated  -       \nthetagpu16-gpu2  single-gpu                                  allocated  -       \nthetagpu16-gpu3  single-gpu                                  allocated  -       \nthetagpu16-gpu4  single-gpu                                  idle       -       \nthetagpu16-gpu5  single-gpu                                  idle       -       \nthetagpu16-gpu6  single-gpu                                  idle       -       \nthetagpu16-gpu7  single-gpu                                  idle       -       \nthetagpu17       CompBioAffin:backfill:full-node             allocated  -       \nthetagpu17-gpu0  single-gpu                                  down       -       \nthetagpu17-gpu1  single-gpu                                  down       -       \nthetagpu17-gpu2  single-gpu                                  down       -       \nthetagpu17-gpu3  single-gpu                                  down       -       \nthetagpu17-gpu4  single-gpu                                  down       -       \nthetagpu17-gpu5  single-gpu                                  down       -       \nthetagpu17-gpu6  single-gpu                                  down       -       \nthetagpu17-gpu7  single-gpu                                  down       -       \nthetagpu18       CompBioAffin:backfill:full-node             allocated  -       \n[...]\n</code></pre></p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#specifying-filesystems","title":"Specifying Filesystems","text":"<p>On ThetaGPU and other systems running Cobalt at the ALCF your job submission should specify which filesystems your will be using.  In the event that a filesystem becomes unavailable, this information is used to preserve jobs that would use that filesystem while allowing other jobs that are not using an affected filesystem to proceed to run normally.</p> <p>You may specify your filesystem by adding <code>filesystems=&lt;list of filesystems&gt;</code> to the <code>--attrs</code> argument of qsub in Cobalt. Valid filesystems are <code>home</code>, <code>eagle</code>, <code>grand</code>, and <code>theta-fs0</code>. The list is comma-delimited. </p> <p>For example, to request the <code>home</code> and <code>eagle</code> filesystems for your job you would add <code>filesystems=home,eagle</code> to your <code>qsub</code> command. If this is not specified a warning will be printed and then the job will be tagged as requesting all filesystems and may be held unnecessarily if a filesystem is not currently available. The warnings are written to stderr of <code>qsub</code> and <code>qalter</code> commands that change the value of the <code>--attrs</code> flag.  Scripts that are parsing stderr from these utilities may encounter errors from the additional warnings if filesystems are not specified in these commands.</p> <p>If a job is submitted while a filesystem it requested is marked down, the job will automatically be placed into a <code>user_hold</code> status and a warning message will be printed, but the job will be otherwise queued. The job is also placed into <code>admin_hold</code> status by a sysadmin script. Once the affected filesystem has been returned to normal operation, the <code>admin_hold</code> is released. You are responsible for releasing the <code>user_hold</code> once you receive the message that the affected filesystem has been returned to normal operation. The job cannot run until both the holds are released.</p> <p>If a job requesting a filesystem that is marked down is already in the queue, it will be placed on <code>admin_hold</code> status and will be released once the filesystem is operational. <pre><code>qsub -n 1 -t 30 -q full-node --attrs filesystems=home,grand -A Project ./my_job.sh\n</code></pre> To update the filesystems list for your job, use <code>qalter</code>. Note that <code>qalter --attrs</code> is a replace and not an update operation. This means that you should once again specify all the attributes that you had in the original <code>qsub</code> command. <pre><code>qalter --attr filesystems=home,eagle:mig-mode=True &lt;jobid&gt;\n</code></pre> To release user hold: <pre><code>qrls &lt;jobid&gt;\n</code></pre></p>"},{"location":"theta-gpu/queueing-and-running-jobs/job-and-queue-scheduling/#references","title":"References","text":""}]}